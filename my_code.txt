├── .env.example
├── .gitignore
├── LICENSE
├── README.md
├── backend
    ├── Dockerfile
    ├── app
    │   ├── __init__.py
    │   ├── api
    │   │   ├── __init__.py
    │   │   ├── v1
    │   │   │   ├── __init__.py
    │   │   │   ├── data.py
    │   │   │   ├── ml.py
    │   │   │   ├── models.py
    │   │   │   └── statistics.py
    │   │   └── v2
    │   │   │   └── __init__.py
    │   ├── config.py
    │   ├── core
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── plugin_manager.py
    │   │   ├── registry.py
    │   │   └── session_manager.py
    │   ├── engines
    │   │   ├── compute
    │   │   │   ├── __init__.py
    │   │   │   ├── distributed.py
    │   │   │   ├── gpu.py
    │   │   │   └── local.py
    │   │   ├── ml
    │   │   │   ├── __init__.py
    │   │   │   ├── deep_learning.py
    │   │   │   ├── model_hub.py
    │   │   │   ├── supervised.py
    │   │   │   └── unsupervised.py
    │   │   └── statistical
    │   │   │   ├── __init__.py
    │   │   │   ├── descriptive.py
    │   │   │   ├── inferential.py
    │   │   │   ├── multivariate.py
    │   │   │   ├── regression.py
    │   │   │   └── timeseries.py
    │   ├── main.py
    │   ├── models
    │   │   ├── __init__.py
    │   │   ├── analysis_models.py
    │   │   ├── data_models.py
    │   │   └── ml_models.py
    │   ├── plugins
    │   │   ├── README.md
    │   │   ├── __init__.py
    │   │   └── example_plugin
    │   │   │   ├── init.py
    │   │   │   └── plugin.json
    │   ├── services
    │   │   ├── __init__.py
    │   │   ├── analysis_service.py
    │   │   ├── data_service.py
    │   │   ├── export_service.py
    │   │   ├── ml_service.py
    │   │   └── visualization_service.py
    │   └── utils
    │   │   ├── __init__.py
    │   │   ├── decorators.py
    │   │   ├── helpers.py
    │   │   └── validators.py
    ├── constraints.txt
    ├── requirements-dev.txt
    ├── requirements-ml.txt
    └── requirements.txt
├── docker-compose.yml
├── docs
    ├── API.md
    ├── ARCHITECTURE.md
    ├── ML_GUIDE.md
    └── PLUGINS.md
├── frontend
    ├── assets
    │   └── logo.svg
    ├── css
    │   └── styles.css
    ├── index.html
    └── js
    │   ├── core
    │       ├── api.js
    │       ├── app.js
    │       └── state.js
    │   ├── modules
    │       ├── data.js
    │       ├── ml.js
    │       ├── statistics.js
    │       └── visualization.js
    │   └── utils
    │       └── helpers.js
└── plugins
    └── README.md


/.env.example:
--------------------------------------------------------------------------------
 1 | # Application
 2 | APP_NAME=OpenStatica
 3 | APP_VERSION=1.0.0
 4 | DEBUG=false
 5 |
 6 | # Security
 7 | SECRET_KEY=your-secret-key-here-change-in-production
 8 |
 9 | # Database
10 | DATABASE_URL=postgresql://openstatica:password@localhost:5432/openstatica
11 | REDIS_URL=redis://localhost:6379
12 |
13 | # Storage
14 | UPLOAD_PATH=./uploads
15 | MODEL_CACHE_PATH=./models
16 |
17 | # ML Features
18 | ENABLE_ML=true
19 | ENABLE_GPU=false
20 | HUGGINGFACE_TOKEN=your-huggingface-token-here
21 |
22 | # Compute
23 | MAX_WORKERS=4
24 | COMPUTATION_TIMEOUT=300
25 |
26 | # Plugins
27 | ENABLE_PLUGINS=true
28 | PLUGIN_PATH=./plugins


--------------------------------------------------------------------------------
/.gitignore:
--------------------------------------------------------------------------------
  1 | # Byte-compiled / optimized / DLL files
  2 | __pycache__/
  3 | *.py[codz]
  4 | *$py.class
  5 |
  6 | # C extensions
  7 | *.so
  8 |
  9 | # Distribution / packaging
 10 | .Python
 11 | build/
 12 | develop-eggs/
 13 | dist/
 14 | downloads/
 15 | eggs/
 16 | .eggs/
 17 | lib/
 18 | lib64/
 19 | parts/
 20 | sdist/
 21 | var/
 22 | wheels/
 23 | share/python-wheels/
 24 | *.egg-info/
 25 | .installed.cfg
 26 | *.egg
 27 | MANIFEST
 28 |
 29 | # PyInstaller
 30 | #  Usually these files are written by a python script from a template
 31 | #  before PyInstaller builds the exe, so as to inject date/other infos into it.
 32 | *.manifest
 33 | *.spec
 34 |
 35 | # Installer logs
 36 | pip-log.txt
 37 | pip-delete-this-directory.txt
 38 |
 39 | # Unit test / coverage reports
 40 | htmlcov/
 41 | .tox/
 42 | .nox/
 43 | .coverage
 44 | .coverage.*
 45 | .cache
 46 | nosetests.xml
 47 | coverage.xml
 48 | *.cover
 49 | *.py.cover
 50 | .hypothesis/
 51 | .pytest_cache/
 52 | cover/
 53 |
 54 | # Translations
 55 | *.mo
 56 | *.pot
 57 |
 58 | # Django stuff:
 59 | *.log
 60 | local_settings.py
 61 | db.sqlite3
 62 | db.sqlite3-journal
 63 |
 64 | # Flask stuff:
 65 | instance/
 66 | .webassets-cache
 67 |
 68 | # Scrapy stuff:
 69 | .scrapy
 70 |
 71 | # Sphinx documentation
 72 | docs/_build/
 73 |
 74 | # PyBuilder
 75 | .pybuilder/
 76 | target/
 77 |
 78 | # Jupyter Notebook
 79 | .ipynb_checkpoints
 80 |
 81 | # IPython
 82 | profile_default/
 83 | ipython_config.py
 84 |
 85 | # pyenv
 86 | #   For a library or package, you might want to ignore these files since the code is
 87 | #   intended to run in multiple environments; otherwise, check them in:
 88 | # .python-version
 89 |
 90 | # pipenv
 91 | #   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
 92 | #   However, in case of collaboration, if having platform-specific dependencies or dependencies
 93 | #   having no cross-platform support, pipenv may install dependencies that don't work, or not
 94 | #   install all needed dependencies.
 95 | #Pipfile.lock
 96 |
 97 | # UV
 98 | #   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
 99 | #   This is especially recommended for binary packages to ensure reproducibility, and is more
100 | #   commonly ignored for libraries.
101 | #uv.lock
102 |
103 | # poetry
104 | #   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
105 | #   This is especially recommended for binary packages to ensure reproducibility, and is more
106 | #   commonly ignored for libraries.
107 | #   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
108 | #poetry.lock
109 | #poetry.toml
110 |
111 | # pdm
112 | #   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
113 | #   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
114 | #   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
115 | #pdm.lock
116 | #pdm.toml
117 | .pdm-python
118 | .pdm-build/
119 |
120 | # pixi
121 | #   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
122 | #pixi.lock
123 | #   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
124 | #   in the .venv directory. It is recommended not to include this directory in version control.
125 | .pixi
126 |
127 | # PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
128 | __pypackages__/
129 |
130 | # Celery stuff
131 | celerybeat-schedule
132 | celerybeat.pid
133 |
134 | # SageMath parsed files
135 | *.sage.py
136 |
137 | # Environments
138 | .env
139 | .envrc
140 | .venv
141 | env/
142 | venv/
143 | ENV/
144 | env.bak/
145 | venv.bak/
146 |
147 | # Spyder project settings
148 | .spyderproject
149 | .spyproject
150 |
151 | # Rope project settings
152 | .ropeproject
153 |
154 | # mkdocs documentation
155 | /site
156 |
157 | # mypy
158 | .mypy_cache/
159 | .dmypy.json
160 | dmypy.json
161 |
162 | # Pyre type checker
163 | .pyre/
164 |
165 | # pytype static type analyzer
166 | .pytype/
167 |
168 | # Cython debug symbols
169 | cython_debug/
170 |
171 | # PyCharm
172 | #  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
173 | #  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
174 | #  and can be added to the global gitignore or merged into this file.  For a more nuclear
175 | #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
176 | #.idea/
177 |
178 | # Abstra
179 | # Abstra is an AI-powered process automation framework.
180 | # Ignore directories containing user credentials, local state, and settings.
181 | # Learn more at https://abstra.io/docs
182 | .abstra/
183 |
184 | # Visual Studio Code
185 | #  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore
186 | #  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
187 | #  and can be added to the global gitignore or merged into this file. However, if you prefer,
188 | #  you could uncomment the following to ignore the entire vscode folder
189 | # .vscode/
190 |
191 | # Ruff stuff:
192 | .ruff_cache/
193 |
194 | # PyPI configuration file
195 | .pypirc
196 |
197 | # Cursor
198 | #  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
199 | #  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
200 | #  refer to https://docs.cursor.com/context/ignore-files
201 | .cursorignore
202 | .cursorindexingignore
203 |
204 | # Marimo
205 | marimo/_static/
206 | marimo/_lsp/
207 | __marimo__/
208 |
209 | # Node
210 | node_modules/
211 | npm-debug.log
212 | yarn-error.log
213 |
214 | # Jupyter
215 | .ipynb_checkpoints/
216 |


--------------------------------------------------------------------------------
/LICENSE:
--------------------------------------------------------------------------------
 1 | MIT License
 2 |
 3 | Copyright (c) 2025 The Wandering Algorithm
 4 |
 5 | Permission is hereby granted, free of charge, to any person obtaining a copy
 6 | of this software and associated documentation files (the "Software"), to deal
 7 | in the Software without restriction, including without limitation the rights
 8 | to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 9 | copies of the Software, and to permit persons to whom the Software is
10 | furnished to do so, subject to the following conditions:
11 |
12 | The above copyright notice and this permission notice shall be included in all
13 | copies or substantial portions of the Software.
14 |
15 | THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
16 | IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
17 | FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
18 | AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
19 | LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
20 | OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
21 | SOFTWARE.
22 |


--------------------------------------------------------------------------------
/README.md:
--------------------------------------------------------------------------------
 1 | # OpenStatica
 2 |
 3 | Open-source, web-based statistical & ML platform that scales from lightweight exploratory analysis to advanced modeling — with endless extensibility.
 4 |
 5 | ---
 6 |
 7 | ## What is it?
 8 |
 9 | OpenStatica is a browser-first analytics workbench. It combines classic statistics, modern machine learning, and a plugin-friendly design so teams can explore data, run tests, train models, and visualize results in one place.
10 |
11 | ---
12 |
13 | ## What it does
14 |
15 | * Descriptive and inferential statistics (from summaries to t-tests and ANOVA)
16 | * Ready-to-use ML workflows (supervised/unsupervised, evaluation, and selection)
17 | * Real-time, responsive analysis with async processing
18 | * Extensible via a modular plugin system
19 | * Works with common data formats (CSV, Excel, JSON, Parquet, …)
20 | * Scales from local runs to distributed and GPU-backed computation
21 | * Optional integration with model hubs (e.g., Hugging Face)
22 |
23 | ---
24 |
25 | ## Problems it solves
26 |
27 | * **One tool for EDA → stats → ML:** Reduces tool-hopping and context switching
28 | * **Scales with your needs:** Start small in the browser, graduate to distributed or GPU compute
29 | * **Extensible by design:** Add new tests, models, and visualizations as plugins
30 | * **Fast iteration:** Async workflows keep the UI responsive during heavy jobs
31 | * **Data compatibility:** Load from common tabular formats without friction
32 |
33 | ---
34 |
35 | ## How it works (high level)
36 |
37 | 1. **Load data in the browser** and create a session.
38 | 2. **Explore** with descriptive stats and frequency distributions.
39 | 3. **Infer** using built-in tests (e.g., t-tests, ANOVA).
40 | 4. **Model** with ML engines (train, predict, evaluate).
41 | 5. **Visualize** and interpret results; export when ready.
42 | 6. **Extend** functionality by enabling plugins.
43 |
44 | ---
45 |
46 | ## Architecture (at a glance)
47 |
48 | * **Frontend (Web UI):**
49 |   Lightweight, modular interface for data upload, variable selection, running analyses, and viewing results/plots.
50 |
51 | * **API Layer:**
52 |   Versioned REST endpoints (e.g., `/api/v1`) for data operations, statistics, ML, models, and visualization.
53 |
54 | * **Core Runtime:**
55 |
56 |   * **Engine Registry:** Registers and orchestrates computation engines (statistical, ML, etc.).
57 |   * **Session Manager:** Tracks per-session data, results, and models.
58 |   * **Plugin Manager:** Discovers, loads, and exposes plugin-provided engines and routes.
59 |
60 | * **Computation Engines:**
61 |
62 |   * **Statistical:** Descriptive, frequency, inferential tests (e.g., t-test, ANOVA), with effect sizes and assumptions checks.
63 |   * **ML:** Supervised/unsupervised pipelines, evaluation utilities, optional deep learning adapters.
64 |
65 | * **Compute Backends:**
66 |   Local by default, with optional **distributed** and **GPU** paths for heavier jobs.
67 |
68 | * **Services Layer:**
69 |   Focused modules for data I/O, analysis coordination, ML orchestration, visualization prep, and export.
70 |
71 | * **Model Hub Integration (optional):**
72 |   Hooks to pull, cache, and serve models from external repositories.
73 |
74 | ---
75 |
76 | ## Who is it for?
77 |
78 | * Data analysts and scientists needing an end-to-end, web-based toolkit
79 | * Educators and teams teaching or standardizing statistical workflows
80 | * Developers who want to extend a stats/ML platform via plugins
81 |
82 | ---
83 |
84 | ## Status & Roadmap (short)
85 |
86 | * **Core stats:** Complete
87 | * **ML workflows:** In progress (training, evaluation, selection, AutoML plans)
88 | * **Advanced features planned:** Distributed compute, real-time collaboration, cloud deployment, model hub integration
89 | * **Enterprise (future):** Auth/SSO, workspaces, audit logs, enterprise-grade plugins
90 |
91 | ---
92 |
93 | ## License
94 |
95 | MIT — free to use, modify, and extend.
96 |


--------------------------------------------------------------------------------
/backend/Dockerfile:
--------------------------------------------------------------------------------
 1 | FROM python:3.10-slim
 2 |
 3 | WORKDIR /app
 4 |
 5 | # Install system dependencies
 6 | RUN apt-get update && apt-get install -y \
 7 |     gcc \
 8 |     g++ \
 9 |     build-essential \
10 |     libpq-dev \
11 |     && rm -rf /var/lib/apt/lists/*
12 |
13 | # Copy requirements
14 | COPY requirements.txt .
15 | COPY requirements-ml.txt .
16 |
17 | # Install Python dependencies
18 | RUN pip install --no-cache-dir -r requirements.txt
19 |
20 | # Optional: Install ML dependencies
21 | ARG INSTALL_ML=false
22 | RUN if [ "$INSTALL_ML" = "true" ]; then \
23 |     pip install --no-cache-dir -r requirements-ml.txt; \
24 |     fi
25 |
26 | # Copy application
27 | COPY . .
28 |
29 | # Create necessary directories
30 | RUN mkdir -p uploads models plugins
31 |
32 | EXPOSE 8000
33 |
34 | CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]


--------------------------------------------------------------------------------
/backend/app/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/__init__.py


--------------------------------------------------------------------------------
/backend/app/api/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/api/__init__.py


--------------------------------------------------------------------------------
/backend/app/api/v1/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/api/v1/__init__.py


--------------------------------------------------------------------------------
/backend/app/api/v1/data.py:
--------------------------------------------------------------------------------
  1 | from fastapi import APIRouter, File, UploadFile, HTTPException, Depends, Request
  2 | from typing import Optional, List
  3 | import pandas as pd
  4 | import numpy as np
  5 | import io
  6 | from app.core.session_manager import SessionManager
  7 | from app.core.base import DataInfo
  8 | from app.models import DataUploadResponse
  9 |
 10 | router = APIRouter()
 11 |
 12 |
 13 | def get_session_manager(request: Request) -> SessionManager:
 14 |     return request.app.state.session_manager
 15 |
 16 |
 17 | @router.post("/upload", response_model=DataUploadResponse)
 18 | async def upload_data(
 19 |         file: UploadFile = File(...),
 20 |         session_manager: SessionManager = Depends(get_session_manager)
 21 | ):
 22 |     """Upload and process data file"""
 23 |     try:
 24 |         # Validate file type
 25 |         if not file.filename.endswith(('.csv', '.xlsx', '.xls', '.json', '.parquet')):
 26 |             raise HTTPException(status_code=400, detail="Unsupported file type")
 27 |
 28 |         # Read file content
 29 |         contents = await file.read()
 30 |
 31 |         # Parse based on file type
 32 |         if file.filename.endswith('.csv'):
 33 |             df = pd.read_csv(io.BytesIO(contents))
 34 |         elif file.filename.endswith(('.xlsx', '.xls')):
 35 |             df = pd.read_excel(io.BytesIO(contents))
 36 |         elif file.filename.endswith('.json'):
 37 |             df = pd.read_json(io.BytesIO(contents))
 38 |         elif file.filename.endswith('.parquet'):
 39 |             df = pd.read_parquet(io.BytesIO(contents))
 40 |
 41 |         # Create session
 42 |         session_id = session_manager.create_session()
 43 |         session = session_manager.get_session(session_id)
 44 |
 45 |         # Analyze data types
 46 |         numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
 47 |         categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
 48 |         datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
 49 |
 50 |         # Calculate missing values
 51 |         missing_values = df.isnull().sum().to_dict()
 52 |
 53 |         # Create data info
 54 |         data_info = DataInfo(
 55 |             rows=len(df),
 56 |             columns=len(df.columns),
 57 |             column_names=df.columns.tolist(),
 58 |             column_types={col: str(df[col].dtype) for col in df.columns},
 59 |             numeric_columns=numeric_cols,
 60 |             categorical_columns=categorical_cols,
 61 |             datetime_columns=datetime_cols,
 62 |             missing_values=missing_values,
 63 |             memory_usage=df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
 64 |         )
 65 |
 66 |         # Store in session
 67 |         session.set_data(df, data_info.dict())
 68 |
 69 |         return DataUploadResponse(
 70 |             session_id=session_id,
 71 |             rows=data_info.rows,
 72 |             columns=data_info.columns,
 73 |             column_names=data_info.column_names,
 74 |             numeric_columns=data_info.numeric_columns,
 75 |             categorical_columns=data_info.categorical_columns,
 76 |             preview=df.head(10).replace({np.nan: None}).to_dict(orient='records')
 77 |         )
 78 |
 79 |     except Exception as e:
 80 |         raise HTTPException(status_code=500, detail=str(e))
 81 |
 82 |
 83 | @router.get("/{session_id}")
 84 | async def get_data(
 85 |         session_id: str,
 86 |         rows: Optional[int] = None,
 87 |         columns: Optional[List[str]] = None,
 88 |         session_manager: SessionManager = Depends(get_session_manager)
 89 | ):
 90 |     """Get session data"""
 91 |     session = session_manager.get_session(session_id)
 92 |     if not session or session.data is None:
 93 |         raise HTTPException(status_code=404, detail="Session not found")
 94 |
 95 |     df = session.data
 96 |
 97 |     # Filter columns if specified
 98 |     if columns:
 99 |         df = df[columns]
100 |
101 |     # Limit rows if specified
102 |     if rows:
103 |         df = df.head(rows)
104 |
105 |     return {
106 |         "data": df.replace({np.nan: None}).to_dict(orient='records'),
107 |         "shape": df.shape,
108 |         "columns": df.columns.tolist()
109 |     }
110 |
111 |
112 | @router.get("/{session_id}/info")
113 | async def get_data_info(
114 |         session_id: str,
115 |         session_manager: SessionManager = Depends(get_session_manager)
116 | ):
117 |     """Get detailed data information"""
118 |     session = session_manager.get_session(session_id)
119 |     if not session:
120 |         raise HTTPException(status_code=404, detail="Session not found")
121 |
122 |     return session.metadata
123 |
124 |
125 | @router.post("/{session_id}/transform")
126 | async def transform_data(
127 |         session_id: str,
128 |         transformation: dict,
129 |         session_manager: SessionManager = Depends(get_session_manager)
130 | ):
131 |     """Apply transformations to data"""
132 |     session = session_manager.get_session(session_id)
133 |     if not session or session.data is None:
134 |         raise HTTPException(status_code=404, detail="Session not found")
135 |
136 |     df = session.data.copy()
137 |     transform_type = transformation.get("type")
138 |
139 |     try:
140 |         if transform_type == "normalize":
141 |             from sklearn.preprocessing import StandardScaler
142 |             columns = transformation.get("columns", df.select_dtypes(include=[np.number]).columns)
143 |             scaler = StandardScaler()
144 |             df[columns] = scaler.fit_transform(df[columns])
145 |
146 |         elif transform_type == "encode":
147 |             columns = transformation.get("columns", df.select_dtypes(include=['object']).columns)
148 |             df = pd.get_dummies(df, columns=columns)
149 |
150 |         elif transform_type == "impute":
151 |             strategy = transformation.get("strategy", "mean")
152 |             columns = transformation.get("columns", df.columns)
153 |
154 |             if strategy == "mean":
155 |                 df[columns] = df[columns].fillna(df[columns].mean())
156 |             elif strategy == "median":
157 |                 df[columns] = df[columns].fillna(df[columns].median())
158 |             elif strategy == "mode":
159 |                 df[columns] = df[columns].fillna(df[columns].mode().iloc[0])
160 |             elif strategy == "forward":
161 |                 df[columns] = df[columns].fillna(method='ffill')
162 |             elif strategy == "backward":
163 |                 df[columns] = df[columns].fillna(method='bfill')
164 |
165 |         # Update session data
166 |         session.data = df
167 |
168 |         return {"message": "Transformation applied successfully", "new_shape": df.shape}
169 |
170 |     except Exception as e:
171 |         raise HTTPException(status_code=500, detail=str(e))
172 |
173 |
174 | @router.delete("/{session_id}")
175 | async def delete_session(
176 |         session_id: str,
177 |         session_manager: SessionManager = Depends(get_session_manager)
178 | ):
179 |     """Delete a session and its data"""
180 |     if session_manager.delete_session(session_id):
181 |         return {"message": "Session deleted successfully"}
182 |     else:
183 |         raise HTTPException(status_code=404, detail="Session not found")
184 |


--------------------------------------------------------------------------------
/backend/app/api/v1/ml.py:
--------------------------------------------------------------------------------
  1 | from fastapi import APIRouter, Depends, HTTPException, Request
  2 | from typing import Dict, Any, List
  3 | from app.core.session_manager import SessionManager
  4 | from app.core.registry import EngineRegistry
  5 | from app.engines.ml.supervised import ClassificationEngine, RegressionEngine
  6 | from app.engines.ml.unsupervised import ClusteringEngine, DimensionalityReductionEngine
  7 | from app.models import MLRequest, MLResult
  8 |
  9 | router = APIRouter()
 10 |
 11 |
 12 | def get_session_manager(request: Request) -> SessionManager:
 13 |     return request.app.state.session_manager
 14 |
 15 |
 16 | def get_registry(request: Request) -> EngineRegistry:
 17 |     return request.app.state.registry
 18 |
 19 |
 20 | @router.post("/train", response_model=MLResult)
 21 | async def train_model(
 22 |         request: MLRequest,
 23 |         session_manager: SessionManager = Depends(get_session_manager),
 24 |         registry: EngineRegistry = Depends(get_registry)
 25 | ):
 26 |     """Train a machine learning model"""
 27 |     session = session_manager.get_session(request.session_id)
 28 |     if not session or session.data is None:
 29 |         raise HTTPException(status_code=404, detail="Session not found")
 30 |
 31 |     try:
 32 |         # Select appropriate engine based on task
 33 |         if request.task == "classification":
 34 |             engine = ClassificationEngine(request.algorithm)
 35 |         elif request.task == "regression":
 36 |             engine = RegressionEngine(request.algorithm)
 37 |         elif request.task == "clustering":
 38 |             engine = ClusteringEngine(request.algorithm)
 39 |         elif request.task == "dimensionality":
 40 |             engine = DimensionalityReductionEngine(request.algorithm)
 41 |         else:
 42 |             raise ValueError(f"Unknown task: {request.task}")
 43 |
 44 |         # Register engine if not already registered
 45 |         if not registry.get(engine.name):
 46 |             await registry.register(engine)
 47 |
 48 |         # Prepare data
 49 |         df = session.data
 50 |
 51 |         if request.task in ["classification", "regression"]:
 52 |             # Supervised learning
 53 |             X = df[request.features]
 54 |             y = df[request.target]
 55 |
 56 |             # Handle categorical variables
 57 |             X = pd.get_dummies(X)
 58 |
 59 |             # Split data
 60 |             from sklearn.model_selection import train_test_split
 61 |             X_train, X_test, y_train, y_test = train_test_split(
 62 |                 X, y, test_size=1 - request.train_test_split, random_state=42
 63 |             )
 64 |
 65 |             # Train model
 66 |             result = await engine.train(X_train, y_train, **request.options)
 67 |
 68 |             # Evaluate on test set
 69 |             evaluation = await engine.evaluate(X_test, y_test)
 70 |             result.update(evaluation)
 71 |
 72 |         else:
 73 |             # Unsupervised learning
 74 |             X = df[request.features]
 75 |             X = pd.get_dummies(X)
 76 |
 77 |             result = await engine.execute(X, request.options)
 78 |
 79 |         # Store model in session
 80 |         import uuid
 81 |         model_id = str(uuid.uuid4())
 82 |         session.add_model(model_id, engine)
 83 |
 84 |         return MLResult(
 85 |             model_id=model_id,
 86 |             task=request.task,
 87 |             algorithm=request.algorithm,
 88 |             metrics=result.get("metrics", {}),
 89 |             feature_importance=result.get("feature_importance"),
 90 |             confusion_matrix=result.get("confusion_matrix"),
 91 |             execution_time=result.get("execution_time", 0)
 92 |         )
 93 |
 94 |     except Exception as e:
 95 |         raise HTTPException(status_code=500, detail=str(e))
 96 |
 97 |
 98 | @router.post("/predict")
 99 | async def predict(
100 |         request: Dict[str, Any],
101 |         session_manager: SessionManager = Depends(get_session_manager)
102 | ):
103 |     """Make predictions with trained model"""
104 |     session = session_manager.get_session(request["session_id"])
105 |     if not session:
106 |         raise HTTPException(status_code=404, detail="Session not found")
107 |
108 |     model_id = request["model_id"]
109 |     if model_id not in session.models:
110 |         raise HTTPException(status_code=404, detail="Model not found")
111 |
112 |     try:
113 |         engine = session.models[model_id]
114 |         data = pd.DataFrame(request["data"])
115 |
116 |         predictions = await engine.predict(data)
117 |
118 |         return {
119 |             "predictions": predictions.tolist() if hasattr(predictions, 'tolist') else predictions,
120 |             "model_id": model_id
121 |         }
122 |
123 |     except Exception as e:
124 |         raise HTTPException(status_code=500, detail=str(e))
125 |
126 |
127 | @router.post("/evaluate")
128 | async def evaluate_model(
129 |         request: Dict[str, Any],
130 |         session_manager: SessionManager = Depends(get_session_manager)
131 | ):
132 |     """Evaluate model performance"""
133 |     session = session_manager.get_session(request["session_id"])
134 |     if not session:
135 |         raise HTTPException(status_code=404, detail="Session not found")
136 |
137 |     model_id = request["model_id"]
138 |     if model_id not in session.models:
139 |         raise HTTPException(status_code=404, detail="Model not found")
140 |
141 |     try:
142 |         engine = session.models[model_id]
143 |         X = pd.DataFrame(request["X"])
144 |         y = request["y"]
145 |
146 |         evaluation = await engine.evaluate(X, y)
147 |
148 |         return evaluation
149 |
150 |     except Exception as e:
151 |         raise HTTPException(status_code=500, detail=str(e))
152 |
153 |
154 | @router.get("/models")
155 | async def list_available_models():
156 |     """List all available ML models"""
157 |     return {
158 |         "classification": [
159 |             "logistic", "svm", "random_forest", "xgboost",
160 |             "neural_network", "knn", "naive_bayes", "decision_tree"
161 |         ],
162 |         "regression": [
163 |             "linear", "ridge", "lasso", "elastic_net",
164 |             "svr", "random_forest_regressor", "xgboost_regressor"
165 |         ],
166 |         "clustering": [
167 |             "kmeans", "dbscan", "hierarchical",
168 |             "gaussian_mixture", "mean_shift", "spectral"
169 |         ],
170 |         "dimensionality_reduction": [
171 |             "pca", "tsne", "umap", "lda", "ica"
172 |         ],
173 |         "deep_learning": [
174 |             "mlp", "cnn", "rnn", "lstm", "transformer"
175 |         ]
176 |     }
177 |
178 |
179 | @router.get("/models/{model_id}")
180 | async def get_model_info(
181 |         model_id: str,
182 |         session_id: str,
183 |         session_manager: SessionManager = Depends(get_session_manager)
184 | ):
185 |     """Get information about a specific model"""
186 |     session = session_manager.get_session(session_id)
187 |     if not session:
188 |         raise HTTPException(status_code=404, detail="Session not found")
189 |
190 |     if model_id not in session.models:
191 |         raise HTTPException(status_code=404, detail="Model not found")
192 |
193 |     model = session.models[model_id]
194 |
195 |     return {
196 |         "model_id": model_id,
197 |         "type": model.model_type,
198 |         "trained": model.is_trained,
199 |         "parameters": model.get_params() if hasattr(model, 'get_params') else {}
200 |     }
201 |


--------------------------------------------------------------------------------
/backend/app/api/v1/models.py:
--------------------------------------------------------------------------------
  1 | from fastapi import APIRouter, HTTPException, Request, Depends
  2 | from typing import Dict, Any, List, Optional
  3 | from app.core.session_manager import SessionManager
  4 | from app.core.registry import EngineRegistry
  5 | from app.engines.ml.model_hub import ModelHubEngine, PretrainedModelRegistry
  6 |
  7 | router = APIRouter()
  8 |
  9 |
 10 | def get_session_manager(request: Request) -> SessionManager:
 11 |     return request.app.state.session_manager
 12 |
 13 |
 14 | def get_registry(request: Request) -> EngineRegistry:
 15 |     return request.app.state.registry
 16 |
 17 |
 18 | @router.get("/available")
 19 | async def list_available_models():
 20 |     """List all available pretrained models"""
 21 |     return PretrainedModelRegistry.get_all_models()
 22 |
 23 |
 24 | @router.get("/search")
 25 | async def search_models(
 26 |         q: str,
 27 |         source: str = "huggingface",
 28 |         task: Optional[str] = None
 29 | ):
 30 |     """Search for models"""
 31 |     if task:
 32 |         models = PretrainedModelRegistry.get_models_by_task(task)
 33 |         # Filter by search query
 34 |         models = [m for m in models if q.lower() in m.lower()]
 35 |     else:
 36 |         # Return all models matching query
 37 |         models = []
 38 |         for category in PretrainedModelRegistry.MODELS.values():
 39 |             for task_models in category.values():
 40 |                 models.extend([m for m in task_models if q.lower() in m.lower()])
 41 |
 42 |     return {
 43 |         "query": q,
 44 |         "source": source,
 45 |         "task": task,
 46 |         "results": models
 47 |     }
 48 |
 49 |
 50 | @router.post("/load")
 51 | async def load_model(
 52 |         request: Dict[str, Any],
 53 |         session_manager: SessionManager = Depends(get_session_manager),
 54 |         registry: EngineRegistry = Depends(get_registry)
 55 | ):
 56 |     """Load a pretrained model from model hub"""
 57 |     session_id = request.get("session_id")
 58 |     model_id = request.get("model_id")
 59 |     source = request.get("source", "huggingface")
 60 |     task = request.get("task")
 61 |
 62 |     session = session_manager.get_session(session_id) if session_id else None
 63 |
 64 |     try:
 65 |         # Create model hub engine
 66 |         engine = ModelHubEngine(source)
 67 |
 68 |         # Register engine
 69 |         await registry.register(engine)
 70 |
 71 |         # Load model
 72 |         result = await engine.load_model(model_id, task)
 73 |
 74 |         # Store in session if available
 75 |         if session:
 76 |             import uuid
 77 |             internal_model_id = str(uuid.uuid4())
 78 |             session.add_model(internal_model_id, engine)
 79 |             result["internal_model_id"] = internal_model_id
 80 |
 81 |         return result
 82 |
 83 |     except Exception as e:
 84 |         raise HTTPException(status_code=500, detail=str(e))
 85 |
 86 |
 87 | @router.post("/inference")
 88 | async def model_inference(
 89 |         request: Dict[str, Any],
 90 |         session_manager: SessionManager = Depends(get_session_manager)
 91 | ):
 92 |     """Run inference with a loaded model"""
 93 |     session_id = request.get("session_id")
 94 |     model_id = request.get("model_id")
 95 |     input_data = request.get("input")
 96 |
 97 |     session = session_manager.get_session(session_id)
 98 |     if not session:
 99 |         raise HTTPException(status_code=404, detail="Session not found")
100 |
101 |     if model_id not in session.models:
102 |         raise HTTPException(status_code=404, detail="Model not found in session")
103 |
104 |     try:
105 |         engine = session.models[model_id]
106 |
107 |         # Run inference
108 |         predictions = await engine.predict(input_data)
109 |
110 |         return {
111 |             "model_id": model_id,
112 |             "predictions": predictions
113 |         }
114 |
115 |     except Exception as e:
116 |         raise HTTPException(status_code=500, detail=str(e))
117 |
118 |
119 | @router.post("/finetune")
120 | async def finetune_model(
121 |         request: Dict[str, Any],
122 |         session_manager: SessionManager = Depends(get_session_manager)
123 | ):
124 |     """Fine-tune a pretrained model"""
125 |     session_id = request.get("session_id")
126 |     model_id = request.get("model_id")
127 |     training_data = request.get("training_data")
128 |     training_params = request.get("params", {})
129 |
130 |     session = session_manager.get_session(session_id)
131 |     if not session:
132 |         raise HTTPException(status_code=404, detail="Session not found")
133 |
134 |     if model_id not in session.models:
135 |         raise HTTPException(status_code=404, detail="Model not found in session")
136 |
137 |     try:
138 |         engine = session.models[model_id]
139 |
140 |         # Fine-tune model
141 |         result = await engine.train(
142 |             training_data.get("X"),
143 |             training_data.get("y"),
144 |             **training_params
145 |         )
146 |
147 |         return {
148 |             "model_id": model_id,
149 |             "status": "fine-tuned",
150 |             "results": result
151 |         }
152 |
153 |     except Exception as e:
154 |         raise HTTPException(status_code=500, detail=str(e))
155 |
156 |
157 | @router.get("/hub/{source}")
158 | async def get_hub_info(source: str):
159 |     """Get information about a specific model hub"""
160 |     hub_info = {
161 |         "huggingface": {
162 |             "name": "Hugging Face Model Hub",
163 |             "url": "https://huggingface.co/models",
164 |             "description": "The largest hub of ready-to-use models for NLP, computer vision, audio, and multimodal tasks",
165 |             "models_count": "350,000+",
166 |             "supported_tasks": [
167 |                 "text-classification", "token-classification", "question-answering",
168 |                 "summarization", "translation", "text-generation", "fill-mask",
169 |                 "image-classification", "object-detection", "image-segmentation",
170 |                 "audio-classification", "automatic-speech-recognition", "text-to-speech"
171 |             ]
172 |         },
173 |         "tensorflow_hub": {
174 |             "name": "TensorFlow Hub",
175 |             "url": "https://tfhub.dev",
176 |             "description": "Repository of trained machine learning models ready for fine-tuning and deployable anywhere",
177 |             "models_count": "3,000+",
178 |             "supported_tasks": [
179 |                 "image-classification", "object-detection", "image-segmentation",
180 |                 "text-embedding", "text-classification", "video-classification"
181 |             ]
182 |         },
183 |         "torch_hub": {
184 |             "name": "PyTorch Hub",
185 |             "url": "https://pytorch.org/hub",
186 |             "description": "Pre-trained models designed to facilitate research reproducibility",
187 |             "models_count": "500+",
188 |             "supported_tasks": [
189 |                 "image-classification", "object-detection", "semantic-segmentation",
190 |                 "image-generation", "nlp", "audio", "generative"
191 |             ]
192 |         }
193 |     }
194 |
195 |     if source not in hub_info:
196 |         raise HTTPException(status_code=404, detail=f"Unknown model hub: {source}")
197 |
198 |     return hub_info[source]
199 |


--------------------------------------------------------------------------------
/backend/app/api/v1/statistics.py:
--------------------------------------------------------------------------------
  1 | from fastapi import APIRouter, Depends, HTTPException, Request
  2 | from typing import Dict, Any, List
  3 | from app.core.session_manager import SessionManager
  4 | from app.core.registry import EngineRegistry
  5 | from app.core.base import AnalysisRequest, AnalysisResult, EngineType
  6 | from app.engines.statistical.descriptive import DescriptiveAnalyzer, FrequencyAnalyzer
  7 | from app.engines.statistical.inferential import TTestAnalyzer, ANOVAAnalyzer
  8 |
  9 | router = APIRouter()
 10 |
 11 |
 12 | def get_session_manager(request: Request) -> SessionManager:
 13 |     return request.app.state.session_manager
 14 |
 15 |
 16 | def get_registry(request: Request) -> EngineRegistry:
 17 |     return request.app.state.registry
 18 |
 19 |
 20 | @router.post("/descriptive", response_model=AnalysisResult)
 21 | async def descriptive_statistics(
 22 |         request: AnalysisRequest,
 23 |         session_manager: SessionManager = Depends(get_session_manager),
 24 |         registry: EngineRegistry = Depends(get_registry)
 25 | ):
 26 |     """Calculate descriptive statistics"""
 27 |     session = session_manager.get_session(request.session_id)
 28 |     if not session or session.data is None:
 29 |         raise HTTPException(status_code=404, detail="Session or data not found")
 30 |
 31 |     analyzer = registry.get("descriptive_statistics")
 32 |     if not analyzer:
 33 |         analyzer = DescriptiveAnalyzer()
 34 |         await registry.register(analyzer)
 35 |
 36 |     result = await analyzer.execute(session.data, request.options)
 37 |
 38 |     if result.success:
 39 |         session.add_result("descriptive", result.data)
 40 |         return AnalysisResult(
 41 |             test_name="Descriptive Statistics",
 42 |             results=result.data,
 43 |             execution_time=0.0
 44 |         )
 45 |     else:
 46 |         raise HTTPException(status_code=400, detail=result.error)
 47 |
 48 |
 49 | @router.post("/frequency", response_model=AnalysisResult)
 50 | async def frequency_distribution(
 51 |         request: AnalysisRequest,
 52 |         session_manager: SessionManager = Depends(get_session_manager),
 53 |         registry: EngineRegistry = Depends(get_registry)
 54 | ):
 55 |     """Calculate frequency distributions"""
 56 |     session = session_manager.get_session(request.session_id)
 57 |     if not session or session.data is None:
 58 |         raise HTTPException(status_code=404, detail="Session or data not found")
 59 |
 60 |     analyzer = registry.get("frequency_distribution")
 61 |     if not analyzer:
 62 |         analyzer = FrequencyAnalyzer()
 63 |         await registry.register(analyzer)
 64 |
 65 |     result = await analyzer.execute(session.data, request.options)
 66 |
 67 |     if result.success:
 68 |         session.add_result("frequency", result.data)
 69 |         return AnalysisResult(
 70 |             test_name="Frequency Distribution",
 71 |             results=result.data,
 72 |             execution_time=0.0
 73 |         )
 74 |     else:
 75 |         raise HTTPException(status_code=400, detail=result.error)
 76 |
 77 |
 78 | @router.post("/ttest", response_model=AnalysisResult)
 79 | async def t_test(
 80 |         request: AnalysisRequest,
 81 |         session_manager: SessionManager = Depends(get_session_manager),
 82 |         registry: EngineRegistry = Depends(get_registry)
 83 | ):
 84 |     """Perform T-test"""
 85 |     session = session_manager.get_session(request.session_id)
 86 |     if not session or session.data is None:
 87 |         raise HTTPException(status_code=404, detail="Session or data not found")
 88 |
 89 |     analyzer = registry.get("ttest")
 90 |     if not analyzer:
 91 |         analyzer = TTestAnalyzer()
 92 |         await registry.register(analyzer)
 93 |
 94 |     result = await analyzer.execute(session.data, request.options)
 95 |
 96 |     if result.success:
 97 |         session.add_result("ttest", result.data)
 98 |         return AnalysisResult(
 99 |             test_name="T-Test",
100 |             results=result.data,
101 |             execution_time=0.0
102 |         )
103 |     else:
104 |         raise HTTPException(status_code=400, detail=result.error)
105 |
106 |
107 | @router.post("/anova", response_model=AnalysisResult)
108 | async def anova(
109 |         request: AnalysisRequest,
110 |         session_manager: SessionManager = Depends(get_session_manager),
111 |         registry: EngineRegistry = Depends(get_registry)
112 | ):
113 |     """Perform ANOVA"""
114 |     session = session_manager.get_session(request.session_id)
115 |     if not session or session.data is None:
116 |         raise HTTPException(status_code=404, detail="Session or data not found")
117 |
118 |     analyzer = registry.get("anova")
119 |     if not analyzer:
120 |         analyzer = ANOVAAnalyzer()
121 |         await registry.register(analyzer)
122 |
123 |     result = await analyzer.execute(session.data, request.options)
124 |
125 |     if result.success:
126 |         session.add_result("anova", result.data)
127 |         return AnalysisResult(
128 |             test_name="ANOVA",
129 |             results=result.data,
130 |             execution_time=0.0
131 |         )
132 |     else:
133 |         raise HTTPException(status_code=400, detail=result.error)
134 |
135 |
136 | @router.get("/available")
137 | async def list_available_tests(registry: EngineRegistry = Depends(get_registry)):
138 |     """List all available statistical tests"""
139 |     engines = registry.get_by_type(EngineType.STATISTICAL)
140 |     return {
141 |         "tests": [engine.name for engine in engines],
142 |         "categories": {
143 |             "descriptive": ["basic_statistics", "frequency_distribution", "crosstabulation"],
144 |             "inferential": ["ttest", "anova", "chi_square", "correlation"],
145 |             "regression": ["linear", "logistic", "polynomial", "ridge", "lasso"],
146 |             "nonparametric": ["mann_whitney", "wilcoxon", "kruskal_wallis", "friedman"],
147 |             "multivariate": ["pca", "factor_analysis", "cluster", "discriminant"],
148 |             "timeseries": ["arima", "seasonal_decomposition", "forecasting"]
149 |         }
150 |     }
151 |


--------------------------------------------------------------------------------
/backend/app/api/v2/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/api/v2/__init__.py


--------------------------------------------------------------------------------
/backend/app/config.py:
--------------------------------------------------------------------------------
 1 | from pydantic_settings import BaseSettings
 2 | from typing import Optional, List
 3 | from functools import lru_cache
 4 | import os
 5 |
 6 |
 7 | class Settings(BaseSettings):
 8 |     # Application
 9 |     APP_NAME: str = "OpenStatica"
10 |     APP_VERSION: str = "1.0.0"
11 |     DEBUG: bool = False
12 |
13 |     # API
14 |     API_V1_PREFIX: str = "/api/v1"
15 |     API_V2_PREFIX: str = "/api/v2"
16 |
17 |     # Security
18 |     SECRET_KEY: str = os.getenv("SECRET_KEY", "your-secret-key-here")
19 |     ALGORITHM: str = "HS256"
20 |     ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
21 |
22 |     # Database
23 |     DATABASE_URL: Optional[str] = os.getenv("DATABASE_URL")
24 |     REDIS_URL: Optional[str] = os.getenv("REDIS_URL", "redis://localhost:6379")
25 |
26 |     # Storage
27 |     UPLOAD_PATH: str = "./uploads"
28 |     MAX_UPLOAD_SIZE: int = 100 * 1024 * 1024  # 100MB
29 |     ALLOWED_EXTENSIONS: List[str] = [".csv", ".xlsx", ".xls", ".json", ".parquet"]
30 |
31 |     # Compute
32 |     ENABLE_GPU: bool = False
33 |     MAX_WORKERS: int = 4
34 |     COMPUTATION_TIMEOUT: int = 300  # seconds
35 |
36 |     # ML Features
37 |     ENABLE_ML: bool = True
38 |     MODEL_CACHE_PATH: str = "./models"
39 |     HUGGINGFACE_TOKEN: Optional[str] = os.getenv("HUGGINGFACE_TOKEN")
40 |
41 |     # Plugins
42 |     PLUGIN_PATH: str = "./plugins"
43 |     ENABLE_PLUGINS: bool = True
44 |
45 |     class Config:
46 |         env_file = ".env"
47 |         case_sensitive = True
48 |
49 |
50 | @lru_cache()
51 | def get_settings() -> Settings:
52 |     return Settings()
53 |


--------------------------------------------------------------------------------
/backend/app/core/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/core/__init__.py


--------------------------------------------------------------------------------
/backend/app/core/base.py:
--------------------------------------------------------------------------------
  1 | from abc import ABC, abstractmethod
  2 | from typing import Any, Dict, List, Optional, TypeVar, Generic
  3 | from pydantic import BaseModel
  4 | import asyncio
  5 | import numpy as np
  6 | import pandas as pd
  7 | from enum import Enum
  8 |
  9 | T = TypeVar('T')
 10 |
 11 |
 12 | class EngineType(str, Enum):
 13 |     """Types of computation engines"""
 14 |     STATISTICAL = "statistical"
 15 |     ML = "ml"
 16 |     DEEP_LEARNING = "deep_learning"
 17 |     VISUALIZATION = "visualization"
 18 |     PREPROCESSING = "preprocessing"
 19 |
 20 |
 21 | class ComputeBackend(str, Enum):
 22 |     """Computation backends"""
 23 |     LOCAL = "local"
 24 |     DISTRIBUTED = "distributed"
 25 |     GPU = "gpu"
 26 |     CLOUD = "cloud"
 27 |
 28 |
 29 | class BaseEngine(ABC):
 30 |     """Base class for all computation engines"""
 31 |
 32 |     def __init__(self, name: str, engine_type: EngineType):
 33 |         self.name = name
 34 |         self.engine_type = engine_type
 35 |         self.backend = ComputeBackend.LOCAL
 36 |         self._initialized = False
 37 |
 38 |     async def initialize(self) -> None:
 39 |         """Initialize the engine"""
 40 |         if not self._initialized:
 41 |             await self._setup()
 42 |             self._initialized = True
 43 |
 44 |     @abstractmethod
 45 |     async def _setup(self) -> None:
 46 |         """Setup engine resources"""
 47 |         pass
 48 |
 49 |     @abstractmethod
 50 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
 51 |         """Execute computation"""
 52 |         pass
 53 |
 54 |     async def cleanup(self) -> None:
 55 |         """Cleanup resources"""
 56 |         pass
 57 |
 58 |     def validate_input(self, data: Any) -> bool:
 59 |         """Validate input data"""
 60 |         return True
 61 |
 62 |
 63 | class BaseAnalyzer(BaseEngine):
 64 |     """Base class for statistical analyzers"""
 65 |
 66 |     def __init__(self, name: str):
 67 |         super().__init__(name, EngineType.STATISTICAL)
 68 |
 69 |     async def _setup(self) -> None:
 70 |         """Setup analyzer"""
 71 |         pass
 72 |
 73 |     def prepare_data(self, data: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
 74 |         """Prepare data for analysis"""
 75 |         return data[columns].dropna()
 76 |
 77 |     def calculate_effect_size(self, *args, **kwargs) -> float:
 78 |         """Calculate effect size for the analysis"""
 79 |         raise NotImplementedError
 80 |
 81 |
 82 | class BaseMLModel(BaseEngine):
 83 |     """Base class for ML models"""
 84 |
 85 |     def __init__(self, name: str, model_type: str):
 86 |         super().__init__(name, EngineType.ML)
 87 |         self.model_type = model_type
 88 |         self.model = None
 89 |         self.is_trained = False
 90 |
 91 |     @abstractmethod
 92 |     async def train(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
 93 |         """Train the model"""
 94 |         pass
 95 |
 96 |     @abstractmethod
 97 |     async def predict(self, X: Any, **kwargs) -> Any:
 98 |         """Make predictions"""
 99 |         pass
100 |
101 |     @abstractmethod
102 |     async def evaluate(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
103 |         """Evaluate model performance"""
104 |         pass
105 |
106 |     async def save(self, path: str) -> None:
107 |         """Save model to disk"""
108 |         pass
109 |
110 |     async def load(self, path: str) -> None:
111 |         """Load model from disk"""
112 |         pass
113 |
114 |
115 | class BasePlugin(ABC):
116 |     """Base class for plugins"""
117 |
118 |     def __init__(self, name: str, version: str):
119 |         self.name = name
120 |         self.version = version
121 |         self.enabled = True
122 |
123 |     @abstractmethod
124 |     async def initialize(self) -> None:
125 |         """Initialize plugin"""
126 |         pass
127 |
128 |     @abstractmethod
129 |     def get_engines(self) -> List[BaseEngine]:
130 |         """Get engines provided by this plugin"""
131 |         pass
132 |
133 |     @abstractmethod
134 |     def get_routes(self) -> List[Any]:
135 |         """Get API routes provided by this plugin"""
136 |         pass
137 |
138 |
139 | class Result(BaseModel, Generic[T]):
140 |     """Generic result wrapper"""
141 |     success: bool
142 |     data: Optional[T] = None
143 |     error: Optional[str] = None
144 |     metadata: Dict[str, Any] = {}
145 |
146 |     @classmethod
147 |     def ok(cls, data: T, **metadata) -> "Result[T]":
148 |         return cls(success=True, data=data, metadata=metadata)
149 |
150 |     @classmethod
151 |     def fail(cls, error: str, **metadata) -> "Result[T]":
152 |         return cls(success=False, error=error, metadata=metadata)
153 |
154 |
155 | class DataInfo(BaseModel):
156 |     """Information about uploaded data"""
157 |     rows: int
158 |     columns: int
159 |     column_names: List[str]
160 |     column_types: Dict[str, str]
161 |     numeric_columns: List[str]
162 |     categorical_columns: List[str]
163 |     datetime_columns: List[str]
164 |     missing_values: Dict[str, int]
165 |     memory_usage: float  # in MB
166 |
167 |
168 | class AnalysisRequest(BaseModel):
169 |     """Base request for analysis"""
170 |     session_id: str
171 |     columns: List[str]
172 |     options: Dict[str, Any] = {}
173 |     backend: ComputeBackend = ComputeBackend.LOCAL
174 |
175 |
176 | class AnalysisResult(BaseModel):
177 |     """Base result for analysis"""
178 |     test_name: str
179 |     results: Dict[str, Any]
180 |     visualizations: Optional[List[Dict[str, Any]]] = None
181 |     interpretation: Optional[str] = None
182 |     execution_time: float
183 |


--------------------------------------------------------------------------------
/backend/app/core/plugin_manager.py:
--------------------------------------------------------------------------------
  1 | import os
  2 | import importlib.util
  3 | import logging
  4 | from typing import List, Dict, Any, Optional
  5 | from pathlib import Path
  6 | from app.core.base import BasePlugin, BaseEngine
  7 |
  8 | logger = logging.getLogger(__name__)
  9 |
 10 |
 11 | class PluginManager:
 12 |     """Manages loading and lifecycle of plugins"""
 13 |
 14 |     def __init__(self, plugin_path: str):
 15 |         self.plugin_path = Path(plugin_path)
 16 |         self.plugins: Dict[str, BasePlugin] = {}
 17 |         self.enabled_plugins: List[str] = []
 18 |
 19 |     async def load_plugins(self) -> None:
 20 |         """Load all plugins from plugin directory"""
 21 |         if not self.plugin_path.exists():
 22 |             logger.warning(f"Plugin directory {self.plugin_path} does not exist")
 23 |             return
 24 |
 25 |         for plugin_dir in self.plugin_path.iterdir():
 26 |             if plugin_dir.is_dir() and not plugin_dir.name.startswith('_'):
 27 |                 try:
 28 |                     await self.load_plugin(plugin_dir.name)
 29 |                 except Exception as e:
 30 |                     logger.error(f"Failed to load plugin {plugin_dir.name}: {e}")
 31 |
 32 |     async def load_plugin(self, plugin_name: str) -> None:
 33 |         """Load a specific plugin"""
 34 |         plugin_path = self.plugin_path / plugin_name
 35 |
 36 |         # Check for plugin.json metadata
 37 |         metadata_path = plugin_path / "plugin.json"
 38 |         if metadata_path.exists():
 39 |             import json
 40 |             with open(metadata_path) as f:
 41 |                 metadata = json.load(f)
 42 |         else:
 43 |             metadata = {"name": plugin_name, "version": "0.0.1"}
 44 |
 45 |         # Load main module
 46 |         main_module_path = plugin_path / "__init__.py"
 47 |         if not main_module_path.exists():
 48 |             main_module_path = plugin_path / "main.py"
 49 |
 50 |         if not main_module_path.exists():
 51 |             raise FileNotFoundError(f"No main module found for plugin {plugin_name}")
 52 |
 53 |         # Import plugin module
 54 |         spec = importlib.util.spec_from_file_location(
 55 |             f"plugins.{plugin_name}",
 56 |             main_module_path
 57 |         )
 58 |         module = importlib.util.module_from_spec(spec)
 59 |         spec.loader.exec_module(module)
 60 |
 61 |         # Get plugin class
 62 |         if hasattr(module, 'Plugin'):
 63 |             PluginClass = module.Plugin
 64 |         else:
 65 |             # Try to find a class that inherits from BasePlugin
 66 |             for name, obj in module.__dict__.items():
 67 |                 if isinstance(obj, type) and issubclass(obj, BasePlugin) and obj != BasePlugin:
 68 |                     PluginClass = obj
 69 |                     break
 70 |             else:
 71 |                 raise ValueError(f"No Plugin class found in {plugin_name}")
 72 |
 73 |         # Instantiate plugin
 74 |         plugin = PluginClass(
 75 |             name=metadata.get("name", plugin_name),
 76 |             version=metadata.get("version", "0.0.1")
 77 |         )
 78 |
 79 |         # Initialize plugin
 80 |         await plugin.initialize()
 81 |
 82 |         # Store plugin
 83 |         self.plugins[plugin_name] = plugin
 84 |         self.enabled_plugins.append(plugin_name)
 85 |
 86 |         logger.info(f"Loaded plugin: {plugin_name} v{plugin.version}")
 87 |
 88 |     async def unload_plugin(self, plugin_name: str) -> None:
 89 |         """Unload a plugin"""
 90 |         if plugin_name in self.plugins:
 91 |             plugin = self.plugins[plugin_name]
 92 |             # Cleanup if method exists
 93 |             if hasattr(plugin, 'cleanup'):
 94 |                 await plugin.cleanup()
 95 |
 96 |             del self.plugins[plugin_name]
 97 |             self.enabled_plugins.remove(plugin_name)
 98 |
 99 |             logger.info(f"Unloaded plugin: {plugin_name}")
100 |
101 |     def get_plugin(self, plugin_name: str) -> Optional[BasePlugin]:
102 |         """Get a specific plugin"""
103 |         return self.plugins.get(plugin_name)
104 |
105 |     def get_all_engines(self) -> List[BaseEngine]:
106 |         """Get all engines from all plugins"""
107 |         engines = []
108 |         for plugin in self.plugins.values():
109 |             if plugin.enabled:
110 |                 engines.extend(plugin.get_engines())
111 |         return engines
112 |
113 |     def get_all_routes(self) -> List[Any]:
114 |         """Get all API routes from all plugins"""
115 |         routes = []
116 |         for plugin in self.plugins.values():
117 |             if plugin.enabled:
118 |                 routes.extend(plugin.get_routes())
119 |         return routes
120 |
121 |     def list_plugins(self) -> Dict[str, Dict[str, Any]]:
122 |         """List all loaded plugins with metadata"""
123 |         return {
124 |             name: {
125 |                 "version": plugin.version,
126 |                 "enabled": plugin.enabled,
127 |                 "engines": [e.name for e in plugin.get_engines()],
128 |                 "routes": len(plugin.get_routes())
129 |             }
130 |             for name, plugin in self.plugins.items()
131 |         }
132 |
133 |     async def enable_plugin(self, plugin_name: str) -> None:
134 |         """Enable a plugin"""
135 |         if plugin_name in self.plugins:
136 |             self.plugins[plugin_name].enabled = True
137 |             logger.info(f"Enabled plugin: {plugin_name}")
138 |
139 |     async def disable_plugin(self, plugin_name: str) -> None:
140 |         """Disable a plugin"""
141 |         if plugin_name in self.plugins:
142 |             self.plugins[plugin_name].enabled = False
143 |             logger.info(f"Disabled plugin: {plugin_name}")
144 |


--------------------------------------------------------------------------------
/backend/app/core/registry.py:
--------------------------------------------------------------------------------
 1 | from typing import Dict, List, Optional, Type
 2 | from app.core.base import BaseEngine, EngineType
 3 | import logging
 4 |
 5 | logger = logging.getLogger(__name__)
 6 |
 7 |
 8 | class EngineRegistry:
 9 |     """Registry for all computation engines"""
10 |
11 |     def __init__(self):
12 |         self._engines: Dict[str, BaseEngine] = {}
13 |         self._engine_types: Dict[EngineType, List[str]] = {
14 |             engine_type: [] for engine_type in EngineType
15 |         }
16 |
17 |     async def register(self, engine: BaseEngine) -> None:
18 |         """Register a new engine"""
19 |         if engine.name in self._engines:
20 |             logger.warning(f"Engine {engine.name} already registered, overwriting")
21 |
22 |         await engine.initialize()
23 |         self._engines[engine.name] = engine
24 |         self._engine_types[engine.engine_type].append(engine.name)
25 |         logger.info(f"Registered engine: {engine.name} ({engine.engine_type})")
26 |
27 |     def get(self, name: str) -> Optional[BaseEngine]:
28 |         """Get engine by name"""
29 |         return self._engines.get(name)
30 |
31 |     def get_by_type(self, engine_type: EngineType) -> List[BaseEngine]:
32 |         """Get all engines of a specific type"""
33 |         engine_names = self._engine_types.get(engine_type, [])
34 |         return [self._engines[name] for name in engine_names]
35 |
36 |     def list_engines(self) -> Dict[str, List[str]]:
37 |         """List all registered engines by type"""
38 |         return {
39 |             engine_type.value: names
40 |             for engine_type, names in self._engine_types.items()
41 |             if names
42 |         }
43 |
44 |     async def cleanup(self) -> None:
45 |         """Cleanup all engines"""
46 |         for engine in self._engines.values():
47 |             await engine.cleanup()
48 |


--------------------------------------------------------------------------------
/backend/app/core/session_manager.py:
--------------------------------------------------------------------------------
  1 | from typing import Dict, Optional, Any
  2 | import uuid
  3 | import asyncio
  4 | from datetime import datetime, timedelta
  5 | import pandas as pd
  6 | import logging
  7 |
  8 | logger = logging.getLogger(__name__)
  9 |
 10 |
 11 | class Session:
 12 |     """User session with data and state"""
 13 |
 14 |     def __init__(self, session_id: str):
 15 |         self.session_id = session_id
 16 |         self.created_at = datetime.now()
 17 |         self.last_accessed = datetime.now()
 18 |         self.data: Optional[pd.DataFrame] = None
 19 |         self.metadata: Dict[str, Any] = {}
 20 |         self.results: Dict[str, Any] = {}
 21 |         self.models: Dict[str, Any] = {}
 22 |
 23 |     def update_access(self):
 24 |         """Update last access time"""
 25 |         self.last_accessed = datetime.now()
 26 |
 27 |     def set_data(self, data: pd.DataFrame, metadata: Dict[str, Any] = None):
 28 |         """Set session data"""
 29 |         self.data = data
 30 |         if metadata:
 31 |             self.metadata.update(metadata)
 32 |         self.update_access()
 33 |
 34 |     def add_result(self, key: str, result: Any):
 35 |         """Add analysis result"""
 36 |         self.results[key] = result
 37 |         self.update_access()
 38 |
 39 |     def add_model(self, key: str, model: Any):
 40 |         """Add trained model"""
 41 |         self.models[key] = model
 42 |         self.update_access()
 43 |
 44 |
 45 | class SessionManager:
 46 |     """Manage user sessions"""
 47 |
 48 |     def __init__(self, ttl_minutes: int = 60):
 49 |         self._sessions: Dict[str, Session] = {}
 50 |         self._ttl = timedelta(minutes=ttl_minutes)
 51 |         self._cleanup_task = None
 52 |         self._start_cleanup_task()
 53 |
 54 |     def create_session(self) -> str:
 55 |         """Create new session"""
 56 |         session_id = str(uuid.uuid4())
 57 |         self._sessions[session_id] = Session(session_id)
 58 |         logger.info(f"Created session: {session_id}")
 59 |         return session_id
 60 |
 61 |     def get_session(self, session_id: str) -> Optional[Session]:
 62 |         """Get session by ID"""
 63 |         session = self._sessions.get(session_id)
 64 |         if session:
 65 |             session.update_access()
 66 |         return session
 67 |
 68 |     def delete_session(self, session_id: str) -> bool:
 69 |         """Delete session"""
 70 |         if session_id in self._sessions:
 71 |             del self._sessions[session_id]
 72 |             logger.info(f"Deleted session: {session_id}")
 73 |             return True
 74 |         return False
 75 |
 76 |     def _start_cleanup_task(self):
 77 |         """Start background cleanup task"""
 78 |         self._cleanup_task = asyncio.create_task(self._cleanup_loop())
 79 |
 80 |     async def _cleanup_loop(self):
 81 |         """Periodically cleanup expired sessions"""
 82 |         while True:
 83 |             await asyncio.sleep(300)  # Check every 5 minutes
 84 |             await self._cleanup_expired()
 85 |
 86 |     async def _cleanup_expired(self):
 87 |         """Remove expired sessions"""
 88 |         now = datetime.now()
 89 |         expired = []
 90 |
 91 |         for session_id, session in self._sessions.items():
 92 |             if now - session.last_accessed > self._ttl:
 93 |                 expired.append(session_id)
 94 |
 95 |         for session_id in expired:
 96 |             self.delete_session(session_id)
 97 |
 98 |         if expired:
 99 |             logger.info(f"Cleaned up {len(expired)} expired sessions")
100 |
101 |     async def cleanup(self):
102 |         """Cleanup manager"""
103 |         if self._cleanup_task:
104 |             self._cleanup_task.cancel()
105 |         self._sessions.clear()
106 |


--------------------------------------------------------------------------------
/backend/app/engines/compute/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/engines/compute/__init__.py


--------------------------------------------------------------------------------
/backend/app/engines/compute/distributed.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Distributed Compute Backend for OpenStatica
  3 | Handles distributed computation across multiple nodes
  4 | """
  5 |
  6 | from typing import Any, Dict, List, Optional, Callable
  7 | import asyncio
  8 | import logging
  9 | from app.core.base import ComputeBackend
 10 |
 11 | logger = logging.getLogger(__name__)
 12 |
 13 |
 14 | class DistributedComputeBackend:
 15 |     """Distributed computation backend (Dask/Ray integration)"""
 16 |
 17 |     def __init__(self, cluster_address: Optional[str] = None):
 18 |         self.cluster_address = cluster_address
 19 |         self.backend_type = ComputeBackend.DISTRIBUTED
 20 |         self.client = None
 21 |         self._initialize_backend()
 22 |
 23 |     def _initialize_backend(self):
 24 |         """Initialize distributed backend"""
 25 |         try:
 26 |             # Try Dask first
 27 |             from dask.distributed import Client
 28 |             if self.cluster_address:
 29 |                 self.client = Client(self.cluster_address)
 30 |             else:
 31 |                 self.client = Client(n_workers=2, threads_per_worker=2)
 32 |             self.backend_name = 'dask'
 33 |             logger.info("Initialized Dask distributed backend")
 34 |         except ImportError:
 35 |             try:
 36 |                 # Fallback to Ray
 37 |                 import ray
 38 |                 if not ray.is_initialized():
 39 |                     ray.init(address=self.cluster_address)
 40 |                 self.client = ray
 41 |                 self.backend_name = 'ray'
 42 |                 logger.info("Initialized Ray distributed backend")
 43 |             except ImportError:
 44 |                 logger.warning("No distributed backend available, falling back to local")
 45 |                 from .local import LocalComputeBackend
 46 |                 self.fallback = LocalComputeBackend()
 47 |                 self.backend_name = 'local_fallback'
 48 |
 49 |     async def execute(self, func: Callable, *args, **kwargs) -> Any:
 50 |         """Execute function on distributed backend"""
 51 |         if self.backend_name == 'dask':
 52 |             future = self.client.submit(func, *args, **kwargs)
 53 |             return await asyncio.wrap_future(future)
 54 |         elif self.backend_name == 'ray':
 55 |             remote_func = self.client.remote(func)
 56 |             return await asyncio.wrap_future(remote_func.remote(*args, **kwargs))
 57 |         else:
 58 |             return await self.fallback.execute(func, *args, **kwargs)
 59 |
 60 |     async def map(self, func: Callable, iterable: List) -> List:
 61 |         """Distributed map operation"""
 62 |         if self.backend_name == 'dask':
 63 |             futures = self.client.map(func, iterable)
 64 |             return await asyncio.gather(*[asyncio.wrap_future(f) for f in futures])
 65 |         elif self.backend_name == 'ray':
 66 |             remote_func = self.client.remote(func)
 67 |             futures = [remote_func.remote(item) for item in iterable]
 68 |             return await asyncio.gather(*[asyncio.wrap_future(f) for f in futures])
 69 |         else:
 70 |             return await self.fallback.map(func, iterable)
 71 |
 72 |     async def scatter(self, data: Any) -> Any:
 73 |         """Scatter data across cluster"""
 74 |         if self.backend_name == 'dask':
 75 |             return await asyncio.wrap_future(self.client.scatter(data))
 76 |         elif self.backend_name == 'ray':
 77 |             return self.client.put(data)
 78 |         else:
 79 |             return data
 80 |
 81 |     def cleanup(self):
 82 |         """Cleanup distributed resources"""
 83 |         if self.backend_name == 'dask' and self.client:
 84 |             self.client.close()
 85 |         elif self.backend_name == 'ray':
 86 |             import ray
 87 |             if ray.is_initialized():
 88 |                 ray.shutdown()
 89 |         elif hasattr(self, 'fallback'):
 90 |             self.fallback.cleanup()
 91 |
 92 |     def get_info(self) -> Dict[str, Any]:
 93 |         """Get backend information"""
 94 |         info = {
 95 |             'type': 'distributed',
 96 |             'backend': self.backend_name
 97 |         }
 98 |
 99 |         if self.backend_name == 'dask' and self.client:
100 |             info.update({
101 |                 'workers': len(self.client.nthreads()),
102 |                 'dashboard_link': self.client.dashboard_link
103 |             })
104 |         elif self.backend_name == 'ray':
105 |             import ray
106 |             if ray.is_initialized():
107 |                 info.update({
108 |                     'nodes': len(ray.nodes()),
109 |                     'resources': ray.cluster_resources()
110 |                 })
111 |
112 |         return info
113 |


--------------------------------------------------------------------------------
/backend/app/engines/compute/gpu.py:
--------------------------------------------------------------------------------
  1 | """
  2 | GPU Compute Backend for OpenStatica
  3 | Handles GPU-accelerated computation
  4 | """
  5 |
  6 | from typing import Any, Dict, List, Optional, Callable
  7 | import logging
  8 | import numpy as np
  9 | from app.core.base import ComputeBackend
 10 |
 11 | logger = logging.getLogger(__name__)
 12 |
 13 |
 14 | class GPUComputeBackend:
 15 |     """GPU computation backend using CuPy/PyTorch/TensorFlow"""
 16 |
 17 |     def __init__(self, device_id: Optional[int] = 0):
 18 |         self.device_id = device_id
 19 |         self.backend_type = ComputeBackend.GPU
 20 |         self.backend_lib = None
 21 |         self._initialize_backend()
 22 |
 23 |     def _initialize_backend(self):
 24 |         """Initialize GPU backend"""
 25 |         try:
 26 |             # Try CuPy first (best for numerical computing)
 27 |             import cupy as cp
 28 |             self.backend_lib = cp
 29 |             self.backend_name = 'cupy'
 30 |             self.device = cp.cuda.Device(self.device_id)
 31 |             logger.info(f"Initialized CuPy GPU backend on device {self.device_id}")
 32 |         except ImportError:
 33 |             try:
 34 |                 # Try PyTorch
 35 |                 import torch
 36 |                 if torch.cuda.is_available():
 37 |                     self.backend_lib = torch
 38 |                     self.backend_name = 'pytorch'
 39 |                     self.device = torch.device(f'cuda:{self.device_id}')
 40 |                     logger.info(f"Initialized PyTorch GPU backend on device {self.device_id}")
 41 |                 else:
 42 |                     raise RuntimeError("CUDA not available")
 43 |             except (ImportError, RuntimeError):
 44 |                 try:
 45 |                     # Try TensorFlow
 46 |                     import tensorflow as tf
 47 |                     gpus = tf.config.list_physical_devices('GPU')
 48 |                     if gpus:
 49 |                         self.backend_lib = tf
 50 |                         self.backend_name = 'tensorflow'
 51 |                         tf.config.set_visible_devices(gpus[self.device_id], 'GPU')
 52 |                         logger.info(f"Initialized TensorFlow GPU backend")
 53 |                     else:
 54 |                         raise RuntimeError("No GPU available")
 55 |                 except (ImportError, RuntimeError) as e:
 56 |                     logger.warning(f"No GPU backend available: {e}")
 57 |                     # Fallback to CPU with NumPy
 58 |                     self.backend_lib = np
 59 |                     self.backend_name = 'numpy_cpu'
 60 |                     logger.info("Falling back to NumPy CPU backend")
 61 |
 62 |     def to_gpu(self, data: np.ndarray) -> Any:
 63 |         """Transfer data to GPU"""
 64 |         if self.backend_name == 'cupy':
 65 |             return self.backend_lib.asarray(data)
 66 |         elif self.backend_name == 'pytorch':
 67 |             return self.backend_lib.tensor(data, device=self.device)
 68 |         elif self.backend_name == 'tensorflow':
 69 |             return self.backend_lib.constant(data)
 70 |         else:
 71 |             return data
 72 |
 73 |     def to_cpu(self, data: Any) -> np.ndarray:
 74 |         """Transfer data from GPU to CPU"""
 75 |         if self.backend_name == 'cupy':
 76 |             return data.get()
 77 |         elif self.backend_name == 'pytorch':
 78 |             return data.cpu().numpy()
 79 |         elif self.backend_name == 'tensorflow':
 80 |             return data.numpy()
 81 |         else:
 82 |             return np.asarray(data)
 83 |
 84 |     async def execute(self, func: Callable, data: Any, **kwargs) -> Any:
 85 |         """Execute function on GPU"""
 86 |         # Transfer to GPU
 87 |         gpu_data = self.to_gpu(data)
 88 |
 89 |         # Execute operation
 90 |         if self.backend_name in ['cupy', 'numpy_cpu']:
 91 |             result = func(gpu_data, **kwargs)
 92 |         elif self.backend_name == 'pytorch':
 93 |             with self.backend_lib.cuda.device(self.device_id):
 94 |                 result = func(gpu_data, **kwargs)
 95 |         elif self.backend_name == 'tensorflow':
 96 |             with self.backend_lib.device(f'/GPU:{self.device_id}'):
 97 |                 result = func(gpu_data, **kwargs)
 98 |         else:
 99 |             result = func(gpu_data, **kwargs)
100 |
101 |         # Transfer back to CPU
102 |         return self.to_cpu(result)
103 |
104 |     async def matrix_multiply(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
105 |         """GPU-accelerated matrix multiplication"""
106 |         a_gpu = self.to_gpu(a)
107 |         b_gpu = self.to_gpu(b)
108 |
109 |         if self.backend_name in ['cupy', 'numpy_cpu']:
110 |             result = self.backend_lib.matmul(a_gpu, b_gpu)
111 |         elif self.backend_name == 'pytorch':
112 |             result = self.backend_lib.matmul(a_gpu, b_gpu)
113 |         elif self.backend_name == 'tensorflow':
114 |             result = self.backend_lib.matmul(a_gpu, b_gpu)
115 |         else:
116 |             result = np.matmul(a, b)
117 |
118 |         return self.to_cpu(result)
119 |
120 |     async def compute_statistics(self, data: np.ndarray, operations: List[str]) -> Dict[str, float]:
121 |         """GPU-accelerated statistics computation"""
122 |         gpu_data = self.to_gpu(data)
123 |         results = {}
124 |
125 |         for op in operations:
126 |             if op == 'mean':
127 |                 result = self.backend_lib.mean(gpu_data)
128 |             elif op == 'std':
129 |                 result = self.backend_lib.std(gpu_data)
130 |             elif op == 'var':
131 |                 result = self.backend_lib.var(gpu_data)
132 |             elif op == 'min':
133 |                 result = self.backend_lib.min(gpu_data)
134 |             elif op == 'max':
135 |                 result = self.backend_lib.max(gpu_data)
136 |             else:
137 |                 continue
138 |
139 |             results[op] = float(self.to_cpu(result))
140 |
141 |         return results
142 |
143 |     def cleanup(self):
144 |         """Cleanup GPU resources"""
145 |         if self.backend_name == 'cupy':
146 |             import cupy
147 |             cupy.get_default_memory_pool().free_all_blocks()
148 |         elif self.backend_name == 'pytorch':
149 |             import torch
150 |             torch.cuda.empty_cache()
151 |         elif self.backend_name == 'tensorflow':
152 |             import tensorflow as tf
153 |             tf.keras.backend.clear_session()
154 |
155 |     def get_info(self) -> Dict[str, Any]:
156 |         """Get GPU information"""
157 |         info = {
158 |             'type': 'gpu',
159 |             'backend': self.backend_name,
160 |             'device_id': self.device_id
161 |         }
162 |
163 |         if self.backend_name == 'cupy':
164 |             import cupy
165 |             mempool = cupy.get_default_memory_pool()
166 |             info.update({
167 |                 'device_name': cupy.cuda.Device(self.device_id).name,
168 |                 'memory_used': mempool.used_bytes(),
169 |                 'memory_total': mempool.total_bytes()
170 |             })
171 |         elif self.backend_name == 'pytorch':
172 |             import torch
173 |             if torch.cuda.is_available():
174 |                 info.update({
175 |                     'device_name': torch.cuda.get_device_name(self.device_id),
176 |                     'memory_allocated': torch.cuda.memory_allocated(self.device_id),
177 |                     'memory_reserved': torch.cuda.memory_reserved(self.device_id)
178 |                 })
179 |
180 |         return info
181 |


--------------------------------------------------------------------------------
/backend/app/engines/compute/local.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Local Compute Backend for OpenStatica
 3 | Handles computation on local machine resources
 4 | """
 5 | import asyncio
 6 | import concurrent.futures
 7 | import multiprocessing as mp
 8 | from typing import Any, Dict, List, Callable, Optional
 9 | import numpy as np
10 | import pandas as pd
11 | from app.core.base import ComputeBackend
12 | import logging
13 |
14 | logger = logging.getLogger(__name__)
15 |
16 |
17 | class LocalComputeBackend:
18 |     """Local computation backend using multiprocessing"""
19 |
20 |     def __init__(self, max_workers: Optional[int] = None):
21 |         self.max_workers = max_workers or mp.cpu_count()
22 |         self.executor = concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers)
23 |         self.backend_type = ComputeBackend.LOCAL
24 |
25 |     async def execute(self, func: Callable, *args, **kwargs) -> Any:
26 |         """Execute function asynchronously"""
27 |         loop = asyncio.get_event_loop()
28 |         return await loop.run_in_executor(self.executor, func, *args, **kwargs)
29 |
30 |     async def map(self, func: Callable, iterable: List) -> List:
31 |         """Map function over iterable in parallel"""
32 |         loop = asyncio.get_event_loop()
33 |         futures = [loop.run_in_executor(self.executor, func, item) for item in iterable]
34 |         return await asyncio.gather(*futures)
35 |
36 |     async def reduce(self, func: Callable, iterable: List, initializer=None) -> Any:
37 |         """Reduce operation"""
38 |         from functools import reduce
39 |         loop = asyncio.get_event_loop()
40 |         return await loop.run_in_executor(
41 |             self.executor,
42 |             lambda: reduce(func, iterable, initializer) if initializer else reduce(func, iterable)
43 |         )
44 |
45 |     def cleanup(self):
46 |         """Cleanup resources"""
47 |         self.executor.shutdown(wait=True)
48 |
49 |     async def compute_statistics(self, data: np.ndarray, operations: List[str]) -> Dict[str, Any]:
50 |         """Compute multiple statistics in parallel"""
51 |         stats_funcs = {
52 |             'mean': np.mean,
53 |             'std': np.std,
54 |             'median': np.median,
55 |             'min': np.min,
56 |             'max': np.max,
57 |             'var': np.var,
58 |             'skew': lambda x: float(pd.Series(x).skew()),
59 |             'kurtosis': lambda x: float(pd.Series(x).kurtosis())
60 |         }
61 |
62 |         results = {}
63 |         tasks = []
64 |
65 |         for op in operations:
66 |             if op in stats_funcs:
67 |                 tasks.append(self.execute(stats_funcs[op], data))
68 |
69 |         computed = await asyncio.gather(*tasks)
70 |
71 |         for op, result in zip(operations, computed):
72 |             results[op] = float(result) if not isinstance(result, (list, np.ndarray)) else result
73 |
74 |         return results
75 |
76 |     def get_info(self) -> Dict[str, Any]:
77 |         """Get backend information"""
78 |         return {
79 |             'type': 'local',
80 |             'max_workers': self.max_workers,
81 |             'cpu_count': mp.cpu_count(),
82 |             'backend': 'ProcessPoolExecutor'
83 |         }
84 |


--------------------------------------------------------------------------------
/backend/app/engines/ml/__init__.py:
--------------------------------------------------------------------------------
 1 | from typing import List
 2 | from app.core.registry import EngineRegistry
 3 | from .supervised import ClassificationEngine, RegressionEngine
 4 | from .unsupervised import ClusteringEngine, DimensionalityReductionEngine
 5 |
 6 |
 7 | async def initialize_ml_engines(registry: EngineRegistry) -> List[str]:
 8 |     """
 9 |     Pre-register a small set of commonly used ML engines so they're
10 |     available immediately. API endpoints will still create/register
11 |     ad-hoc engines as needed.
12 |     """
13 |     names = []
14 |     defaults = [
15 |         ClassificationEngine("rf"),
16 |         RegressionEngine("linear"),
17 |         ClusteringEngine("kmeans"),
18 |         DimensionalityReductionEngine("pca"),
19 |     ]
20 |     for engine in defaults:
21 |         if not registry.get(engine.name):
22 |             await registry.register(engine)
23 |             names.append(engine.name)
24 |     return names
25 |


--------------------------------------------------------------------------------
/backend/app/engines/ml/deep_learning.py:
--------------------------------------------------------------------------------
  1 | from typing import Any, Dict, Optional
  2 | import numpy as np
  3 | from app.core.base import BaseMLModel
  4 | import time
  5 |
  6 |
  7 | class DeepLearningEngine(BaseMLModel):
  8 |     """Engine for deep learning models"""
  9 |
 10 |     def __init__(self, model_type: str):
 11 |         super().__init__(f"deep_{model_type}", "deep_learning")
 12 |         self.model_type = model_type
 13 |         self.model = None
 14 |         self.history = None
 15 |
 16 |     async def _setup(self):
 17 |         """Initialize deep learning model"""
 18 |         try:
 19 |             import tensorflow as tf
 20 |             self.framework = "tensorflow"
 21 |         except ImportError:
 22 |             try:
 23 |                 import torch
 24 |                 self.framework = "pytorch"
 25 |             except ImportError:
 26 |                 raise ImportError("No deep learning framework available. Install tensorflow or pytorch.")
 27 |
 28 |         if self.framework == "tensorflow":
 29 |             await self._setup_tensorflow()
 30 |         else:
 31 |             await self._setup_pytorch()
 32 |
 33 |     async def _setup_tensorflow(self):
 34 |         """Setup TensorFlow model"""
 35 |         import tensorflow as tf
 36 |         from tensorflow import keras
 37 |
 38 |         if self.model_type == "mlp":
 39 |             self.model = keras.Sequential([
 40 |                 keras.layers.Dense(128, activation='relu'),
 41 |                 keras.layers.Dropout(0.2),
 42 |                 keras.layers.Dense(64, activation='relu'),
 43 |                 keras.layers.Dropout(0.2),
 44 |                 keras.layers.Dense(32, activation='relu'),
 45 |                 keras.layers.Dense(1)
 46 |             ])
 47 |
 48 |         elif self.model_type == "cnn":
 49 |             self.model = keras.Sequential([
 50 |                 keras.layers.Conv2D(32, (3, 3), activation='relu'),
 51 |                 keras.layers.MaxPooling2D((2, 2)),
 52 |                 keras.layers.Conv2D(64, (3, 3), activation='relu'),
 53 |                 keras.layers.MaxPooling2D((2, 2)),
 54 |                 keras.layers.Conv2D(64, (3, 3), activation='relu'),
 55 |                 keras.layers.Flatten(),
 56 |                 keras.layers.Dense(64, activation='relu'),
 57 |                 keras.layers.Dense(10)
 58 |             ])
 59 |
 60 |         elif self.model_type == "rnn":
 61 |             self.model = keras.Sequential([
 62 |                 keras.layers.SimpleRNN(128, return_sequences=True),
 63 |                 keras.layers.SimpleRNN(64),
 64 |                 keras.layers.Dense(32, activation='relu'),
 65 |                 keras.layers.Dense(1)
 66 |             ])
 67 |
 68 |         elif self.model_type == "lstm":
 69 |             self.model = keras.Sequential([
 70 |                 keras.layers.LSTM(128, return_sequences=True),
 71 |                 keras.layers.LSTM(64),
 72 |                 keras.layers.Dense(32, activation='relu'),
 73 |                 keras.layers.Dense(1)
 74 |             ])
 75 |
 76 |         elif self.model_type == "transformer":
 77 |             # Simplified transformer for demonstration
 78 |             self.model = keras.Sequential([
 79 |                 keras.layers.Dense(512, activation='relu'),
 80 |                 keras.layers.Dense(256, activation='relu'),
 81 |                 keras.layers.Dense(128, activation='relu'),
 82 |                 keras.layers.Dense(1)
 83 |             ])
 84 |
 85 |         elif self.model_type == "autoencoder":
 86 |             # Encoder
 87 |             encoder = keras.Sequential([
 88 |                 keras.layers.Dense(128, activation='relu'),
 89 |                 keras.layers.Dense(64, activation='relu'),
 90 |                 keras.layers.Dense(32, activation='relu')
 91 |             ])
 92 |
 93 |             # Decoder
 94 |             decoder = keras.Sequential([
 95 |                 keras.layers.Dense(64, activation='relu'),
 96 |                 keras.layers.Dense(128, activation='relu'),
 97 |                 keras.layers.Dense(784, activation='sigmoid')
 98 |             ])
 99 |
100 |             self.encoder = encoder
101 |             self.decoder = decoder
102 |             self.model = keras.Sequential([encoder, decoder])
103 |
104 |     async def _setup_pytorch(self):
105 |         """Setup PyTorch model"""
106 |         import torch
107 |         import torch.nn as nn
108 |
109 |         if self.model_type == "mlp":
110 |             class MLP(nn.Module):
111 |                 def __init__(self, input_size, hidden_sizes, output_size):
112 |                     super(MLP, self).__init__()
113 |                     self.layers = nn.ModuleList()
114 |
115 |                     # Input layer
116 |                     self.layers.append(nn.Linear(input_size, hidden_sizes[0]))
117 |
118 |                     # Hidden layers
119 |                     for i in range(len(hidden_sizes) - 1):
120 |                         self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))
121 |
122 |                     # Output layer
123 |                     self.layers.append(nn.Linear(hidden_sizes[-1], output_size))
124 |
125 |                     self.activation = nn.ReLU()
126 |                     self.dropout = nn.Dropout(0.2)
127 |
128 |                 def forward(self, x):
129 |                     for i, layer in enumerate(self.layers[:-1]):
130 |                         x = self.activation(layer(x))
131 |                         x = self.dropout(x)
132 |                     x = self.layers[-1](x)
133 |                     return x
134 |
135 |             self.model = MLP(784, [128, 64, 32], 10)  # Example dimensions
136 |
137 |         elif self.model_type == "cnn":
138 |             class CNN(nn.Module):
139 |                 def __init__(self):
140 |                     super(CNN, self).__init__()
141 |                     self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
142 |                     self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
143 |                     self.pool = nn.MaxPool2d(2)
144 |                     self.fc1 = nn.Linear(64 * 12 * 12, 128)
145 |                     self.fc2 = nn.Linear(128, 10)
146 |                     self.relu = nn.ReLU()
147 |
148 |                 def forward(self, x):
149 |                     x = self.pool(self.relu(self.conv1(x)))
150 |                     x = self.pool(self.relu(self.conv2(x)))
151 |                     x = x.view(-1, 64 * 12 * 12)
152 |                     x = self.relu(self.fc1(x))
153 |                     x = self.fc2(x)
154 |                     return x
155 |
156 |             self.model = CNN()
157 |
158 |     async def train(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
159 |         """Train deep learning model"""
160 |         if not self._initialized:
161 |             await self.initialize()
162 |
163 |         start_time = time.time()
164 |
165 |         epochs = kwargs.get('epochs', 10)
166 |         batch_size = kwargs.get('batch_size', 32)
167 |         learning_rate = kwargs.get('learning_rate', 0.001)
168 |
169 |         if self.framework == "tensorflow":
170 |             history = await self._train_tensorflow(X, y, epochs, batch_size, learning_rate)
171 |         else:
172 |             history = await self._train_pytorch(X, y, epochs, batch_size, learning_rate)
173 |
174 |         self.is_trained = True
175 |         self.history = history
176 |
177 |         training_time = time.time() - start_time
178 |
179 |         return {
180 |             "model_type": self.model_type,
181 |             "framework": self.framework,
182 |             "epochs": epochs,
183 |             "history": history,
184 |             "training_time": training_time
185 |         }
186 |
187 |     async def _train_tensorflow(self, X, y, epochs, batch_size, learning_rate):
188 |         """Train using TensorFlow"""
189 |         import tensorflow as tf
190 |
191 |         # Compile model
192 |         self.model.compile(
193 |             optimizer=tf.keras.optimizers.Adam(learning_rate),
194 |             loss='sparse_categorical_crossentropy',
195 |             metrics=['accuracy']
196 |         )
197 |
198 |         # Train model
199 |         history = self.model.fit(
200 |             X, y,
201 |             epochs=epochs,
202 |             batch_size=batch_size,
203 |             validation_split=0.2,
204 |             verbose=0
205 |         )
206 |
207 |         return {
208 |             "loss": history.history['loss'],
209 |             "accuracy": history.history.get('accuracy', []),
210 |             "val_loss": history.history.get('val_loss', []),
211 |             "val_accuracy": history.history.get('val_accuracy', [])
212 |         }
213 |
214 |     async def _train_pytorch(self, X, y, epochs, batch_size, learning_rate):
215 |         """Train using PyTorch"""
216 |         import torch
217 |         import torch.nn as nn
218 |         import torch.optim as optim
219 |         from torch.utils.data import DataLoader, TensorDataset
220 |
221 |         # Convert to tensors
222 |         X_tensor = torch.FloatTensor(X)
223 |         y_tensor = torch.LongTensor(y)
224 |
225 |         # Create data loader
226 |         dataset = TensorDataset(X_tensor, y_tensor)
227 |         dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
228 |
229 |         # Setup training
230 |         criterion = nn.CrossEntropyLoss()
231 |         optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
232 |
233 |         history = {"loss": [], "accuracy": []}
234 |
235 |         # Training loop
236 |         for epoch in range(epochs):
237 |             epoch_loss = 0
238 |             correct = 0
239 |             total = 0
240 |
241 |             for batch_X, batch_y in dataloader:
242 |                 optimizer.zero_grad()
243 |                 outputs = self.model(batch_X)
244 |                 loss = criterion(outputs, batch_y)
245 |                 loss.backward()
246 |                 optimizer.step()
247 |
248 |                 epoch_loss += loss.item()
249 |                 _, predicted = torch.max(outputs.data, 1)
250 |                 total += batch_y.size(0)
251 |                 correct += (predicted == batch_y).sum().item()
252 |
253 |             history["loss"].append(epoch_loss / len(dataloader))
254 |             history["accuracy"].append(correct / total)
255 |
256 |         return history
257 |
258 |     async def predict(self, X: Any, **kwargs) -> Any:
259 |         """Make predictions"""
260 |         if not self.is_trained:
261 |             raise RuntimeError("Model not trained yet")
262 |
263 |         if self.framework == "tensorflow":
264 |             predictions = self.model.predict(X)
265 |         else:
266 |             import torch
267 |             self.model.eval()
268 |             with torch.no_grad():
269 |                 X_tensor = torch.FloatTensor(X)
270 |                 predictions = self.model(X_tensor).numpy()
271 |
272 |         return predictions
273 |
274 |     async def evaluate(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
275 |         """Evaluate model"""
276 |         if not self.is_trained:
277 |             raise RuntimeError("Model not trained yet")
278 |
279 |         predictions = await self.predict(X)
280 |
281 |         # Calculate metrics based on task
282 |         from sklearn.metrics import accuracy_score, mean_squared_error
283 |
284 |         if self.model_type in ["mlp", "cnn", "transformer"]:
285 |             # Classification metrics
286 |             pred_classes = np.argmax(predictions, axis=1)
287 |             accuracy = accuracy_score(y, pred_classes)
288 |             metrics = {"accuracy": float(accuracy)}
289 |         else:
290 |             # Regression metrics
291 |             mse = mean_squared_error(y, predictions)
292 |             metrics = {"mse": float(mse), "rmse": float(np.sqrt(mse))}
293 |
294 |         return {"metrics": metrics}
295 |
296 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
297 |         """Execute deep learning task"""
298 |         if params.get('mode') == 'train':
299 |             return await self.train(data, params.get('target'), **params)
300 |         elif params.get('mode') == 'predict':
301 |             return await self.predict(data, **params)
302 |         else:
303 |             raise ValueError("Mode must be 'train' or 'predict'")
304 |


--------------------------------------------------------------------------------
/backend/app/engines/ml/model_hub.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Model Hub Engine for OpenStatica
  3 | Integration with Hugging Face, TensorFlow Hub, and other model repositories
  4 | """
  5 |
  6 | from typing import Dict, Any, List, Optional
  7 | import asyncio
  8 | import logging
  9 | from app.core.base import BaseMLModel, Result
 10 | import os
 11 |
 12 | logger = logging.getLogger(__name__)
 13 |
 14 |
 15 | class PretrainedModelRegistry:
 16 |     """Registry of available pretrained models"""
 17 |
 18 |     MODELS = {
 19 |         "nlp": {
 20 |             "text_classification": [
 21 |                 "bert-base-uncased",
 22 |                 "distilbert-base-uncased",
 23 |                 "roberta-base",
 24 |                 "albert-base-v2",
 25 |                 "xlnet-base-cased"
 26 |             ],
 27 |             "named_entity_recognition": [
 28 |                 "dslim/bert-base-NER",
 29 |                 "dbmdz/bert-large-cased-finetuned-conll03-english",
 30 |                 "Jean-Baptiste/camembert-ner"
 31 |             ],
 32 |             "question_answering": [
 33 |                 "distilbert-base-uncased-distilled-squad",
 34 |                 "bert-large-uncased-whole-word-masking-squad2",
 35 |                 "deepset/roberta-base-squad2"
 36 |             ],
 37 |             "summarization": [
 38 |                 "facebook/bart-large-cnn",
 39 |                 "google/pegasus-xsum",
 40 |                 "t5-base"
 41 |             ],
 42 |             "translation": [
 43 |                 "Helsinki-NLP/opus-mt-en-de",
 44 |                 "Helsinki-NLP/opus-mt-en-fr",
 45 |                 "facebook/mbart-large-50-many-to-many-mmt"
 46 |             ],
 47 |             "text_generation": [
 48 |                 "gpt2",
 49 |                 "gpt2-medium",
 50 |                 "EleutherAI/gpt-neo-2.7B",
 51 |                 "microsoft/DialoGPT-medium"
 52 |             ]
 53 |         },
 54 |         "computer_vision": {
 55 |             "image_classification": [
 56 |                 "google/vit-base-patch16-224",
 57 |                 "microsoft/resnet-50",
 58 |                 "facebook/deit-base-distilled-patch16-224",
 59 |                 "microsoft/beit-base-patch16-224"
 60 |             ],
 61 |             "object_detection": [
 62 |                 "facebook/detr-resnet-50",
 63 |                 "hustvl/yolos-tiny",
 64 |                 "microsoft/table-transformer-detection"
 65 |             ],
 66 |             "image_segmentation": [
 67 |                 "nvidia/segformer-b0-finetuned-ade-512-512",
 68 |                 "facebook/detr-resnet-50-panoptic",
 69 |                 "facebook/maskformer-swin-base-ade"
 70 |             ]
 71 |         },
 72 |         "audio": {
 73 |             "speech_recognition": [
 74 |                 "facebook/wav2vec2-base-960h",
 75 |                 "openai/whisper-base",
 76 |                 "microsoft/wavlm-base-plus"
 77 |             ],
 78 |             "audio_classification": [
 79 |                 "MIT/ast-finetuned-audioset-10-10-0.4593",
 80 |                 "facebook/wav2vec2-base",
 81 |                 "superb/wav2vec2-base-superb-ks"
 82 |             ]
 83 |         },
 84 |         "multimodal": {
 85 |             "image_text": [
 86 |                 "openai/clip-vit-base-patch32",
 87 |                 "microsoft/layoutlmv3-base",
 88 |                 "google/pix2struct-base"
 89 |             ],
 90 |             "video": [
 91 |                 "MCG-NJU/videomae-base",
 92 |                 "microsoft/xclip-base-patch32"
 93 |             ]
 94 |         },
 95 |         "tabular": {
 96 |             "classification": [
 97 |                 "autogluon/tabular-classification",
 98 |                 "microsoft/table-transformer-detection"
 99 |             ],
100 |             "regression": [
101 |                 "autogluon/tabular-regression"
102 |             ]
103 |         }
104 |     }
105 |
106 |     @classmethod
107 |     def get_all_models(cls) -> Dict[str, Dict[str, List[str]]]:
108 |         """Get all available models"""
109 |         return cls.MODELS
110 |
111 |     @classmethod
112 |     def get_models_by_task(cls, task: str) -> List[str]:
113 |         """Get models for a specific task"""
114 |         models = []
115 |         for category in cls.MODELS.values():
116 |             if task in category:
117 |                 models.extend(category[task])
118 |         return models
119 |
120 |     @classmethod
121 |     def get_models_by_category(cls, category: str) -> Dict[str, List[str]]:
122 |         """Get all models in a category"""
123 |         return cls.MODELS.get(category, {})
124 |
125 |
126 | class ModelHubEngine(BaseMLModel):
127 |     """Engine for loading and using pretrained models"""
128 |
129 |     def __init__(self, source: str = "huggingface"):
130 |         super().__init__(f"model_hub_{source}", "pretrained")
131 |         self.source = source
132 |         self.model = None
133 |         self.tokenizer = None
134 |         self.processor = None
135 |         self.pipeline = None
136 |         self.model_info = {}
137 |
138 |     async def _setup(self):
139 |         """Setup model hub connection"""
140 |         if self.source == "huggingface":
141 |             await self._setup_huggingface()
142 |         elif self.source == "tensorflow_hub":
143 |             await self._setup_tfhub()
144 |         elif self.source == "torch_hub":
145 |             await self._setup_torchhub()
146 |         else:
147 |             raise ValueError(f"Unknown model hub source: {self.source}")
148 |
149 |     async def _setup_huggingface(self):
150 |         """Setup Hugging Face integration"""
151 |         try:
152 |             from transformers import AutoModel, AutoTokenizer, pipeline
153 |             self.hf_available = True
154 |             logger.info("Hugging Face transformers library available")
155 |         except ImportError:
156 |             self.hf_available = False
157 |             logger.warning("Hugging Face transformers not installed")
158 |
159 |     async def _setup_tfhub(self):
160 |         """Setup TensorFlow Hub integration"""
161 |         try:
162 |             import tensorflow_hub as hub
163 |             self.tfhub_available = True
164 |             logger.info("TensorFlow Hub available")
165 |         except ImportError:
166 |             self.tfhub_available = False
167 |             logger.warning("TensorFlow Hub not installed")
168 |
169 |     async def _setup_torchhub(self):
170 |         """Setup PyTorch Hub integration"""
171 |         try:
172 |             import torch
173 |             self.torchhub_available = True
174 |             logger.info("PyTorch Hub available")
175 |         except ImportError:
176 |             self.torchhub_available = False
177 |             logger.warning("PyTorch not installed")
178 |
179 |     async def load_model(self, model_id: str, task: Optional[str] = None, **kwargs) -> Dict[str, Any]:
180 |         """Load a pretrained model from the hub"""
181 |         if not self._initialized:
182 |             await self.initialize()
183 |
184 |         try:
185 |             if self.source == "huggingface":
186 |                 return await self._load_huggingface_model(model_id, task, **kwargs)
187 |             elif self.source == "tensorflow_hub":
188 |                 return await self._load_tfhub_model(model_id, **kwargs)
189 |             elif self.source == "torch_hub":
190 |                 return await self._load_torchhub_model(model_id, **kwargs)
191 |             else:
192 |                 raise ValueError(f"Unsupported source: {self.source}")
193 |
194 |         except Exception as e:
195 |             logger.error(f"Failed to load model {model_id}: {e}")
196 |             raise
197 |
198 |     async def _load_huggingface_model(self, model_id: str, task: Optional[str] = None, **kwargs):
199 |         """Load model from Hugging Face"""
200 |         if not self.hf_available:
201 |             raise ImportError("Hugging Face transformers not available")
202 |
203 |         from transformers import (
204 |             AutoModel, AutoTokenizer, AutoModelForSequenceClassification,
205 |             AutoModelForTokenClassification, AutoModelForQuestionAnswering,
206 |             AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline
207 |         )
208 |
209 |         # Determine model class based on task
210 |         model_classes = {
211 |             "text-classification": AutoModelForSequenceClassification,
212 |             "token-classification": AutoModelForTokenClassification,
213 |             "question-answering": AutoModelForQuestionAnswering,
214 |             "text-generation": AutoModelForCausalLM,
215 |             "text2text-generation": AutoModelForSeq2SeqLM,
216 |             "default": AutoModel
217 |         }
218 |
219 |         model_class = model_classes.get(task, model_classes["default"])
220 |
221 |         # Load model and tokenizer
222 |         try:
223 |             # Use HF token if available
224 |             hf_token = os.getenv("HUGGINGFACE_TOKEN")
225 |
226 |             self.model = model_class.from_pretrained(
227 |                 model_id,
228 |                 use_auth_token=hf_token if hf_token else None,
229 |                 **kwargs
230 |             )
231 |
232 |             self.tokenizer = AutoTokenizer.from_pretrained(
233 |                 model_id,
234 |                 use_auth_token=hf_token if hf_token else None
235 |             )
236 |
237 |             # Create pipeline for easy inference
238 |             if task:
239 |                 self.pipeline = pipeline(
240 |                     task,
241 |                     model=self.model,
242 |                     tokenizer=self.tokenizer
243 |                 )
244 |
245 |             self.is_trained = True
246 |             self.model_info = {
247 |                 "model_id": model_id,
248 |                 "task": task,
249 |                 "source": "huggingface",
250 |                 "model_type": model_class.__name__
251 |             }
252 |
253 |             return {
254 |                 "status": "success",
255 |                 "model_id": model_id,
256 |                 "task": task,
257 |                 "model_info": self.model_info
258 |             }
259 |
260 |         except Exception as e:
261 |             logger.error(f"Error loading HuggingFace model: {e}")
262 |             raise
263 |
264 |     async def _load_tfhub_model(self, model_url: str, **kwargs):
265 |         """Load model from TensorFlow Hub"""
266 |         if not self.tfhub_available:
267 |             raise ImportError("TensorFlow Hub not available")
268 |
269 |         import tensorflow_hub as hub
270 |         import tensorflow as tf
271 |
272 |         # Load model
273 |         self.model = hub.load(model_url)
274 |         self.is_trained = True
275 |
276 |         self.model_info = {
277 |             "model_url": model_url,
278 |             "source": "tensorflow_hub",
279 |             "framework": "tensorflow"
280 |         }
281 |
282 |         return {
283 |             "status": "success",
284 |             "model_url": model_url,
285 |             "model_info": self.model_info
286 |         }
287 |
288 |     async def _load_torchhub_model(self, model_id: str, **kwargs):
289 |         """Load model from PyTorch Hub"""
290 |         if not self.torchhub_available:
291 |             raise ImportError("PyTorch not available")
292 |
293 |         import torch
294 |
295 |         # Parse repo and model name
296 |         if '/' in model_id:
297 |             repo, model_name = model_id.split('/', 1)
298 |         else:
299 |             repo = "pytorch/vision"
300 |             model_name = model_id
301 |
302 |         # Load model
303 |         self.model = torch.hub.load(repo, model_name, pretrained=True, **kwargs)
304 |         self.model.eval()
305 |         self.is_trained = True
306 |
307 |         self.model_info = {
308 |             "model_id": model_id,
309 |             "repo": repo,
310 |             "model_name": model_name,
311 |             "source": "torch_hub",
312 |             "framework": "pytorch"
313 |         }
314 |
315 |         return {
316 |             "status": "success",
317 |             "model_id": model_id,
318 |             "model_info": self.model_info
319 |         }
320 |
321 |     async def predict(self, inputs: Any, **kwargs) -> Any:
322 |         """Make predictions with loaded model"""
323 |         if not self.is_trained:
324 |             raise RuntimeError("No model loaded")
325 |
326 |         if self.source == "huggingface":
327 |             return await self._predict_huggingface(inputs, **kwargs)
328 |         elif self.source == "tensorflow_hub":
329 |             return await self._predict_tfhub(inputs, **kwargs)
330 |         elif self.source == "torch_hub":
331 |             return await self._predict_torchhub(inputs, **kwargs)
332 |
333 |     async def _predict_huggingface(self, inputs: Any, **kwargs):
334 |         """Predict using Hugging Face model"""
335 |         if self.pipeline:
336 |             # Use pipeline for inference
337 |             results = self.pipeline(inputs, **kwargs)
338 |             return results
339 |         else:
340 |             # Manual inference
341 |             if self.tokenizer:
342 |                 encoded = self.tokenizer(
343 |                     inputs,
344 |                     return_tensors="pt",
345 |                     padding=True,
346 |                     truncation=True,
347 |                     **kwargs
348 |                 )
349 |
350 |                 outputs = self.model(**encoded)
351 |
352 |                 # Process outputs based on task
353 |                 if hasattr(outputs, "logits"):
354 |                     import torch
355 |                     predictions = torch.softmax(outputs.logits, dim=-1)
356 |                     return predictions.tolist()
357 |                 else:
358 |                     return outputs
359 |
360 |     async def _predict_tfhub(self, inputs: Any, **kwargs):
361 |         """Predict using TensorFlow Hub model"""
362 |         import tensorflow as tf
363 |
364 |         # Prepare inputs
365 |         if isinstance(inputs, str):
366 |             inputs = [inputs]
367 |
368 |         # Run inference
369 |         outputs = self.model(inputs)
370 |
371 |         return outputs.numpy().tolist() if hasattr(outputs, 'numpy') else outputs
372 |
373 |     async def _predict_torchhub(self, inputs: Any, **kwargs):
374 |         """Predict using PyTorch Hub model"""
375 |         import torch
376 |
377 |         # Prepare inputs
378 |         if hasattr(self.model, 'transform'):
379 |             inputs = self.model.transform(inputs)
380 |
381 |         with torch.no_grad():
382 |             outputs = self.model(inputs)
383 |
384 |         return outputs.numpy().tolist() if hasattr(outputs, 'numpy') else outputs.tolist()
385 |
386 |     async def train(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
387 |         """Fine-tune the pretrained model"""
388 |         if not self.is_trained:
389 |             raise RuntimeError("No model loaded")
390 |
391 |         if self.source == "huggingface":
392 |             return await self._finetune_huggingface(X, y, **kwargs)
393 |         else:
394 |             raise NotImplementedError(f"Fine-tuning not implemented for {self.source}")
395 |
396 |     async def _finetune_huggingface(self, X: Any, y: Any, **kwargs):
397 |         """Fine-tune Hugging Face model"""
398 |         from transformers import TrainingArguments, Trainer
399 |         import torch
400 |         from torch.utils.data import Dataset
401 |
402 |         class CustomDataset(Dataset):
403 |             def __init__(self, texts, labels, tokenizer):
404 |                 self.texts = texts
405 |                 self.labels = labels
406 |                 self.tokenizer = tokenizer
407 |
408 |             def __len__(self):
409 |                 return len(self.texts)
410 |
411 |             def __getitem__(self, idx):
412 |                 encoding = self.tokenizer(
413 |                     self.texts[idx],
414 |                     truncation=True,
415 |                     padding="max_length",
416 |                     max_length=512,
417 |                     return_tensors="pt"
418 |                 )
419 |
420 |                 return {
421 |                     'input_ids': encoding['input_ids'].flatten(),
422 |                     'attention_mask': encoding['attention_mask'].flatten(),
423 |                     'labels': torch.tensor(self.labels[idx], dtype=torch.long)
424 |                 }
425 |
426 |         # Create dataset
427 |         dataset = CustomDataset(X, y, self.tokenizer)
428 |
429 |         # Training arguments
430 |         training_args = TrainingArguments(
431 |             output_dir="./results",
432 |             num_train_epochs=kwargs.get('epochs', 3),
433 |             per_device_train_batch_size=kwargs.get('batch_size', 16),
434 |             per_device_eval_batch_size=kwargs.get('batch_size', 16),
435 |             warmup_steps=kwargs.get('warmup_steps', 500),
436 |             weight_decay=kwargs.get('weight_decay', 0.01),
437 |             logging_dir='./logs',
438 |         )
439 |
440 |         # Create trainer
441 |         trainer = Trainer(
442 |             model=self.model,
443 |             args=training_args,
444 |             train_dataset=dataset,
445 |             eval_dataset=dataset,  # Should use separate validation set
446 |         )
447 |
448 |         # Train
449 |         trainer.train()
450 |
451 |         return {
452 |             "status": "success",
453 |             "training_completed": True,
454 |             "final_loss": trainer.state.log_history[-1].get('loss', None)
455 |         }
456 |
457 |     async def evaluate(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
458 |         """Evaluate the model"""
459 |         predictions = await self.predict(X, **kwargs)
460 |
461 |         # Calculate metrics based on task
462 |         from sklearn.metrics import accuracy_score, f1_score
463 |
464 |         if self.model_info.get("task") == "text-classification":
465 |             # Extract predicted classes
466 |             if isinstance(predictions, list) and len(predictions) > 0:
467 |                 if isinstance(predictions[0], dict):
468 |                     pred_classes = [p['label'] for p in predictions]
469 |                 else:
470 |                     import numpy as np
471 |                     pred_classes = np.argmax(predictions, axis=1)
472 |
473 |                 accuracy = accuracy_score(y, pred_classes)
474 |                 f1 = f1_score(y, pred_classes, average='weighted')
475 |
476 |                 return {
477 |                     "accuracy": float(accuracy),
478 |                     "f1_score": float(f1),
479 |                     "n_samples": len(y)
480 |                 }
481 |
482 |         return {"message": "Evaluation not implemented for this task"}
483 |
484 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
485 |         """Execute model hub operations"""
486 |         operation = params.get("operation", "predict")
487 |
488 |         if operation == "load":
489 |             return await self.load_model(
490 |                 params.get("model_id"),
491 |                 params.get("task"),
492 |                 **params.get("kwargs", {})
493 |             )
494 |         elif operation == "predict":
495 |             return await self.predict(data, **params.get("kwargs", {}))
496 |         elif operation == "train":
497 |             return await self.train(
498 |                 data,
499 |                 params.get("labels"),
500 |                 **params.get("kwargs", {})
501 |             )
502 |         else:
503 |             raise ValueError(f"Unknown operation: {operation}")
504 |
505 |     async def list_available_models(self, task: Optional[str] = None) -> List[str]:
506 |         """List available models for a task"""
507 |         if task:
508 |             return PretrainedModelRegistry.get_models_by_task(task)
509 |         else:
510 |             all_models = []
511 |             for category in PretrainedModelRegistry.MODELS.values():
512 |                 for models in category.values():
513 |                     all_models.extend(models)
514 |             return all_models
515 |
516 |     async def get_model_info(self, model_id: str) -> Dict[str, Any]:
517 |         """Get information about a specific model"""
518 |         if self.source == "huggingface":
519 |             try:
520 |                 from huggingface_hub import HfApi
521 |                 api = HfApi()
522 |                 model_info = api.model_info(model_id)
523 |
524 |                 return {
525 |                     "model_id": model_id,
526 |                     "downloads": model_info.downloads,
527 |                     "likes": model_info.likes,
528 |                     "tags": model_info.tags,
529 |                     "pipeline_tag": model_info.pipeline_tag,
530 |                     "library_name": model_info.library_name
531 |                 }
532 |             except:
533 |                 return {"model_id": model_id, "source": self.source}
534 |
535 |         return {"model_id": model_id, "source": self.source}
536 |


--------------------------------------------------------------------------------
/backend/app/engines/ml/supervised.py:
--------------------------------------------------------------------------------
  1 | from typing import Any, Dict, Optional
  2 | import numpy as np
  3 | import pandas as pd
  4 | from sklearn.model_selection import cross_val_score, GridSearchCV
  5 | from sklearn.metrics import (
  6 |     accuracy_score, precision_score, recall_score, f1_score,
  7 |     roc_auc_score, confusion_matrix, mean_squared_error,
  8 |     mean_absolute_error, r2_score
  9 | )
 10 | from app.core.base import BaseMLModel, Result
 11 | import time
 12 |
 13 |
 14 | class ClassificationEngine(BaseMLModel):
 15 |     """Engine for classification algorithms"""
 16 |
 17 |     def __init__(self, algorithm: str):
 18 |         super().__init__(f"classification_{algorithm}", "classification")
 19 |         self.algorithm = algorithm
 20 |
 21 |     async def _setup(self):
 22 |         """Initialize the model based on algorithm"""
 23 |         if self.algorithm == "logistic":
 24 |             from sklearn.linear_model import LogisticRegression
 25 |             self.model = LogisticRegression(max_iter=1000)
 26 |
 27 |         elif self.algorithm == "svm":
 28 |             from sklearn.svm import SVC
 29 |             self.model = SVC(probability=True)
 30 |
 31 |         elif self.algorithm == "rf":
 32 |             from sklearn.ensemble import RandomForestClassifier
 33 |             self.model = RandomForestClassifier(n_estimators=100)
 34 |
 35 |         elif self.algorithm == "xgboost":
 36 |             try:
 37 |                 import xgboost as xgb
 38 |                 self.model = xgb.XGBClassifier()
 39 |             except ImportError:
 40 |                 from sklearn.ensemble import GradientBoostingClassifier
 41 |                 self.model = GradientBoostingClassifier()
 42 |
 43 |         elif self.algorithm == "nn":
 44 |             from sklearn.neural_network import MLPClassifier
 45 |             self.model = MLPClassifier(hidden_layer_sizes=(100, 50))
 46 |
 47 |         elif self.algorithm == "knn":
 48 |             from sklearn.neighbors import KNeighborsClassifier
 49 |             self.model = KNeighborsClassifier()
 50 |
 51 |         elif self.algorithm == "nb":
 52 |             from sklearn.naive_bayes import GaussianNB
 53 |             self.model = GaussianNB()
 54 |
 55 |         elif self.algorithm == "dt":
 56 |             from sklearn.tree import DecisionTreeClassifier
 57 |             self.model = DecisionTreeClassifier()
 58 |
 59 |         else:
 60 |             raise ValueError(f"Unknown algorithm: {self.algorithm}")
 61 |
 62 |     async def train(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
 63 |         """Train the classification model"""
 64 |         if not self._initialized:
 65 |             await self.initialize()
 66 |
 67 |         start_time = time.time()
 68 |
 69 |         # Perform cross-validation if requested
 70 |         if kwargs.get('cross_validate', True):
 71 |             cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='accuracy')
 72 |             cv_mean = cv_scores.mean()
 73 |             cv_std = cv_scores.std()
 74 |         else:
 75 |             cv_mean = None
 76 |             cv_std = None
 77 |
 78 |         # Hyperparameter tuning if requested
 79 |         if kwargs.get('auto_ml', False):
 80 |             self.model = await self._auto_tune(X, y)
 81 |
 82 |         # Train the model
 83 |         self.model.fit(X, y)
 84 |         self.is_trained = True
 85 |
 86 |         # Calculate feature importance if available
 87 |         feature_importance = None
 88 |         if hasattr(self.model, 'feature_importances_'):
 89 |             feature_importance = dict(zip(
 90 |                 X.columns if hasattr(X, 'columns') else range(X.shape[1]),
 91 |                 self.model.feature_importances_
 92 |             ))
 93 |         elif hasattr(self.model, 'coef_'):
 94 |             feature_importance = dict(zip(
 95 |                 X.columns if hasattr(X, 'columns') else range(X.shape[1]),
 96 |                 np.abs(self.model.coef_[0]) if self.model.coef_.ndim > 1 else np.abs(self.model.coef_)
 97 |             ))
 98 |
 99 |         training_time = time.time() - start_time
100 |
101 |         return {
102 |             "algorithm": self.algorithm,
103 |             "training_time": training_time,
104 |             "cv_accuracy_mean": cv_mean,
105 |             "cv_accuracy_std": cv_std,
106 |             "feature_importance": feature_importance,
107 |             "model_params": self.model.get_params()
108 |         }
109 |
110 |     async def predict(self, X: Any, **kwargs) -> Any:
111 |         """Make predictions"""
112 |         if not self.is_trained:
113 |             raise RuntimeError("Model not trained yet")
114 |
115 |         predictions = self.model.predict(X)
116 |
117 |         if kwargs.get('probability', False):
118 |             probabilities = self.model.predict_proba(X)
119 |             return {'predictions': predictions, 'probabilities': probabilities}
120 |
121 |         return predictions
122 |
123 |     async def evaluate(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
124 |         """Evaluate model performance"""
125 |         if not self.is_trained:
126 |             raise RuntimeError("Model not trained yet")
127 |
128 |         predictions = self.model.predict(X)
129 |
130 |         # Calculate metrics
131 |         metrics = {
132 |             "accuracy": float(accuracy_score(y, predictions)),
133 |             "precision": float(precision_score(y, predictions, average='weighted')),
134 |             "recall": float(recall_score(y, predictions, average='weighted')),
135 |             "f1_score": float(f1_score(y, predictions, average='weighted'))
136 |         }
137 |
138 |         # Add AUC-ROC for binary classification
139 |         if len(np.unique(y)) == 2:
140 |             probabilities = self.model.predict_proba(X)[:, 1]
141 |             metrics["auc_roc"] = float(roc_auc_score(y, probabilities))
142 |
143 |         # Confusion matrix
144 |         cm = confusion_matrix(y, predictions)
145 |
146 |         return {
147 |             "metrics": metrics,
148 |             "confusion_matrix": cm.tolist()
149 |         }
150 |
151 |     async def _auto_tune(self, X: Any, y: Any) -> Any:
152 |         """Automatic hyperparameter tuning"""
153 |         param_grid = self._get_param_grid()
154 |
155 |         grid_search = GridSearchCV(
156 |             self.model, param_grid, cv=3,
157 |             scoring='accuracy', n_jobs=-1
158 |         )
159 |         grid_search.fit(X, y)
160 |
161 |         return grid_search.best_estimator_
162 |
163 |     def _get_param_grid(self) -> Dict[str, list]:
164 |         """Get parameter grid for hyperparameter tuning"""
165 |         if self.algorithm == "logistic":
166 |             return {
167 |                 'C': [0.01, 0.1, 1, 10],
168 |                 'penalty': ['l1', 'l2'],
169 |                 'solver': ['liblinear', 'saga']
170 |             }
171 |         elif self.algorithm == "svm":
172 |             return {
173 |                 'C': [0.1, 1, 10],
174 |                 'kernel': ['linear', 'rbf', 'poly'],
175 |                 'gamma': ['scale', 'auto']
176 |             }
177 |         elif self.algorithm == "rf":
178 |             return {
179 |                 'n_estimators': [50, 100, 200],
180 |                 'max_depth': [None, 10, 20],
181 |                 'min_samples_split': [2, 5, 10]
182 |             }
183 |         else:
184 |             return {}
185 |
186 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
187 |         """Execute classification task"""
188 |         # This method is for compatibility with base engine
189 |         if params.get('mode') == 'train':
190 |             return await self.train(data, params.get('target'), **params)
191 |         elif params.get('mode') == 'predict':
192 |             return await self.predict(data, **params)
193 |         else:
194 |             raise ValueError("Mode must be 'train' or 'predict'")
195 |
196 |
197 | class RegressionEngine(BaseMLModel):
198 |     """Engine for regression algorithms"""
199 |
200 |     def __init__(self, algorithm: str):
201 |         super().__init__(f"regression_{algorithm}", "regression")
202 |         self.algorithm = algorithm
203 |
204 |     async def _setup(self):
205 |         """Initialize the regression model"""
206 |         if self.algorithm == "linear":
207 |             from sklearn.linear_model import LinearRegression
208 |             self.model = LinearRegression()
209 |
210 |         elif self.algorithm == "ridge":
211 |             from sklearn.linear_model import Ridge
212 |             self.model = Ridge()
213 |
214 |         elif self.algorithm == "lasso":
215 |             from sklearn.linear_model import Lasso
216 |             self.model = Lasso()
217 |
218 |         elif self.algorithm == "elastic":
219 |             from sklearn.linear_model import ElasticNet
220 |             self.model = ElasticNet()
221 |
222 |         elif self.algorithm == "svr":
223 |             from sklearn.svm import SVR
224 |             self.model = SVR()
225 |
226 |         elif self.algorithm == "rf_reg":
227 |             from sklearn.ensemble import RandomForestRegressor
228 |             self.model = RandomForestRegressor(n_estimators=100)
229 |
230 |         elif self.algorithm == "xgb_reg":
231 |             try:
232 |                 import xgboost as xgb
233 |                 self.model = xgb.XGBRegressor()
234 |             except ImportError:
235 |                 from sklearn.ensemble import GradientBoostingRegressor
236 |                 self.model = GradientBoostingRegressor()
237 |
238 |         elif self.algorithm == "nn_reg":
239 |             from sklearn.neural_network import MLPRegressor
240 |             self.model = MLPRegressor(hidden_layer_sizes=(100, 50))
241 |
242 |         else:
243 |             raise ValueError(f"Unknown algorithm: {self.algorithm}")
244 |
245 |     async def train(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
246 |         """Train the regression model"""
247 |         if not self._initialized:
248 |             await self.initialize()
249 |
250 |         start_time = time.time()
251 |
252 |         # Cross-validation
253 |         if kwargs.get('cross_validate', True):
254 |             cv_scores = cross_val_score(self.model, X, y, cv=5, scoring='r2')
255 |             cv_mean = cv_scores.mean()
256 |             cv_std = cv_scores.std()
257 |         else:
258 |             cv_mean = None
259 |             cv_std = None
260 |
261 |         # Train model
262 |         self.model.fit(X, y)
263 |         self.is_trained = True
264 |
265 |         # Feature importance
266 |         feature_importance = None
267 |         if hasattr(self.model, 'feature_importances_'):
268 |             feature_importance = dict(zip(
269 |                 X.columns if hasattr(X, 'columns') else range(X.shape[1]),
270 |                 self.model.feature_importances_
271 |             ))
272 |         elif hasattr(self.model, 'coef_'):
273 |             feature_importance = dict(zip(
274 |                 X.columns if hasattr(X, 'columns') else range(X.shape[1]),
275 |                 np.abs(self.model.coef_)
276 |             ))
277 |
278 |         training_time = time.time() - start_time
279 |
280 |         return {
281 |             "algorithm": self.algorithm,
282 |             "training_time": training_time,
283 |             "cv_r2_mean": cv_mean,
284 |             "cv_r2_std": cv_std,
285 |             "feature_importance": feature_importance,
286 |             "model_params": self.model.get_params()
287 |         }
288 |
289 |     async def predict(self, X: Any, **kwargs) -> Any:
290 |         """Make predictions"""
291 |         if not self.is_trained:
292 |             raise RuntimeError("Model not trained yet")
293 |
294 |         predictions = self.model.predict(X)
295 |
296 |         if kwargs.get('return_uncertainty', False) and hasattr(self.model, 'predict_std'):
297 |             uncertainty = self.model.predict_std(X)
298 |             return {'predictions': predictions, 'uncertainty': uncertainty}
299 |
300 |         return predictions
301 |
302 |     async def evaluate(self, X: Any, y: Any, **kwargs) -> Dict[str, Any]:
303 |         """Evaluate model performance"""
304 |         if not self.is_trained:
305 |             raise RuntimeError("Model not trained yet")
306 |
307 |         predictions = self.model.predict(X)
308 |
309 |         metrics = {
310 |             "mse": float(mean_squared_error(y, predictions)),
311 |             "rmse": float(np.sqrt(mean_squared_error(y, predictions))),
312 |             "mae": float(mean_absolute_error(y, predictions)),
313 |             "r2": float(r2_score(y, predictions))
314 |         }
315 |
316 |         # Calculate residuals
317 |         residuals = y - predictions
318 |
319 |         return {
320 |             "metrics": metrics,
321 |             "residuals_stats": {
322 |                 "mean": float(np.mean(residuals)),
323 |                 "std": float(np.std(residuals)),
324 |                 "min": float(np.min(residuals)),
325 |                 "max": float(np.max(residuals))
326 |             }
327 |         }
328 |
329 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
330 |         """Execute regression task"""
331 |         if params.get('mode') == 'train':
332 |             return await self.train(data, params.get('target'), **params)
333 |         elif params.get('mode') == 'predict':
334 |             return await self.predict(data, **params)
335 |         else:
336 |             raise ValueError("Mode must be 'train' or 'predict'")
337 |


--------------------------------------------------------------------------------
/backend/app/engines/ml/unsupervised.py:
--------------------------------------------------------------------------------
  1 | from typing import Any, Dict, Optional, List
  2 | import numpy as np
  3 | import pandas as pd
  4 | from sklearn.preprocessing import StandardScaler
  5 | from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
  6 | from app.core.base import BaseMLModel, Result
  7 | import time
  8 |
  9 |
 10 | class ClusteringEngine(BaseMLModel):
 11 |     """Engine for clustering algorithms"""
 12 |
 13 |     def __init__(self, algorithm: str):
 14 |         super().__init__(f"clustering_{algorithm}", "clustering")
 15 |         self.algorithm = algorithm
 16 |         self.labels_ = None
 17 |
 18 |     async def _setup(self):
 19 |         """Initialize clustering model"""
 20 |         if self.algorithm == "kmeans":
 21 |             from sklearn.cluster import KMeans
 22 |             self.model = KMeans(n_clusters=3, random_state=42)
 23 |
 24 |         elif self.algorithm == "dbscan":
 25 |             from sklearn.cluster import DBSCAN
 26 |             self.model = DBSCAN(eps=0.5, min_samples=5)
 27 |
 28 |         elif self.algorithm == "hierarchical":
 29 |             from sklearn.cluster import AgglomerativeClustering
 30 |             self.model = AgglomerativeClustering(n_clusters=3)
 31 |
 32 |         elif self.algorithm == "gaussian":
 33 |             from sklearn.mixture import GaussianMixture
 34 |             self.model = GaussianMixture(n_components=3, random_state=42)
 35 |
 36 |         elif self.algorithm == "meanshift":
 37 |             from sklearn.cluster import MeanShift
 38 |             self.model = MeanShift()
 39 |
 40 |         elif self.algorithm == "spectral":
 41 |             from sklearn.cluster import SpectralClustering
 42 |             self.model = SpectralClustering(n_clusters=3, random_state=42)
 43 |
 44 |         elif self.algorithm == "optics":
 45 |             from sklearn.cluster import OPTICS
 46 |             self.model = OPTICS(min_samples=5)
 47 |
 48 |         elif self.algorithm == "birch":
 49 |             from sklearn.cluster import Birch
 50 |             self.model = Birch(n_clusters=3)
 51 |
 52 |         else:
 53 |             raise ValueError(f"Unknown clustering algorithm: {self.algorithm}")
 54 |
 55 |     async def train(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
 56 |         """Fit clustering model"""
 57 |         if not self._initialized:
 58 |             await self.initialize()
 59 |
 60 |         start_time = time.time()
 61 |
 62 |         # Update parameters if provided
 63 |         if 'n_clusters' in kwargs and hasattr(self.model, 'n_clusters'):
 64 |             self.model.n_clusters = kwargs['n_clusters']
 65 |
 66 |         # Standardize features if requested
 67 |         if kwargs.get('standardize', True):
 68 |             scaler = StandardScaler()
 69 |             X_scaled = scaler.fit_transform(X)
 70 |         else:
 71 |             X_scaled = X
 72 |
 73 |         # Fit model
 74 |         if self.algorithm == "gaussian":
 75 |             self.model.fit(X_scaled)
 76 |             self.labels_ = self.model.predict(X_scaled)
 77 |         else:
 78 |             self.labels_ = self.model.fit_predict(X_scaled)
 79 |
 80 |         self.is_trained = True
 81 |
 82 |         # Calculate metrics
 83 |         metrics = await self._calculate_metrics(X_scaled, self.labels_)
 84 |
 85 |         # Get cluster centers if available
 86 |         centers = None
 87 |         if hasattr(self.model, 'cluster_centers_'):
 88 |             centers = self.model.cluster_centers_.tolist()
 89 |         elif hasattr(self.model, 'means_'):
 90 |             centers = self.model.means_.tolist()
 91 |
 92 |         training_time = time.time() - start_time
 93 |
 94 |         return {
 95 |             "algorithm": self.algorithm,
 96 |             "n_clusters": len(np.unique(self.labels_)),
 97 |             "labels": self.labels_.tolist(),
 98 |             "metrics": metrics,
 99 |             "centers": centers,
100 |             "training_time": training_time
101 |         }
102 |
103 |     async def predict(self, X: Any, **kwargs) -> Any:
104 |         """Predict cluster labels for new data"""
105 |         if not self.is_trained:
106 |             raise RuntimeError("Model not trained yet")
107 |
108 |         # Standardize if needed
109 |         if kwargs.get('standardize', True):
110 |             scaler = StandardScaler()
111 |             X_scaled = scaler.fit_transform(X)
112 |         else:
113 |             X_scaled = X
114 |
115 |         if hasattr(self.model, 'predict'):
116 |             return self.model.predict(X_scaled)
117 |         else:
118 |             # For algorithms without predict method
119 |             return self.model.fit_predict(X_scaled)
120 |
121 |     async def evaluate(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
122 |         """Evaluate clustering quality"""
123 |         if not self.is_trained:
124 |             raise RuntimeError("Model not trained yet")
125 |
126 |         labels = self.labels_ if self.labels_ is not None else self.model.labels_
127 |         metrics = await self._calculate_metrics(X, labels)
128 |
129 |         return {"metrics": metrics}
130 |
131 |     async def _calculate_metrics(self, X: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
132 |         """Calculate clustering metrics"""
133 |         metrics = {}
134 |
135 |         if len(np.unique(labels)) > 1:
136 |             metrics["silhouette_score"] = float(silhouette_score(X, labels))
137 |             metrics["calinski_harabasz_score"] = float(calinski_harabasz_score(X, labels))
138 |             metrics["davies_bouldin_score"] = float(davies_bouldin_score(X, labels))
139 |
140 |         # Calculate inertia for KMeans
141 |         if hasattr(self.model, 'inertia_'):
142 |             metrics["inertia"] = float(self.model.inertia_)
143 |
144 |         return metrics
145 |
146 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
147 |         """Execute clustering task"""
148 |         return await self.train(data, **params)
149 |
150 |
151 | class DimensionalityReductionEngine(BaseMLModel):
152 |     """Engine for dimensionality reduction"""
153 |
154 |     def __init__(self, algorithm: str):
155 |         super().__init__(f"dim_reduction_{algorithm}", "dimensionality_reduction")
156 |         self.algorithm = algorithm
157 |
158 |     async def _setup(self):
159 |         """Initialize dimensionality reduction model"""
160 |         if self.algorithm == "pca":
161 |             from sklearn.decomposition import PCA
162 |             self.model = PCA(n_components=2)
163 |
164 |         elif self.algorithm == "tsne":
165 |             from sklearn.manifold import TSNE
166 |             self.model = TSNE(n_components=2, random_state=42)
167 |
168 |         elif self.algorithm == "umap":
169 |             try:
170 |                 import umap
171 |                 self.model = umap.UMAP(n_components=2, random_state=42)
172 |             except ImportError:
173 |                 # Fallback to PCA if UMAP not installed
174 |                 from sklearn.decomposition import PCA
175 |                 self.model = PCA(n_components=2)
176 |
177 |         elif self.algorithm == "lda":
178 |             from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
179 |             self.model = LinearDiscriminantAnalysis()
180 |
181 |         elif self.algorithm == "ica":
182 |             from sklearn.decomposition import FastICA
183 |             self.model = FastICA(n_components=2, random_state=42)
184 |
185 |         elif self.algorithm == "nmf":
186 |             from sklearn.decomposition import NMF
187 |             self.model = NMF(n_components=2, random_state=42)
188 |
189 |         elif self.algorithm == "factor":
190 |             from sklearn.decomposition import FactorAnalysis
191 |             self.model = FactorAnalysis(n_components=2, random_state=42)
192 |
193 |         elif self.algorithm == "truncated_svd":
194 |             from sklearn.decomposition import TruncatedSVD
195 |             self.model = TruncatedSVD(n_components=2, random_state=42)
196 |
197 |         else:
198 |             raise ValueError(f"Unknown dimensionality reduction algorithm: {self.algorithm}")
199 |
200 |     async def train(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
201 |         """Fit dimensionality reduction model"""
202 |         if not self._initialized:
203 |             await self.initialize()
204 |
205 |         start_time = time.time()
206 |
207 |         # Update components if provided
208 |         if 'n_components' in kwargs and hasattr(self.model, 'n_components'):
209 |             self.model.n_components = kwargs['n_components']
210 |
211 |         # Fit model
212 |         if self.algorithm in ["lda"] and y is not None:
213 |             self.model.fit(X, y)
214 |         else:
215 |             self.model.fit(X)
216 |
217 |         self.is_trained = True
218 |
219 |         # Transform data
220 |         X_transformed = self.model.transform(X)
221 |
222 |         # Get explained variance if available
223 |         explained_variance = None
224 |         if hasattr(self.model, 'explained_variance_ratio_'):
225 |             explained_variance = self.model.explained_variance_ratio_.tolist()
226 |
227 |         # Get components if available
228 |         components = None
229 |         if hasattr(self.model, 'components_'):
230 |             components = self.model.components_.tolist()
231 |
232 |         training_time = time.time() - start_time
233 |
234 |         return {
235 |             "algorithm": self.algorithm,
236 |             "n_components": self.model.n_components if hasattr(self.model, 'n_components') else None,
237 |             "transformed_shape": X_transformed.shape,
238 |             "explained_variance": explained_variance,
239 |             "components": components,
240 |             "training_time": training_time
241 |         }
242 |
243 |     async def predict(self, X: Any, **kwargs) -> Any:
244 |         """Transform new data"""
245 |         if not self.is_trained:
246 |             raise RuntimeError("Model not trained yet")
247 |
248 |         return self.model.transform(X)
249 |
250 |     async def evaluate(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
251 |         """Evaluate dimensionality reduction"""
252 |         if not self.is_trained:
253 |             raise RuntimeError("Model not trained yet")
254 |
255 |         X_transformed = self.model.transform(X)
256 |
257 |         metrics = {
258 |             "original_shape": X.shape,
259 |             "reduced_shape": X_transformed.shape,
260 |             "reduction_ratio": X_transformed.shape[1] / X.shape[1]
261 |         }
262 |
263 |         # Reconstruction error for applicable methods
264 |         if hasattr(self.model, 'inverse_transform'):
265 |             X_reconstructed = self.model.inverse_transform(X_transformed)
266 |             reconstruction_error = np.mean((X - X_reconstructed) ** 2)
267 |             metrics["reconstruction_error"] = float(reconstruction_error)
268 |
269 |         return {"metrics": metrics}
270 |
271 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
272 |         """Execute dimensionality reduction task"""
273 |         return await self.train(data, params.get('target'), **params)
274 |
275 |
276 | class AnomalyDetectionEngine(BaseMLModel):
277 |     """Engine for anomaly detection"""
278 |
279 |     def __init__(self, algorithm: str):
280 |         super().__init__(f"anomaly_{algorithm}", "anomaly_detection")
281 |         self.algorithm = algorithm
282 |
283 |     async def _setup(self):
284 |         """Initialize anomaly detection model"""
285 |         if self.algorithm == "isolation":
286 |             from sklearn.ensemble import IsolationForest
287 |             self.model = IsolationForest(random_state=42)
288 |
289 |         elif self.algorithm == "lof":
290 |             from sklearn.neighbors import LocalOutlierFactor
291 |             self.model = LocalOutlierFactor(novelty=True)
292 |
293 |         elif self.algorithm == "ocsvm":
294 |             from sklearn.svm import OneClassSVM
295 |             self.model = OneClassSVM()
296 |
297 |         elif self.algorithm == "elliptic":
298 |             from sklearn.covariance import EllipticEnvelope
299 |             self.model = EllipticEnvelope(random_state=42)
300 |
301 |         else:
302 |             raise ValueError(f"Unknown anomaly detection algorithm: {self.algorithm}")
303 |
304 |     async def train(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
305 |         """Train anomaly detection model"""
306 |         if not self._initialized:
307 |             await self.initialize()
308 |
309 |         start_time = time.time()
310 |
311 |         # Fit model
312 |         self.model.fit(X)
313 |         self.is_trained = True
314 |
315 |         # Predict anomalies on training data
316 |         predictions = self.model.predict(X)
317 |         anomaly_scores = None
318 |
319 |         if hasattr(self.model, 'score_samples'):
320 |             anomaly_scores = self.model.score_samples(X)
321 |         elif hasattr(self.model, 'decision_function'):
322 |             anomaly_scores = self.model.decision_function(X)
323 |
324 |         # Calculate statistics
325 |         n_anomalies = np.sum(predictions == -1)
326 |         anomaly_ratio = n_anomalies / len(predictions)
327 |
328 |         training_time = time.time() - start_time
329 |
330 |         return {
331 |             "algorithm": self.algorithm,
332 |             "n_samples": len(X),
333 |             "n_anomalies": int(n_anomalies),
334 |             "anomaly_ratio": float(anomaly_ratio),
335 |             "predictions": predictions.tolist(),
336 |             "anomaly_scores": anomaly_scores.tolist() if anomaly_scores is not None else None,
337 |             "training_time": training_time
338 |         }
339 |
340 |     async def predict(self, X: Any, **kwargs) -> Any:
341 |         """Predict anomalies"""
342 |         if not self.is_trained:
343 |             raise RuntimeError("Model not trained yet")
344 |
345 |         predictions = self.model.predict(X)
346 |
347 |         if kwargs.get('return_scores', False):
348 |             if hasattr(self.model, 'score_samples'):
349 |                 scores = self.model.score_samples(X)
350 |             elif hasattr(self.model, 'decision_function'):
351 |                 scores = self.model.decision_function(X)
352 |             else:
353 |                 scores = None
354 |
355 |             return {'predictions': predictions, 'scores': scores}
356 |
357 |         return predictions
358 |
359 |     async def evaluate(self, X: Any, y: Any = None, **kwargs) -> Dict[str, Any]:
360 |         """Evaluate anomaly detection"""
361 |         if not self.is_trained:
362 |             raise RuntimeError("Model not trained yet")
363 |
364 |         predictions = self.model.predict(X)
365 |
366 |         metrics = {
367 |             "n_anomalies": int(np.sum(predictions == -1)),
368 |             "anomaly_ratio": float(np.sum(predictions == -1) / len(predictions))
369 |         }
370 |
371 |         # If true labels provided, calculate accuracy metrics
372 |         if y is not None:
373 |             from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
374 |
375 |             # Convert -1/1 to 0/1 for metrics
376 |             pred_binary = (predictions == -1).astype(int)
377 |             y_binary = (y == -1).astype(int) if -1 in y else y
378 |
379 |             metrics.update({
380 |                 "accuracy": float(accuracy_score(y_binary, pred_binary)),
381 |                 "precision": float(precision_score(y_binary, pred_binary, zero_division=0)),
382 |                 "recall": float(recall_score(y_binary, pred_binary, zero_division=0)),
383 |                 "f1_score": float(f1_score(y_binary, pred_binary, zero_division=0))
384 |             })
385 |
386 |         return {"metrics": metrics}
387 |
388 |     async def execute(self, data: Any, params: Dict[str, Any]) -> Any:
389 |         """Execute anomaly detection task"""
390 |         return await self.train(data, **params)
391 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/__init__.py:
--------------------------------------------------------------------------------
 1 | """
 2 | Statistical Engines for OpenStatica
 3 | """
 4 |
 5 | from typing import List
 6 | from app.core.registry import EngineRegistry
 7 | from .descriptive import DescriptiveAnalyzer, FrequencyAnalyzer
 8 | from .inferential import TTestAnalyzer, ANOVAAnalyzer
 9 |
10 |
11 | async def initialize_statistical_engines(registry: EngineRegistry) -> List[str]:
12 |     """
13 |     Pre-register commonly used statistical engines
14 |     """
15 |     names = []
16 |     engines = [
17 |         DescriptiveAnalyzer(),
18 |         FrequencyAnalyzer(),
19 |         TTestAnalyzer(),
20 |         ANOVAAnalyzer()
21 |     ]
22 |
23 |     for engine in engines:
24 |         if not registry.get(engine.name):
25 |             await registry.register(engine)
26 |             names.append(engine.name)
27 |
28 |     return names
29 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/descriptive.py:
--------------------------------------------------------------------------------
  1 | from typing import List, Dict, Any
  2 | import pandas as pd
  3 | import numpy as np
  4 | from scipy import stats
  5 | from app.core.base import BaseAnalyzer, Result
  6 |
  7 |
  8 | class DescriptiveAnalyzer(BaseAnalyzer):
  9 |     """Analyzer for descriptive statistics"""
 10 |
 11 |     def __init__(self):
 12 |         super().__init__("descriptive_statistics")
 13 |
 14 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 15 |         """Calculate descriptive statistics"""
 16 |         try:
 17 |             columns = params.get("columns", data.columns.tolist())
 18 |             include_advanced = params.get("include_advanced", True)
 19 |
 20 |             results = {}
 21 |
 22 |             for col in columns:
 23 |                 if col not in data.columns:
 24 |                     continue
 25 |
 26 |                 if pd.api.types.is_numeric_dtype(data[col]):
 27 |                     results[col] = self._analyze_numeric(data[col], include_advanced)
 28 |                 else:
 29 |                     results[col] = self._analyze_categorical(data[col])
 30 |
 31 |             return Result.ok(results, columns=columns)
 32 |
 33 |         except Exception as e:
 34 |             return Result.fail(str(e))
 35 |
 36 |     def _analyze_numeric(self, series: pd.Series, include_advanced: bool) -> Dict[str, Any]:
 37 |         """Analyze numeric column"""
 38 |         clean_data = series.dropna()
 39 |
 40 |         stats_dict = {
 41 |             # Basic statistics
 42 |             "count": len(clean_data),
 43 |             "missing": series.isna().sum(),
 44 |             "missing_pct": (series.isna().sum() / len(series)) * 100,
 45 |             "mean": float(clean_data.mean()),
 46 |             "median": float(clean_data.median()),
 47 |             "mode": float(clean_data.mode()[0]) if len(clean_data.mode()) > 0 else None,
 48 |             "std": float(clean_data.std()),
 49 |             "variance": float(clean_data.var()),
 50 |             "min": float(clean_data.min()),
 51 |             "max": float(clean_data.max()),
 52 |             "range": float(clean_data.max() - clean_data.min()),
 53 |
 54 |             # Quartiles
 55 |             "q1": float(clean_data.quantile(0.25)),
 56 |             "q2": float(clean_data.quantile(0.50)),
 57 |             "q3": float(clean_data.quantile(0.75)),
 58 |             "iqr": float(clean_data.quantile(0.75) - clean_data.quantile(0.25)),
 59 |
 60 |             # Percentiles
 61 |             "percentiles": {
 62 |                 f"p{p}": float(clean_data.quantile(p / 100))
 63 |                 for p in [5, 10, 25, 50, 75, 90, 95, 99]
 64 |             }
 65 |         }
 66 |
 67 |         if include_advanced:
 68 |             # Advanced statistics
 69 |             stats_dict.update({
 70 |                 "skewness": float(clean_data.skew()),
 71 |                 "kurtosis": float(clean_data.kurtosis()),
 72 |                 "sem": float(clean_data.sem()),
 73 |                 "mad": float(clean_data.mad()),
 74 |                 "cv": float((clean_data.std() / clean_data.mean()) * 100) if clean_data.mean() != 0 else None,
 75 |                 "geometric_mean": float(stats.gmean(clean_data[clean_data > 0])) if (clean_data > 0).any() else None,
 76 |                 "harmonic_mean": float(stats.hmean(clean_data[clean_data > 0])) if (clean_data > 0).any() else None,
 77 |                 "trimmed_mean": float(stats.trim_mean(clean_data, 0.1)),
 78 |
 79 |                 # Confidence intervals
 80 |                 "ci_95": self._confidence_interval(clean_data, 0.95),
 81 |                 "ci_99": self._confidence_interval(clean_data, 0.99),
 82 |
 83 |                 # Outliers (using IQR method)
 84 |                 "outliers": self._detect_outliers(clean_data),
 85 |
 86 |                 # Normality test
 87 |                 "normality": self._test_normality(clean_data)
 88 |             })
 89 |
 90 |         return stats_dict
 91 |
 92 |     def _analyze_categorical(self, series: pd.Series) -> Dict[str, Any]:
 93 |         """Analyze categorical column"""
 94 |         value_counts = series.value_counts()
 95 |         value_pcts = series.value_counts(normalize=True) * 100
 96 |
 97 |         return {
 98 |             "count": len(series.dropna()),
 99 |             "missing": series.isna().sum(),
100 |             "missing_pct": (series.isna().sum() / len(series)) * 100,
101 |             "unique": series.nunique(),
102 |             "unique_pct": (series.nunique() / len(series)) * 100,
103 |             "mode": series.mode()[0] if len(series.mode()) > 0 else None,
104 |             "mode_freq": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
105 |             "mode_pct": float(value_pcts.iloc[0]) if len(value_pcts) > 0 else 0,
106 |
107 |             # Top values
108 |             "top_values": value_counts.head(10).to_dict(),
109 |             "top_percentages": value_pcts.head(10).to_dict(),
110 |
111 |             # Entropy (measure of randomness)
112 |             "entropy": float(stats.entropy(value_counts))
113 |         }
114 |
115 |     def _confidence_interval(self, data: pd.Series, confidence: float) -> List[float]:
116 |         """Calculate confidence interval"""
117 |         mean = data.mean()
118 |         sem = data.sem()
119 |         interval = stats.t.interval(confidence, len(data) - 1, loc=mean, scale=sem)
120 |         return [float(interval[0]), float(interval[1])]
121 |
122 |     def _detect_outliers(self, data: pd.Series) -> Dict[str, Any]:
123 |         """Detect outliers using IQR method"""
124 |         q1 = data.quantile(0.25)
125 |         q3 = data.quantile(0.75)
126 |         iqr = q3 - q1
127 |
128 |         lower_bound = q1 - 1.5 * iqr
129 |         upper_bound = q3 + 1.5 * iqr
130 |
131 |         outliers = data[(data < lower_bound) | (data > upper_bound)]
132 |
133 |         return {
134 |             "count": len(outliers),
135 |             "percentage": (len(outliers) / len(data)) * 100,
136 |             "lower_bound": float(lower_bound),
137 |             "upper_bound": float(upper_bound),
138 |             "values": outliers.tolist()[:100]  # Limit to 100 outliers
139 |         }
140 |
141 |     def _test_normality(self, data: pd.Series) -> Dict[str, Any]:
142 |         """Test for normality"""
143 |         if len(data) < 3:
144 |             return {"error": "Not enough data for normality test"}
145 |
146 |         # Shapiro-Wilk test (good for small samples)
147 |         if len(data) <= 5000:
148 |             stat, p_value = stats.shapiro(data)
149 |             test_name = "Shapiro-Wilk"
150 |         else:
151 |             # Kolmogorov-Smirnov test for larger samples
152 |             stat, p_value = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
153 |             test_name = "Kolmogorov-Smirnov"
154 |
155 |         return {
156 |             "test": test_name,
157 |             "statistic": float(stat),
158 |             "p_value": float(p_value),
159 |             "is_normal": p_value > 0.05
160 |         }
161 |
162 |
163 | class FrequencyAnalyzer(BaseAnalyzer):
164 |     """Analyzer for frequency distributions"""
165 |
166 |     def __init__(self):
167 |         super().__init__("frequency_distribution")
168 |
169 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
170 |         """Calculate frequency distributions"""
171 |         try:
172 |             columns = params.get("columns", data.columns.tolist())
173 |             bins = params.get("bins", "auto")
174 |
175 |             results = {}
176 |
177 |             for col in columns:
178 |                 if col not in data.columns:
179 |                     continue
180 |
181 |                 if pd.api.types.is_numeric_dtype(data[col]):
182 |                     results[col] = self._numeric_frequency(data[col], bins)
183 |                 else:
184 |                     results[col] = self._categorical_frequency(data[col])
185 |
186 |             return Result.ok(results)
187 |
188 |         except Exception as e:
189 |             return Result.fail(str(e))
190 |
191 |     def _numeric_frequency(self, series: pd.Series, bins) -> Dict[str, Any]:
192 |         """Calculate frequency for numeric data"""
193 |         clean_data = series.dropna()
194 |
195 |         # Create histogram
196 |         if bins == "auto":
197 |             bins = min(50, int(np.sqrt(len(clean_data))))
198 |
199 |         counts, edges = np.histogram(clean_data, bins=bins)
200 |
201 |         # Create bins labels
202 |         bin_labels = [f"{edges[i]:.2f}-{edges[i + 1]:.2f}" for i in range(len(edges) - 1)]
203 |
204 |         return {
205 |             "type": "numeric",
206 |             "bins": bin_labels,
207 |             "frequencies": counts.tolist(),
208 |             "relative_frequencies": (counts / len(clean_data)).tolist(),
209 |             "cumulative_frequencies": np.cumsum(counts).tolist(),
210 |             "bin_edges": edges.tolist(),
211 |             "statistics": {
212 |                 "total": len(clean_data),
213 |                 "bin_width": float(edges[1] - edges[0]),
214 |                 "num_bins": len(bin_labels)
215 |             }
216 |         }
217 |
218 |     def _categorical_frequency(self, series: pd.Series) -> Dict[str, Any]:
219 |         """Calculate frequency for categorical data"""
220 |         value_counts = series.value_counts()
221 |         value_pcts = series.value_counts(normalize=True) * 100
222 |         cumulative_pcts = value_pcts.cumsum()
223 |
224 |         return {
225 |             "type": "categorical",
226 |             "categories": value_counts.index.tolist(),
227 |             "frequencies": value_counts.values.tolist(),
228 |             "relative_frequencies": value_pcts.values.tolist(),
229 |             "cumulative_frequencies": cumulative_pcts.values.tolist(),
230 |             "statistics": {
231 |                 "total": len(series.dropna()),
232 |                 "unique_values": series.nunique(),
233 |                 "mode": value_counts.index[0] if len(value_counts) > 0 else None
234 |             }
235 |         }
236 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/inferential.py:
--------------------------------------------------------------------------------
  1 | from typing import Dict, Any, List, Optional
  2 | import pandas as pd
  3 | import numpy as np
  4 | from scipy import stats
  5 | import pingouin as pg
  6 | from app.core.base import BaseAnalyzer, Result
  7 |
  8 |
  9 | class TTestAnalyzer(BaseAnalyzer):
 10 |     """Analyzer for T-tests"""
 11 |
 12 |     def __init__(self):
 13 |         super().__init__("ttest")
 14 |
 15 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 16 |         """Perform T-test"""
 17 |         try:
 18 |             test_type = params.get("test_type", "independent")
 19 |
 20 |             if test_type == "one_sample":
 21 |                 return await self._one_sample_ttest(data, params)
 22 |             elif test_type == "independent":
 23 |                 return await self._independent_ttest(data, params)
 24 |             elif test_type == "paired":
 25 |                 return await self._paired_ttest(data, params)
 26 |             else:
 27 |                 return Result.fail(f"Unknown test type: {test_type}")
 28 |
 29 |         except Exception as e:
 30 |             return Result.fail(str(e))
 31 |
 32 |     async def _one_sample_ttest(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 33 |         """One-sample T-test"""
 34 |         column = params.get("column")
 35 |         test_value = params.get("test_value", 0)
 36 |         alternative = params.get("alternative", "two-sided")
 37 |
 38 |         sample_data = data[column].dropna()
 39 |
 40 |         # Perform test
 41 |         t_stat, p_value = stats.ttest_1samp(sample_data, test_value)
 42 |
 43 |         # Calculate effect size (Cohen's d)
 44 |         cohens_d = (sample_data.mean() - test_value) / sample_data.std()
 45 |
 46 |         # Confidence interval
 47 |         ci = stats.t.interval(0.95, len(sample_data) - 1,
 48 |                               loc=sample_data.mean(),
 49 |                               scale=sample_data.sem())
 50 |
 51 |         results = {
 52 |             "test": "One-Sample T-Test",
 53 |             "variable": column,
 54 |             "test_value": test_value,
 55 |             "sample_size": len(sample_data),
 56 |             "sample_mean": float(sample_data.mean()),
 57 |             "sample_std": float(sample_data.std()),
 58 |             "t_statistic": float(t_stat),
 59 |             "p_value": float(p_value),
 60 |             "degrees_of_freedom": len(sample_data) - 1,
 61 |             "confidence_interval_95": [float(ci[0]), float(ci[1])],
 62 |             "effect_size": {
 63 |                 "cohens_d": float(cohens_d),
 64 |                 "interpretation": self._interpret_cohens_d(cohens_d)
 65 |             },
 66 |             "conclusion": self._interpret_p_value(p_value)
 67 |         }
 68 |
 69 |         return Result.ok(results)
 70 |
 71 |     async def _independent_ttest(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 72 |         """Independent samples T-test"""
 73 |         group_col = params.get("group_column")
 74 |         value_col = params.get("value_column")
 75 |
 76 |         # Get unique groups
 77 |         groups = data[group_col].unique()
 78 |         if len(groups) != 2:
 79 |             return Result.fail(f"Expected 2 groups, found {len(groups)}")
 80 |
 81 |         # Split data into groups
 82 |         group1_data = data[data[group_col] == groups[0]][value_col].dropna()
 83 |         group2_data = data[data[group_col] == groups[1]][value_col].dropna()
 84 |
 85 |         # Test for equal variances (Levene's test)
 86 |         levene_stat, levene_p = stats.levene(group1_data, group2_data)
 87 |
 88 |         # Perform T-test
 89 |         equal_var = levene_p > 0.05
 90 |         t_stat, p_value = stats.ttest_ind(group1_data, group2_data, equal_var=equal_var)
 91 |
 92 |         # Calculate effect size
 93 |         pooled_std = np.sqrt(((len(group1_data) - 1) * group1_data.var() +
 94 |                               (len(group2_data) - 1) * group2_data.var()) /
 95 |                              (len(group1_data) + len(group2_data) - 2))
 96 |         cohens_d = (group1_data.mean() - group2_data.mean()) / pooled_std
 97 |
 98 |         results = {
 99 |             "test": "Independent Samples T-Test",
100 |             "groups": {
101 |                 str(groups[0]): {
102 |                     "n": len(group1_data),
103 |                     "mean": float(group1_data.mean()),
104 |                     "std": float(group1_data.std()),
105 |                     "sem": float(group1_data.sem())
106 |                 },
107 |                 str(groups[1]): {
108 |                     "n": len(group2_data),
109 |                     "mean": float(group2_data.mean()),
110 |                     "std": float(group2_data.std()),
111 |                     "sem": float(group2_data.sem())
112 |                 }
113 |             },
114 |             "mean_difference": float(group1_data.mean() - group2_data.mean()),
115 |             "t_statistic": float(t_stat),
116 |             "p_value": float(p_value),
117 |             "degrees_of_freedom": len(group1_data) + len(group2_data) - 2,
118 |             "equal_variances_assumed": equal_var,
119 |             "levene_test": {
120 |                 "statistic": float(levene_stat),
121 |                 "p_value": float(levene_p)
122 |             },
123 |             "effect_size": {
124 |                 "cohens_d": float(cohens_d),
125 |                 "interpretation": self._interpret_cohens_d(cohens_d)
126 |             },
127 |             "conclusion": self._interpret_p_value(p_value)
128 |         }
129 |
130 |         return Result.ok(results)
131 |
132 |     async def _paired_ttest(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
133 |         """Paired samples T-test"""
134 |         before_col = params.get("before_column")
135 |         after_col = params.get("after_column")
136 |
137 |         # Remove rows with missing values
138 |         paired_data = data[[before_col, after_col]].dropna()
139 |         before_data = paired_data[before_col]
140 |         after_data = paired_data[after_col]
141 |
142 |         # Calculate differences
143 |         differences = after_data - before_data
144 |
145 |         # Perform test
146 |         t_stat, p_value = stats.ttest_rel(before_data, after_data)
147 |
148 |         # Effect size
149 |         cohens_d = differences.mean() / differences.std()
150 |
151 |         # Confidence interval for mean difference
152 |         ci = stats.t.interval(0.95, len(differences) - 1,
153 |                               loc=differences.mean(),
154 |                               scale=differences.sem())
155 |
156 |         results = {
157 |             "test": "Paired Samples T-Test",
158 |             "n_pairs": len(paired_data),
159 |             "before": {
160 |                 "mean": float(before_data.mean()),
161 |                 "std": float(before_data.std())
162 |             },
163 |             "after": {
164 |                 "mean": float(after_data.mean()),
165 |                 "std": float(after_data.std())
166 |             },
167 |             "difference": {
168 |                 "mean": float(differences.mean()),
169 |                 "std": float(differences.std()),
170 |                 "sem": float(differences.sem()),
171 |                 "min": float(differences.min()),
172 |                 "max": float(differences.max())
173 |             },
174 |             "t_statistic": float(t_stat),
175 |             "p_value": float(p_value),
176 |             "degrees_of_freedom": len(differences) - 1,
177 |             "confidence_interval_95": [float(ci[0]), float(ci[1])],
178 |             "effect_size": {
179 |                 "cohens_d": float(cohens_d),
180 |                 "interpretation": self._interpret_cohens_d(cohens_d)
181 |             },
182 |             "conclusion": self._interpret_p_value(p_value)
183 |         }
184 |
185 |         return Result.ok(results)
186 |
187 |     def _interpret_cohens_d(self, d: float) -> str:
188 |         """Interpret Cohen's d effect size"""
189 |         d = abs(d)
190 |         if d < 0.2:
191 |             return "negligible"
192 |         elif d < 0.5:
193 |             return "small"
194 |         elif d < 0.8:
195 |             return "medium"
196 |         else:
197 |             return "large"
198 |
199 |     def _interpret_p_value(self, p: float, alpha: float = 0.05) -> str:
200 |         """Interpret p-value"""
201 |         if p < alpha:
202 |             return f"Statistically significant (p={p:.4f} < {alpha})"
203 |         else:
204 |             return f"Not statistically significant (p={p:.4f} >= {alpha})"
205 |
206 |
207 | class ANOVAAnalyzer(BaseAnalyzer):
208 |     """Analyzer for ANOVA tests"""
209 |
210 |     def __init__(self):
211 |         super().__init__("anova")
212 |
213 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
214 |         """Perform ANOVA"""
215 |         try:
216 |             anova_type = params.get("type", "one_way")
217 |
218 |             if anova_type == "one_way":
219 |                 return await self._one_way_anova(data, params)
220 |             elif anova_type == "two_way":
221 |                 return await self._two_way_anova(data, params)
222 |             elif anova_type == "repeated_measures":
223 |                 return await self._repeated_measures_anova(data, params)
224 |             else:
225 |                 return Result.fail(f"Unknown ANOVA type: {anova_type}")
226 |
227 |         except Exception as e:
228 |             return Result.fail(str(e))
229 |
230 |     async def _one_way_anova(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
231 |         """One-way ANOVA"""
232 |         dependent = params.get("dependent")
233 |         factor = params.get("factor")
234 |
235 |         # Perform ANOVA
236 |         aov = pg.anova(data=data, dv=dependent, between=factor)
237 |
238 |         # Post-hoc tests if significant
239 |         p_value = aov['p-unc'].values[0]
240 |         post_hoc = None
241 |         if p_value < 0.05:
242 |             post_hoc = pg.pairwise_tukey(data=data, dv=dependent, between=factor)
243 |
244 |         # Homogeneity of variance test
245 |         homogeneity = pg.homoscedasticity(data=data, dv=dependent, group=factor)
246 |
247 |         # Effect size (eta-squared)
248 |         ss_between = aov['SS'].values[0]
249 |         ss_total = aov['SS'].sum()
250 |         eta_squared = ss_between / ss_total
251 |
252 |         results = {
253 |             "test": "One-Way ANOVA",
254 |             "dependent": dependent,
255 |             "factor": factor,
256 |             "anova_table": aov.to_dict(orient='records')[0],
257 |             "effect_size": {
258 |                 "eta_squared": float(eta_squared),
259 |                 "interpretation": self._interpret_eta_squared(eta_squared)
260 |             },
261 |             "homogeneity_test": homogeneity.to_dict(orient='records')[0],
262 |             "post_hoc": post_hoc.to_dict(orient='records') if post_hoc is not None else None,
263 |             "conclusion": self._interpret_p_value(p_value)
264 |         }
265 |
266 |         return Result.ok(results)
267 |
268 |     async def _two_way_anova(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
269 |         """Two-way ANOVA"""
270 |         dependent = params.get("dependent")
271 |         factor1 = params.get("factor1")
272 |         factor2 = params.get("factor2")
273 |
274 |         # Perform two-way ANOVA
275 |         aov = pg.anova(data=data, dv=dependent, between=[factor1, factor2])
276 |
277 |         results = {
278 |             "test": "Two-Way ANOVA",
279 |             "dependent": dependent,
280 |             "factors": [factor1, factor2],
281 |             "anova_table": aov.to_dict(orient='records'),
282 |             "main_effects": {
283 |                 factor1: {
284 |                     "F": float(aov.loc[aov['Source'] == factor1, 'F'].values[0]),
285 |                     "p_value": float(aov.loc[aov['Source'] == factor1, 'p-unc'].values[0])
286 |                 },
287 |                 factor2: {
288 |                     "F": float(aov.loc[aov['Source'] == factor2, 'F'].values[0]),
289 |                     "p_value": float(aov.loc[aov['Source'] == factor2, 'p-unc'].values[0])
290 |                 }
291 |             },
292 |             "interaction": {
293 |                 "F": float(aov.loc[aov['Source'].str.contains('*'), 'F'].values[0]) if any(
294 |                     aov['Source'].str.contains('*')) else None,
295 |                 "p_value": float(aov.loc[aov['Source'].str.contains('*'), 'p-unc'].values[0]) if any(
296 |                     aov['Source'].str.contains('*')) else None
297 |             }
298 |         }
299 |
300 |         return Result.ok(results)
301 |
302 |     async def _repeated_measures_anova(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
303 |         """Repeated measures ANOVA"""
304 |         subject = params.get("subject")
305 |         within = params.get("within")
306 |         dependent = params.get("dependent")
307 |
308 |         # Perform repeated measures ANOVA
309 |         aov = pg.rm_anova(data=data, dv=dependent, within=within, subject=subject)
310 |
311 |         # Sphericity test
312 |         sphericity = pg.sphericity(data=data, dv=dependent, within=within, subject=subject)
313 |
314 |         results = {
315 |             "test": "Repeated Measures ANOVA",
316 |             "dependent": dependent,
317 |             "within_factor": within,
318 |             "anova_table": aov.to_dict(orient='records')[0],
319 |             "sphericity_test": sphericity.to_dict(orient='records')[0] if sphericity is not None else None
320 |         }
321 |
322 |         return Result.ok(results)
323 |
324 |     def _interpret_eta_squared(self, eta: float) -> str:
325 |         """Interpret eta-squared effect size"""
326 |         if eta < 0.01:
327 |             return "negligible"
328 |         elif eta < 0.06:
329 |             return "small"
330 |         elif eta < 0.14:
331 |             return "medium"
332 |         else:
333 |             return "large"
334 |
335 |     def _interpret_p_value(self, p: float, alpha: float = 0.05) -> str:
336 |         """Interpret p-value"""
337 |         if p < alpha:
338 |             return f"Statistically significant (p={p:.4f} < {alpha})"
339 |         else:
340 |             return f"Not statistically significant (p={p:.4f} >= {alpha})"
341 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/multivariate.py:
--------------------------------------------------------------------------------
  1 | from typing import Dict, Any, List, Optional
  2 | import pandas as pd
  3 | import numpy as np
  4 | from sklearn.decomposition import PCA, FactorAnalysis
  5 | from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  6 | from sklearn.preprocessing import StandardScaler
  7 | from factor_analyzer import FactorAnalyzer
  8 | from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
  9 | from app.core.base import BaseAnalyzer, Result
 10 |
 11 |
 12 | class MultivariateAnalyzer(BaseAnalyzer):
 13 |     """Analyzer for multivariate statistical methods"""
 14 |
 15 |     def __init__(self):
 16 |         super().__init__("multivariate_analysis")
 17 |
 18 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 19 |         """Execute multivariate analysis"""
 20 |         try:
 21 |             analysis_type = params.get("type")
 22 |
 23 |             if analysis_type == "pca":
 24 |                 return await self._pca_analysis(data, params)
 25 |             elif analysis_type == "factor":
 26 |                 return await self._factor_analysis(data, params)
 27 |             elif analysis_type == "lda":
 28 |                 return await self._lda_analysis(data, params)
 29 |             elif analysis_type == "canonical":
 30 |                 return await self._canonical_correlation(data, params)
 31 |             elif analysis_type == "manova":
 32 |                 return await self._manova(data, params)
 33 |             else:
 34 |                 return Result.fail(f"Unknown analysis type: {analysis_type}")
 35 |
 36 |         except Exception as e:
 37 |             return Result.fail(str(e))
 38 |
 39 |     async def _pca_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 40 |         """Principal Component Analysis"""
 41 |         variables = params.get("variables")
 42 |         n_components = params.get("n_components", None)
 43 |
 44 |         # Prepare data
 45 |         X = data[variables].dropna()
 46 |
 47 |         # Standardize
 48 |         scaler = StandardScaler()
 49 |         X_scaled = scaler.fit_transform(X)
 50 |
 51 |         # Perform PCA
 52 |         pca = PCA(n_components=n_components)
 53 |         X_pca = pca.fit_transform(X_scaled)
 54 |
 55 |         # Calculate loadings
 56 |         loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
 57 |
 58 |         # Create loadings DataFrame
 59 |         loadings_df = pd.DataFrame(
 60 |             loadings,
 61 |             columns=[f'PC{i + 1}' for i in range(len(pca.components_))],
 62 |             index=variables
 63 |         )
 64 |
 65 |         # Kaiser criterion (eigenvalues > 1)
 66 |         eigenvalues = pca.explained_variance_
 67 |         n_kaiser = np.sum(eigenvalues > 1)
 68 |
 69 |         results = {
 70 |             "method": "Principal Component Analysis",
 71 |             "n_observations": len(X),
 72 |             "n_variables": len(variables),
 73 |             "n_components": len(pca.components_),
 74 |             "n_components_kaiser": int(n_kaiser),
 75 |             "explained_variance": pca.explained_variance_.tolist(),
 76 |             "explained_variance_ratio": pca.explained_variance_ratio_.tolist(),
 77 |             "cumulative_variance_ratio": np.cumsum(pca.explained_variance_ratio_).tolist(),
 78 |             "eigenvalues": eigenvalues.tolist(),
 79 |             "loadings": loadings_df.to_dict(),
 80 |             "component_scores": {
 81 |                 f"PC{i + 1}": X_pca[:, i].tolist()[:100]  # First 100 scores
 82 |                 for i in range(min(3, len(pca.components_)))
 83 |             },
 84 |             "interpretation": self._interpret_pca(pca.explained_variance_ratio_)
 85 |         }
 86 |
 87 |         return Result.ok(results)
 88 |
 89 |     async def _factor_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 90 |         """Exploratory Factor Analysis"""
 91 |         variables = params.get("variables")
 92 |         n_factors = params.get("n_factors", None)
 93 |         rotation = params.get("rotation", "varimax")
 94 |
 95 |         # Prepare data
 96 |         X = data[variables].dropna()
 97 |
 98 |         # Test assumptions
 99 |         chi_square, p_value = calculate_bartlett_sphericity(X)
100 |         kmo_all, kmo_model = calculate_kmo(X)
101 |
102 |         # Determine number of factors if not specified
103 |         if n_factors is None:
104 |             # Use eigenvalue > 1 criterion
105 |             fa_test = FactorAnalyzer(n_factors=len(variables), rotation=None)
106 |             fa_test.fit(X)
107 |             eigenvalues = fa_test.get_eigenvalues()[0]
108 |             n_factors = np.sum(eigenvalues > 1)
109 |
110 |         # Perform factor analysis
111 |         fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)
112 |         fa.fit(X)
113 |
114 |         # Get results
115 |         loadings = fa.loadings_
116 |         communalities = fa.get_communalities()
117 |         uniqueness = fa.get_uniquenesses()
118 |         factor_variance = fa.get_factor_variance()
119 |
120 |         # Create loadings DataFrame
121 |         loadings_df = pd.DataFrame(
122 |             loadings,
123 |             columns=[f'Factor{i + 1}' for i in range(n_factors)],
124 |             index=variables
125 |         )
126 |
127 |         results = {
128 |             "method": "Exploratory Factor Analysis",
129 |             "n_observations": len(X),
130 |             "n_variables": len(variables),
131 |             "n_factors": n_factors,
132 |             "rotation": rotation,
133 |             "bartlett_test": {
134 |                 "chi_square": float(chi_square),
135 |                 "p_value": float(p_value),
136 |                 "suitable": p_value < 0.05
137 |             },
138 |             "kmo": {
139 |                 "overall": float(kmo_model),
140 |                 "variables": {var: float(kmo_all[i]) for i, var in enumerate(variables)},
141 |                 "interpretation": self._interpret_kmo(kmo_model)
142 |             },
143 |             "loadings": loadings_df.to_dict(),
144 |             "communalities": {var: float(communalities[i]) for i, var in enumerate(variables)},
145 |             "uniqueness": {var: float(uniqueness[i]) for i, var in enumerate(variables)},
146 |             "variance_explained": {
147 |                 "SS_loadings": factor_variance[0].tolist(),
148 |                 "proportion_var": factor_variance[1].tolist(),
149 |                 "cumulative_var": factor_variance[2].tolist()
150 |             }
151 |         }
152 |
153 |         return Result.ok(results)
154 |
155 |     async def _lda_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
156 |         """Linear Discriminant Analysis"""
157 |         features = params.get("features")
158 |         target = params.get("target")
159 |
160 |         # Prepare data
161 |         X = data[features].dropna()
162 |         y = data.loc[X.index, target]
163 |
164 |         # Standardize features
165 |         scaler = StandardScaler()
166 |         X_scaled = scaler.fit_transform(X)
167 |
168 |         # Perform LDA
169 |         lda = LinearDiscriminantAnalysis()
170 |         X_lda = lda.fit_transform(X_scaled, y)
171 |
172 |         # Get results
173 |         n_components = min(len(np.unique(y)) - 1, len(features))
174 |
175 |         results = {
176 |             "method": "Linear Discriminant Analysis",
177 |             "n_observations": len(X),
178 |             "n_features": len(features),
179 |             "n_classes": len(np.unique(y)),
180 |             "n_components": n_components,
181 |             "explained_variance_ratio": lda.explained_variance_ratio_.tolist() if hasattr(lda,
182 |                                                                                           'explained_variance_ratio_') else None,
183 |             "coefficients": lda.coef_.tolist(),
184 |             "intercept": lda.intercept_.tolist(),
185 |             "means": {
186 |                 str(cls): lda.means_[i].tolist()
187 |                 for i, cls in enumerate(lda.classes_)
188 |             },
189 |             "classification_accuracy": float(lda.score(X_scaled, y))
190 |         }
191 |
192 |         return Result.ok(results)
193 |
194 |     async def _canonical_correlation(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
195 |         """Canonical Correlation Analysis"""
196 |         x_vars = params.get("x_variables")
197 |         y_vars = params.get("y_variables")
198 |
199 |         # Prepare data
200 |         X = data[x_vars].dropna()
201 |         Y = data.loc[X.index, y_vars]
202 |
203 |         # Standardize
204 |         X_std = (X - X.mean()) / X.std()
205 |         Y_std = (Y - Y.mean()) / Y.std()
206 |
207 |         # Calculate correlation matrices
208 |         n = len(X)
209 |         Rxx = X_std.T @ X_std / n
210 |         Ryy = Y_std.T @ Y_std / n
211 |         Rxy = X_std.T @ Y_std / n
212 |         Ryx = Rxy.T
213 |
214 |         # Solve eigenvalue problem
215 |         Rxx_inv = np.linalg.inv(Rxx)
216 |         Ryy_inv = np.linalg.inv(Ryy)
217 |
218 |         M = Rxx_inv @ Rxy @ Ryy_inv @ Ryx
219 |         eigenvalues, eigenvectors = np.linalg.eig(M)
220 |
221 |         # Sort by eigenvalue
222 |         idx = eigenvalues.argsort()[::-1]
223 |         eigenvalues = eigenvalues[idx]
224 |         eigenvectors = eigenvectors[:, idx]
225 |
226 |         # Canonical correlations
227 |         canonical_corr = np.sqrt(np.real(eigenvalues))
228 |
229 |         # Canonical loadings
230 |         x_loadings = eigenvectors[:, :min(len(x_vars), len(y_vars))]
231 |
232 |         results = {
233 |             "method": "Canonical Correlation Analysis",
234 |             "n_observations": n,
235 |             "x_variables": x_vars,
236 |             "y_variables": y_vars,
237 |             "canonical_correlations": canonical_corr.tolist(),
238 |             "eigenvalues": eigenvalues.real.tolist(),
239 |             "wilks_lambda": float(np.prod(1 - eigenvalues[:min(len(x_vars), len(y_vars))])),
240 |             "x_loadings": x_loadings.real.tolist()
241 |         }
242 |
243 |         return Result.ok(results)
244 |
245 |     async def _manova(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
246 |         """Multivariate Analysis of Variance"""
247 |         dependent_vars = params.get("dependent_variables")
248 |         factor = params.get("factor")
249 |
250 |         # This is a simplified MANOVA implementation
251 |         # For production, use statsmodels.multivariate.manova
252 |
253 |         # Group data
254 |         groups = data.groupby(factor)
255 |         k = len(groups)  # number of groups
256 |         p = len(dependent_vars)  # number of dependent variables
257 |         n = len(data)
258 |
259 |         # Calculate SSCP matrices
260 |         # Total SSCP
261 |         Y = data[dependent_vars].values
262 |         Y_mean = Y.mean(axis=0)
263 |         T = (Y - Y_mean).T @ (Y - Y_mean)
264 |
265 |         # Within-groups SSCP
266 |         W = np.zeros((p, p))
267 |         for name, group in groups:
268 |             Y_g = group[dependent_vars].values
269 |             Y_g_mean = Y_g.mean(axis=0)
270 |             W += (Y_g - Y_g_mean).T @ (Y_g - Y_g_mean)
271 |
272 |         # Between-groups SSCP
273 |         B = T - W
274 |
275 |         # Wilks' Lambda
276 |         wilks_lambda = np.linalg.det(W) / np.linalg.det(T)
277 |
278 |         # Approximate F-statistic
279 |         df1 = p * (k - 1)
280 |         df2 = p * (n - k)
281 |         F = ((1 - wilks_lambda) / wilks_lambda) * (df2 / df1)
282 |
283 |         from scipy.stats import f
284 |         p_value = 1 - f.cdf(F, df1, df2)
285 |
286 |         results = {
287 |             "method": "MANOVA",
288 |             "dependent_variables": dependent_vars,
289 |             "factor": factor,
290 |             "n_groups": k,
291 |             "n_observations": n,
292 |             "wilks_lambda": float(wilks_lambda),
293 |             "f_statistic": float(F),
294 |             "df1": df1,
295 |             "df2": df2,
296 |             "p_value": float(p_value),
297 |             "conclusion": self._interpret_p_value(p_value)
298 |         }
299 |
300 |         return Result.ok(results)
301 |
302 |     def _interpret_pca(self, variance_ratio: np.ndarray) -> str:
303 |         """Interpret PCA results"""
304 |         cumsum = np.cumsum(variance_ratio)
305 |
306 |         # Find number of components for 80% variance
307 |         n_80 = np.argmax(cumsum >= 0.8) + 1
308 |         n_90 = np.argmax(cumsum >= 0.9) + 1
309 |
310 |         return (f"First {n_80} components explain {cumsum[n_80 - 1] * 100:.1f}% of variance. "
311 |                 f"First {n_90} components explain {cumsum[n_90 - 1] * 100:.1f}% of variance.")
312 |
313 |     def _interpret_kmo(self, kmo: float) -> str:
314 |         """Interpret KMO measure"""
315 |         if kmo >= 0.9:
316 |             return "Excellent"
317 |         elif kmo >= 0.8:
318 |             return "Good"
319 |         elif kmo >= 0.7:
320 |             return "Acceptable"
321 |         elif kmo >= 0.6:
322 |             return "Mediocre"
323 |         elif kmo >= 0.5:
324 |             return "Poor"
325 |         else:
326 |             return "Unacceptable"
327 |
328 |     def _interpret_p_value(self, p: float, alpha: float = 0.05) -> str:
329 |         """Interpret p-value"""
330 |         if p < alpha:
331 |             return f"Statistically significant (p={p:.4f} < {alpha})"
332 |         else:
333 |             return f"Not statistically significant (p={p:.4f} >= {alpha})"
334 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/regression.py:
--------------------------------------------------------------------------------
  1 | from typing import Dict, Any, List, Optional
  2 | import pandas as pd
  3 | import numpy as np
  4 | from scipy import stats
  5 | import statsmodels.api as sm
  6 | from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox
  7 | from statsmodels.stats.outliers_influence import variance_inflation_factor
  8 | from sklearn.preprocessing import PolynomialFeatures
  9 | from app.core.base import BaseAnalyzer, Result
 10 |
 11 |
 12 | class RegressionAnalyzer(BaseAnalyzer):
 13 |     """Analyzer for regression analysis"""
 14 |
 15 |     def __init__(self):
 16 |         super().__init__("regression_analysis")
 17 |
 18 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 19 |         """Perform regression analysis"""
 20 |         try:
 21 |             regression_type = params.get("type", "linear")
 22 |
 23 |             if regression_type == "linear":
 24 |                 return await self._linear_regression(data, params)
 25 |             elif regression_type == "multiple":
 26 |                 return await self._multiple_regression(data, params)
 27 |             elif regression_type == "polynomial":
 28 |                 return await self._polynomial_regression(data, params)
 29 |             elif regression_type == "logistic":
 30 |                 return await self._logistic_regression(data, params)
 31 |             elif regression_type == "stepwise":
 32 |                 return await self._stepwise_regression(data, params)
 33 |             elif regression_type == "quantile":
 34 |                 return await self._quantile_regression(data, params)
 35 |             else:
 36 |                 return Result.fail(f"Unknown regression type: {regression_type}")
 37 |
 38 |         except Exception as e:
 39 |             return Result.fail(str(e))
 40 |
 41 |     async def _linear_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 42 |         """Simple linear regression"""
 43 |         x_col = params.get("independent")
 44 |         y_col = params.get("dependent")
 45 |
 46 |         # Prepare data
 47 |         df = data[[x_col, y_col]].dropna()
 48 |         X = df[x_col]
 49 |         y = df[y_col]
 50 |
 51 |         # Add constant
 52 |         X_with_const = sm.add_constant(X)
 53 |
 54 |         # Fit model
 55 |         model = sm.OLS(y, X_with_const).fit()
 56 |
 57 |         # Get predictions
 58 |         predictions = model.predict(X_with_const)
 59 |         residuals = y - predictions
 60 |
 61 |         # Diagnostic tests
 62 |         diagnostics = self._regression_diagnostics(model, residuals, X_with_const)
 63 |
 64 |         results = {
 65 |             "model": "Simple Linear Regression",
 66 |             "formula": f"{y_col} ~ {x_col}",
 67 |             "n_observations": len(df),
 68 |             "coefficients": {
 69 |                 "intercept": float(model.params[0]),
 70 |                 x_col: float(model.params[1])
 71 |             },
 72 |             "std_errors": {
 73 |                 "intercept": float(model.bse[0]),
 74 |                 x_col: float(model.bse[1])
 75 |             },
 76 |             "t_values": {
 77 |                 "intercept": float(model.tvalues[0]),
 78 |                 x_col: float(model.tvalues[1])
 79 |             },
 80 |             "p_values": {
 81 |                 "intercept": float(model.pvalues[0]),
 82 |                 x_col: float(model.pvalues[1])
 83 |             },
 84 |             "confidence_intervals": {
 85 |                 "intercept": model.conf_int()[0].tolist(),
 86 |                 x_col: model.conf_int()[1].tolist()
 87 |             },
 88 |             "r_squared": float(model.rsquared),
 89 |             "adjusted_r_squared": float(model.rsquared_adj),
 90 |             "f_statistic": float(model.fvalue),
 91 |             "f_pvalue": float(model.f_pvalue),
 92 |             "aic": float(model.aic),
 93 |             "bic": float(model.bic),
 94 |             "diagnostics": diagnostics,
 95 |             "residuals_summary": {
 96 |                 "mean": float(residuals.mean()),
 97 |                 "std": float(residuals.std()),
 98 |                 "min": float(residuals.min()),
 99 |                 "max": float(residuals.max())
100 |             }
101 |         }
102 |
103 |         return Result.ok(results)
104 |
105 |     async def _multiple_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
106 |         """Multiple linear regression"""
107 |         independent_vars = params.get("independent", [])
108 |         dependent_var = params.get("dependent")
109 |
110 |         # Prepare data
111 |         all_vars = independent_vars + [dependent_var]
112 |         df = data[all_vars].dropna()
113 |         X = df[independent_vars]
114 |         y = df[dependent_var]
115 |
116 |         # Add constant
117 |         X_with_const = sm.add_constant(X)
118 |
119 |         # Fit model
120 |         model = sm.OLS(y, X_with_const).fit()
121 |
122 |         # Get predictions
123 |         predictions = model.predict(X_with_const)
124 |         residuals = y - predictions
125 |
126 |         # Calculate VIF for multicollinearity
127 |         vif_data = pd.DataFrame()
128 |         vif_data["Variable"] = X.columns
129 |         vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
130 |
131 |         # Diagnostic tests
132 |         diagnostics = self._regression_diagnostics(model, residuals, X_with_const)
133 |
134 |         results = {
135 |             "model": "Multiple Linear Regression",
136 |             "formula": f"{dependent_var} ~ {' + '.join(independent_vars)}",
137 |             "n_observations": len(df),
138 |             "n_predictors": len(independent_vars),
139 |             "coefficients": {
140 |                 "intercept": float(model.params[0]),
141 |                 **{var: float(model.params[i + 1]) for i, var in enumerate(independent_vars)}
142 |             },
143 |             "std_errors": {
144 |                 "intercept": float(model.bse[0]),
145 |                 **{var: float(model.bse[i + 1]) for i, var in enumerate(independent_vars)}
146 |             },
147 |             "t_values": {
148 |                 "intercept": float(model.tvalues[0]),
149 |                 **{var: float(model.tvalues[i + 1]) for i, var in enumerate(independent_vars)}
150 |             },
151 |             "p_values": {
152 |                 "intercept": float(model.pvalues[0]),
153 |                 **{var: float(model.pvalues[i + 1]) for i, var in enumerate(independent_vars)}
154 |             },
155 |             "confidence_intervals": {
156 |                 "intercept": model.conf_int()[0].tolist(),
157 |                 **{var: model.conf_int()[i + 1].tolist() for i, var in enumerate(independent_vars)}
158 |             },
159 |             "r_squared": float(model.rsquared),
160 |             "adjusted_r_squared": float(model.rsquared_adj),
161 |             "f_statistic": float(model.fvalue),
162 |             "f_pvalue": float(model.f_pvalue),
163 |             "aic": float(model.aic),
164 |             "bic": float(model.bic),
165 |             "vif": vif_data.to_dict(orient='records'),
166 |             "diagnostics": diagnostics
167 |         }
168 |
169 |         return Result.ok(results)
170 |
171 |     async def _polynomial_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
172 |         """Polynomial regression"""
173 |         x_col = params.get("independent")
174 |         y_col = params.get("dependent")
175 |         degree = params.get("degree", 2)
176 |
177 |         # Prepare data
178 |         df = data[[x_col, y_col]].dropna()
179 |         X = df[[x_col]]
180 |         y = df[y_col]
181 |
182 |         # Create polynomial features
183 |         poly = PolynomialFeatures(degree=degree, include_bias=False)
184 |         X_poly = poly.fit_transform(X)
185 |
186 |         # Create DataFrame with polynomial features
187 |         feature_names = [f"{x_col}^{i}" for i in range(1, degree + 1)]
188 |         X_poly_df = pd.DataFrame(X_poly, columns=feature_names, index=X.index)
189 |
190 |         # Add constant
191 |         X_with_const = sm.add_constant(X_poly_df)
192 |
193 |         # Fit model
194 |         model = sm.OLS(y, X_with_const).fit()
195 |
196 |         results = {
197 |             "model": f"Polynomial Regression (degree={degree})",
198 |             "formula": f"{y_col} ~ {' + '.join(feature_names)}",
199 |             "n_observations": len(df),
200 |             "degree": degree,
201 |             "coefficients": {
202 |                 "intercept": float(model.params[0]),
203 |                 **{feat: float(model.params[i + 1]) for i, feat in enumerate(feature_names)}
204 |             },
205 |             "r_squared": float(model.rsquared),
206 |             "adjusted_r_squared": float(model.rsquared_adj),
207 |             "f_statistic": float(model.fvalue),
208 |             "f_pvalue": float(model.f_pvalue),
209 |             "aic": float(model.aic),
210 |             "bic": float(model.bic)
211 |         }
212 |
213 |         return Result.ok(results)
214 |
215 |     async def _logistic_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
216 |         """Logistic regression"""
217 |         independent_vars = params.get("independent", [])
218 |         dependent_var = params.get("dependent")
219 |
220 |         # Prepare data
221 |         all_vars = independent_vars + [dependent_var]
222 |         df = data[all_vars].dropna()
223 |         X = df[independent_vars]
224 |         y = df[dependent_var]
225 |
226 |         # Convert y to binary if needed
227 |         if y.nunique() == 2:
228 |             y = pd.get_dummies(y, drop_first=True).iloc[:, 0]
229 |
230 |         # Add constant
231 |         X_with_const = sm.add_constant(X)
232 |
233 |         # Fit model
234 |         model = sm.Logit(y, X_with_const).fit()
235 |
236 |         # Calculate odds ratios
237 |         odds_ratios = np.exp(model.params)
238 |
239 |         # Get predictions
240 |         predictions = model.predict(X_with_const)
241 |         predicted_classes = (predictions > 0.5).astype(int)
242 |
243 |         # Calculate accuracy metrics
244 |         from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
245 |
246 |         accuracy = accuracy_score(y, predicted_classes)
247 |         precision = precision_score(y, predicted_classes)
248 |         recall = recall_score(y, predicted_classes)
249 |         f1 = f1_score(y, predicted_classes)
250 |         auc = roc_auc_score(y, predictions)
251 |
252 |         results = {
253 |             "model": "Logistic Regression",
254 |             "n_observations": len(df),
255 |             "coefficients": {
256 |                 "intercept": float(model.params[0]),
257 |                 **{var: float(model.params[i + 1]) for i, var in enumerate(independent_vars)}
258 |             },
259 |             "odds_ratios": {
260 |                 "intercept": float(odds_ratios[0]),
261 |                 **{var: float(odds_ratios[i + 1]) for i, var in enumerate(independent_vars)}
262 |             },
263 |             "std_errors": {
264 |                 "intercept": float(model.bse[0]),
265 |                 **{var: float(model.bse[i + 1]) for i, var in enumerate(independent_vars)}
266 |             },
267 |             "z_values": {
268 |                 "intercept": float(model.tvalues[0]),
269 |                 **{var: float(model.tvalues[i + 1]) for i, var in enumerate(independent_vars)}
270 |             },
271 |             "p_values": {
272 |                 "intercept": float(model.pvalues[0]),
273 |                 **{var: float(model.pvalues[i + 1]) for i, var in enumerate(independent_vars)}
274 |             },
275 |             "pseudo_r_squared": float(model.prsquared),
276 |             "log_likelihood": float(model.llf),
277 |             "aic": float(model.aic),
278 |             "bic": float(model.bic),
279 |             "classification_metrics": {
280 |                 "accuracy": float(accuracy),
281 |                 "precision": float(precision),
282 |                 "recall": float(recall),
283 |                 "f1_score": float(f1),
284 |                 "auc_roc": float(auc)
285 |             }
286 |         }
287 |
288 |         return Result.ok(results)
289 |
290 |     async def _stepwise_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
291 |         """Stepwise regression using forward/backward selection"""
292 |         independent_vars = params.get("independent", [])
293 |         dependent_var = params.get("dependent")
294 |         method = params.get("method", "forward")  # forward, backward, or both
295 |         threshold = params.get("threshold", 0.05)
296 |
297 |         # Prepare data
298 |         all_vars = independent_vars + [dependent_var]
299 |         df = data[all_vars].dropna()
300 |         X = df[independent_vars]
301 |         y = df[dependent_var]
302 |
303 |         if method == "forward":
304 |             selected_vars = await self._forward_selection(X, y, threshold)
305 |         elif method == "backward":
306 |             selected_vars = await self._backward_elimination(X, y, threshold)
307 |         else:  # both
308 |             selected_vars = await self._bidirectional_selection(X, y, threshold)
309 |
310 |         # Fit final model with selected variables
311 |         if selected_vars:
312 |             X_selected = X[selected_vars]
313 |             X_with_const = sm.add_constant(X_selected)
314 |             model = sm.OLS(y, X_with_const).fit()
315 |
316 |             results = {
317 |                 "model": f"Stepwise Regression ({method})",
318 |                 "selected_variables": selected_vars,
319 |                 "n_selected": len(selected_vars),
320 |                 "n_original": len(independent_vars),
321 |                 "threshold": threshold,
322 |                 "coefficients": {
323 |                     "intercept": float(model.params[0]),
324 |                     **{var: float(model.params[i + 1]) for i, var in enumerate(selected_vars)}
325 |                 },
326 |                 "p_values": {
327 |                     "intercept": float(model.pvalues[0]),
328 |                     **{var: float(model.pvalues[i + 1]) for i, var in enumerate(selected_vars)}
329 |                 },
330 |                 "r_squared": float(model.rsquared),
331 |                 "adjusted_r_squared": float(model.rsquared_adj),
332 |                 "aic": float(model.aic),
333 |                 "bic": float(model.bic)
334 |             }
335 |         else:
336 |             results = {
337 |                 "model": f"Stepwise Regression ({method})",
338 |                 "error": "No variables selected"
339 |             }
340 |
341 |         return Result.ok(results)
342 |
343 |     async def _forward_selection(self, X: pd.DataFrame, y: pd.Series, threshold: float) -> List[str]:
344 |         """Forward selection algorithm"""
345 |         selected = []
346 |         remaining = list(X.columns)
347 |
348 |         while remaining:
349 |             best_pvalue = 1.0
350 |             best_var = None
351 |
352 |             for var in remaining:
353 |                 vars_to_test = selected + [var]
354 |                 X_test = sm.add_constant(X[vars_to_test])
355 |                 model = sm.OLS(y, X_test).fit()
356 |
357 |                 # Get p-value for the new variable
358 |                 pvalue = model.pvalues[var]
359 |
360 |                 if pvalue < best_pvalue:
361 |                     best_pvalue = pvalue
362 |                     best_var = var
363 |
364 |             if best_pvalue < threshold:
365 |                 selected.append(best_var)
366 |                 remaining.remove(best_var)
367 |             else:
368 |                 break
369 |
370 |         return selected
371 |
372 |     async def _backward_elimination(self, X: pd.DataFrame, y: pd.Series, threshold: float) -> List[str]:
373 |         """Backward elimination algorithm"""
374 |         selected = list(X.columns)
375 |
376 |         while len(selected) > 0:
377 |             X_test = sm.add_constant(X[selected])
378 |             model = sm.OLS(y, X_test).fit()
379 |
380 |             # Get variable with highest p-value
381 |             pvalues = model.pvalues[1:]  # Exclude intercept
382 |             max_pvalue = pvalues.max()
383 |
384 |             if max_pvalue > threshold:
385 |                 worst_var = pvalues.idxmax()
386 |                 selected.remove(worst_var)
387 |             else:
388 |                 break
389 |
390 |         return selected
391 |
392 |     async def _bidirectional_selection(self, X: pd.DataFrame, y: pd.Series, threshold: float) -> List[str]:
393 |         """Bidirectional (stepwise) selection"""
394 |         # Start with forward selection
395 |         selected = await self._forward_selection(X, y, threshold)
396 |
397 |         # Then apply backward elimination on selected variables
398 |         if selected:
399 |             X_selected = X[selected]
400 |             selected = await self._backward_elimination(X_selected, y, threshold)
401 |
402 |         return selected
403 |
404 |     async def _quantile_regression(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
405 |         """Quantile regression"""
406 |         independent_vars = params.get("independent", [])
407 |         dependent_var = params.get("dependent")
408 |         quantiles = params.get("quantiles", [0.25, 0.5, 0.75])
409 |
410 |         # Prepare data
411 |         all_vars = independent_vars + [dependent_var]
412 |         df = data[all_vars].dropna()
413 |         X = df[independent_vars]
414 |         y = df[dependent_var]
415 |
416 |         # Add constant
417 |         X_with_const = sm.add_constant(X)
418 |
419 |         results = {
420 |             "model": "Quantile Regression",
421 |             "quantiles": {}
422 |         }
423 |
424 |         # Fit model for each quantile
425 |         for q in quantiles:
426 |             model = sm.QuantReg(y, X_with_const).fit(q=q)
427 |
428 |             results["quantiles"][str(q)] = {
429 |                 "coefficients": {
430 |                     "intercept": float(model.params[0]),
431 |                     **{var: float(model.params[i + 1]) for i, var in enumerate(independent_vars)}
432 |                 },
433 |                 "pseudo_r_squared": float(model.prsquared),
434 |                 "scale": float(model.scale)
435 |             }
436 |
437 |         return Result.ok(results)
438 |
439 |     def _regression_diagnostics(self, model, residuals, X) -> Dict[str, Any]:
440 |         """Perform regression diagnostic tests"""
441 |         diagnostics = {}
442 |
443 |         # Durbin-Watson test for autocorrelation
444 |         from statsmodels.stats.stattools import durbin_watson
445 |         dw = durbin_watson(residuals)
446 |         diagnostics["durbin_watson"] = float(dw)
447 |
448 |         # Breusch-Pagan test for heteroscedasticity
449 |         bp_test = het_breuschpagan(residuals, X)
450 |         diagnostics["breusch_pagan"] = {
451 |             "statistic": float(bp_test[0]),
452 |             "p_value": float(bp_test[1])
453 |         }
454 |
455 |         # Jarque-Bera test for normality of residuals
456 |         jb_test = stats.jarque_bera(residuals)
457 |         diagnostics["jarque_bera"] = {
458 |             "statistic": float(jb_test[0]),
459 |             "p_value": float(jb_test[1])
460 |         }
461 |
462 |         # Ljung-Box test for autocorrelation
463 |         lb_test = acorr_ljungbox(residuals, lags=10, return_df=True)
464 |         diagnostics["ljung_box"] = {
465 |             "statistics": lb_test['lb_stat'].tolist(),
466 |             "p_values": lb_test['lb_pvalue'].tolist()
467 |         }
468 |
469 |         return diagnostics
470 |


--------------------------------------------------------------------------------
/backend/app/engines/statistical/timeseries.py:
--------------------------------------------------------------------------------
  1 | from typing import Dict, Any, List, Optional
  2 | import pandas as pd
  3 | import numpy as np
  4 | from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf
  5 | from statsmodels.tsa.seasonal import seasonal_decompose
  6 | from statsmodels.tsa.arima.model import ARIMA
  7 | from statsmodels.tsa.statespace.sarimax import SARIMAX
  8 | from statsmodels.tsa.holtwinters import ExponentialSmoothing
  9 | import pmdarima as pm
 10 | from app.core.base import BaseAnalyzer, Result
 11 |
 12 |
 13 | class TimeSeriesAnalyzer(BaseAnalyzer):
 14 |     """Analyzer for time series analysis"""
 15 |
 16 |     def __init__(self):
 17 |         super().__init__("timeseries_analysis")
 18 |
 19 |     async def execute(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 20 |         """Execute time series analysis"""
 21 |         try:
 22 |             analysis_type = params.get("type")
 23 |
 24 |             if analysis_type == "decomposition":
 25 |                 return await self._decomposition(data, params)
 26 |             elif analysis_type == "stationarity":
 27 |                 return await self._stationarity_tests(data, params)
 28 |             elif analysis_type == "arima":
 29 |                 return await self._arima_analysis(data, params)
 30 |             elif analysis_type == "sarima":
 31 |                 return await self._sarima_analysis(data, params)
 32 |             elif analysis_type == "exponential_smoothing":
 33 |                 return await self._exponential_smoothing(data, params)
 34 |             elif analysis_type == "autocorrelation":
 35 |                 return await self._autocorrelation_analysis(data, params)
 36 |             elif analysis_type == "forecast":
 37 |                 return await self._forecast(data, params)
 38 |             else:
 39 |                 return Result.fail(f"Unknown analysis type: {analysis_type}")
 40 |
 41 |         except Exception as e:
 42 |             return Result.fail(str(e))
 43 |
 44 |     async def _decomposition(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 45 |         """Seasonal decomposition"""
 46 |         column = params.get("column")
 47 |         date_column = params.get("date_column")
 48 |         period = params.get("period", None)
 49 |         model = params.get("model", "additive")  # additive or multiplicative
 50 |
 51 |         # Prepare time series
 52 |         if date_column:
 53 |             data = data.set_index(date_column)
 54 |         ts = data[column].dropna()
 55 |
 56 |         # Perform decomposition
 57 |         decomposition = seasonal_decompose(ts, model=model, period=period)
 58 |
 59 |         results = {
 60 |             "method": "Seasonal Decomposition",
 61 |             "model": model,
 62 |             "period": period,
 63 |             "components": {
 64 |                 "trend": decomposition.trend.dropna().tolist()[:100],
 65 |                 "seasonal": decomposition.seasonal.dropna().tolist()[:100],
 66 |                 "residual": decomposition.resid.dropna().tolist()[:100]
 67 |             },
 68 |             "statistics": {
 69 |                 "trend_strength": float(1 - np.var(decomposition.resid.dropna()) / np.var(ts - decomposition.seasonal)),
 70 |                 "seasonal_strength": float(
 71 |                     1 - np.var(decomposition.resid.dropna()) / np.var(ts - decomposition.trend.dropna()))
 72 |             }
 73 |         }
 74 |
 75 |         return Result.ok(results)
 76 |
 77 |     async def _stationarity_tests(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
 78 |         """Test for stationarity"""
 79 |         column = params.get("column")
 80 |         ts = data[column].dropna()
 81 |
 82 |         # ADF test
 83 |         adf_result = adfuller(ts, autolag='AIC')
 84 |
 85 |         # KPSS test
 86 |         kpss_result = kpss(ts, regression='c', nlags='auto')
 87 |
 88 |         # Calculate rolling statistics
 89 |         window = min(12, len(ts) // 4)
 90 |         rolling_mean = ts.rolling(window=window).mean()
 91 |         rolling_std = ts.rolling(window=window).std()
 92 |
 93 |         results = {
 94 |             "method": "Stationarity Tests",
 95 |             "n_observations": len(ts),
 96 |             "adf_test": {
 97 |                 "statistic": float(adf_result[0]),
 98 |                 "p_value": float(adf_result[1]),
 99 |                 "critical_values": {
100 |                     "1%": float(adf_result[4]['1%']),
101 |                     "5%": float(adf_result[4]['5%']),
102 |                     "10%": float(adf_result[4]['10%'])
103 |                 },
104 |                 "conclusion": "Stationary" if adf_result[1] < 0.05 else "Non-stationary"
105 |             },
106 |             "kpss_test": {
107 |                 "statistic": float(kpss_result[0]),
108 |                 "p_value": float(kpss_result[1]),
109 |                 "critical_values": {
110 |                     "1%": float(kpss_result[3]['1%']),
111 |                     "2.5%": float(kpss_result[3]['2.5%']),
112 |                     "5%": float(kpss_result[3]['5%']),
113 |                     "10%": float(kpss_result[3]['10%'])
114 |                 },
115 |                 "conclusion": "Stationary" if kpss_result[1] > 0.05 else "Non-stationary"
116 |             },
117 |             "rolling_statistics": {
118 |                 "mean_variance": float(rolling_mean.var()),
119 |                 "std_variance": float(rolling_std.var())
120 |             }
121 |         }
122 |
123 |         return Result.ok(results)
124 |
125 |     async def _arima_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
126 |         """ARIMA model analysis"""
127 |         column = params.get("column")
128 |         order = params.get("order", None)  # (p, d, q)
129 |         auto = params.get("auto", True)
130 |
131 |         ts = data[column].dropna()
132 |
133 |         if auto:
134 |             # Auto ARIMA
135 |             model = pm.auto_arima(
136 |                 ts,
137 |                 start_p=0, start_q=0,
138 |                 max_p=5, max_q=5,
139 |                 seasonal=False,
140 |                 stepwise=True,
141 |                 suppress_warnings=True
142 |             )
143 |             order = model.order
144 |         else:
145 |             if order is None:
146 |                 order = (1, 1, 1)
147 |             model = ARIMA(ts, order=order).fit()
148 |
149 |         # Get model diagnostics
150 |         if hasattr(model, 'summary'):
151 |             aic = model.aic
152 |             bic = model.bic
153 |         else:
154 |             aic = model.aic()
155 |             bic = model.bic()
156 |
157 |         # Residual diagnostics
158 |         residuals = model.resid if hasattr(model, 'resid') else model.resid()
159 |
160 |         results = {
161 |             "method": "ARIMA Model",
162 |             "order": order,
163 |             "auto_selected": auto,
164 |             "aic": float(aic),
165 |             "bic": float(bic),
166 |             "coefficients": {
167 |                 f"ar{i + 1}": float(c) for i, c in enumerate(model.arparams if hasattr(model, 'arparams') else [])
168 |             },
169 |             "residual_diagnostics": {
170 |                 "mean": float(residuals.mean()),
171 |                 "std": float(residuals.std()),
172 |                 "skewness": float(residuals.skew()),
173 |                 "kurtosis": float(residuals.kurtosis())
174 |             }
175 |         }
176 |
177 |         return Result.ok(results)
178 |
179 |     async def _sarima_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
180 |         """SARIMA model analysis"""
181 |         column = params.get("column")
182 |         order = params.get("order", (1, 1, 1))
183 |         seasonal_order = params.get("seasonal_order", (1, 1, 1, 12))
184 |
185 |         ts = data[column].dropna()
186 |
187 |         # Fit SARIMA model
188 |         model = SARIMAX(ts, order=order, seasonal_order=seasonal_order).fit()
189 |
190 |         results = {
191 |             "method": "SARIMA Model",
192 |             "order": order,
193 |             "seasonal_order": seasonal_order,
194 |             "aic": float(model.aic),
195 |             "bic": float(model.bic),
196 |             "log_likelihood": float(model.llf),
197 |             "coefficients": model.params.to_dict()
198 |         }
199 |
200 |         return Result.ok(results)
201 |
202 |     async def _exponential_smoothing(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
203 |         """Exponential smoothing analysis"""
204 |         column = params.get("column")
205 |         trend = params.get("trend", "add")
206 |         seasonal = params.get("seasonal", "add")
207 |         seasonal_periods = params.get("seasonal_periods", 12)
208 |
209 |         ts = data[column].dropna()
210 |
211 |         # Fit model
212 |         model = ExponentialSmoothing(
213 |             ts,
214 |             trend=trend,
215 |             seasonal=seasonal,
216 |             seasonal_periods=seasonal_periods
217 |         ).fit()
218 |
219 |         results = {
220 |             "method": "Exponential Smoothing",
221 |             "trend": trend,
222 |             "seasonal": seasonal,
223 |             "seasonal_periods": seasonal_periods,
224 |             "aic": float(model.aic),
225 |             "bic": float(model.bic),
226 |             "smoothing_parameters": {
227 |                 "alpha": float(model.params['smoothing_level']),
228 |                 "beta": float(model.params.get('smoothing_trend', 0)),
229 |                 "gamma": float(model.params.get('smoothing_seasonal', 0))
230 |             },
231 |             "fitted_values": model.fittedvalues.tolist()[:100]
232 |         }
233 |
234 |         return Result.ok(results)
235 |
236 |     async def _autocorrelation_analysis(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
237 |         """Autocorrelation and partial autocorrelation analysis"""
238 |         column = params.get("column")
239 |         nlags = params.get("nlags", 40)
240 |
241 |         ts = data[column].dropna()
242 |
243 |         # Calculate ACF and PACF
244 |         acf_values = acf(ts, nlags=nlags)
245 |         pacf_values = pacf(ts, nlags=nlags)
246 |
247 |         # Find significant lags
248 |         n = len(ts)
249 |         confidence_interval = 1.96 / np.sqrt(n)
250 |
251 |         significant_acf_lags = [i for i, v in enumerate(acf_values[1:], 1)
252 |                                 if abs(v) > confidence_interval]
253 |         significant_pacf_lags = [i for i, v in enumerate(pacf_values[1:], 1)
254 |                                  if abs(v) > confidence_interval]
255 |
256 |         results = {
257 |             "method": "Autocorrelation Analysis",
258 |             "n_observations": n,
259 |             "nlags": nlags,
260 |             "acf": acf_values.tolist(),
261 |             "pacf": pacf_values.tolist(),
262 |             "confidence_interval": float(confidence_interval),
263 |             "significant_acf_lags": significant_acf_lags,
264 |             "significant_pacf_lags": significant_pacf_lags,
265 |             "suggested_ar_order": len(significant_pacf_lags),
266 |             "suggested_ma_order": len(significant_acf_lags)
267 |         }
268 |
269 |         return Result.ok(results)
270 |
271 |     async def _forecast(self, data: pd.DataFrame, params: Dict[str, Any]) -> Result:
272 |         """Generate forecasts"""
273 |         column = params.get("column")
274 |         model_type = params.get("model_type", "arima")
275 |         horizon = params.get("horizon", 12)
276 |
277 |         ts = data[column].dropna()
278 |
279 |         if model_type == "arima":
280 |             # Auto ARIMA for forecasting
281 |             model = pm.auto_arima(ts, suppress_warnings=True)
282 |             forecast, conf_int = model.predict(n_periods=horizon, return_conf_int=True)
283 |
284 |             results = {
285 |                 "method": "ARIMA Forecast",
286 |                 "model_order": model.order,
287 |                 "horizon": horizon,
288 |                 "forecast": forecast.tolist(),
289 |                 "lower_bound": conf_int[:, 0].tolist(),
290 |                 "upper_bound": conf_int[:, 1].tolist(),
291 |                 "in_sample_metrics": {
292 |                     "mape": float(np.mean(np.abs((ts - model.predict_in_sample()) / ts)) * 100),
293 |                     "rmse": float(np.sqrt(np.mean((ts - model.predict_in_sample()) ** 2)))
294 |                 }
295 |             }
296 |         else:
297 |             # Exponential smoothing forecast
298 |             model = ExponentialSmoothing(ts, seasonal_periods=12).fit()
299 |             forecast = model.forecast(horizon)
300 |
301 |             results = {
302 |                 "method": "Exponential Smoothing Forecast",
303 |                 "horizon": horizon,
304 |                 "forecast": forecast.tolist(),
305 |                 "in_sample_metrics": {
306 |                     "mape": float(np.mean(np.abs((ts - model.fittedvalues) / ts)) * 100),
307 |                     "rmse": float(np.sqrt(np.mean((ts - model.fittedvalues) ** 2)))
308 |                 }
309 |             }
310 |
311 |         return Result.ok(results)
312 |


--------------------------------------------------------------------------------
/backend/app/main.py:
--------------------------------------------------------------------------------
  1 | from fastapi import FastAPI, Request
  2 | from fastapi.middleware.cors import CORSMiddleware
  3 | from fastapi.responses import JSONResponse
  4 | from contextlib import asynccontextmanager
  5 | import time
  6 | import logging
  7 |
  8 | from app.config import get_settings
  9 | from app.core.registry import EngineRegistry
 10 | from app.core.session_manager import SessionManager
 11 | from app.core.plugin_manager import PluginManager
 12 | from app.api.v1 import data, statistics, ml, models
 13 |
 14 | # Configure logging
 15 | logging.basicConfig(level=logging.INFO)
 16 | logger = logging.getLogger(__name__)
 17 |
 18 | settings = get_settings()
 19 |
 20 |
 21 | @asynccontextmanager
 22 | async def lifespan(app: FastAPI):
 23 |     """Manage application lifecycle"""
 24 |     # Startup
 25 |     logger.info(f"Starting {settings.APP_NAME} v{settings.APP_VERSION}")
 26 |
 27 |     # Initialize core components
 28 |     app.state.registry = EngineRegistry()
 29 |     app.state.session_manager = SessionManager()
 30 |
 31 |     # Load plugins if enabled
 32 |     if settings.ENABLE_PLUGINS:
 33 |         app.state.plugin_manager = PluginManager(settings.PLUGIN_PATH)
 34 |         await app.state.plugin_manager.load_plugins()
 35 |
 36 |     # Initialize ML engines if enabled
 37 |     if settings.ENABLE_ML:
 38 |         from app.engines.ml import initialize_ml_engines
 39 |         await initialize_ml_engines(app.state.registry)
 40 |
 41 |     yield
 42 |
 43 |     # Shutdown
 44 |     logger.info("Shutting down application")
 45 |     await app.state.session_manager.cleanup()
 46 |
 47 |
 48 | # Create FastAPI app
 49 | app = FastAPI(
 50 |     title=settings.APP_NAME,
 51 |     description="Open-source web-based statistical & ML platform scalable from lightweight analysis to advanced models with endless extensibility",
 52 |     version=settings.APP_VERSION,
 53 |     lifespan=lifespan
 54 | )
 55 |
 56 | # CORS middleware
 57 | app.add_middleware(
 58 |     CORSMiddleware,
 59 |     allow_origins=["*"],
 60 |     allow_credentials=True,
 61 |     allow_methods=["*"],
 62 |     allow_headers=["*"],
 63 | )
 64 |
 65 |
 66 | # Request timing middleware
 67 | @app.middleware("http")
 68 | async def add_process_time_header(request: Request, call_next):
 69 |     start_time = time.time()
 70 |     response = await call_next(request)
 71 |     process_time = time.time() - start_time
 72 |     response.headers["X-Process-Time"] = str(process_time)
 73 |     return response
 74 |
 75 |
 76 | # Include API routers
 77 | app.include_router(data.router, prefix=f"{settings.API_V1_PREFIX}/data", tags=["Data"])
 78 | app.include_router(statistics.router, prefix=f"{settings.API_V1_PREFIX}/statistics", tags=["Statistics"])
 79 | app.include_router(ml.router, prefix=f"{settings.API_V1_PREFIX}/ml", tags=["Machine Learning"])
 80 | app.include_router(models.router, prefix=f"{settings.API_V1_PREFIX}/models", tags=["Models"])
 81 |
 82 |
 83 | # Root endpoint
 84 | @app.get("/")
 85 | async def root():
 86 |     return {
 87 |         "name": settings.APP_NAME,
 88 |         "version": settings.APP_VERSION,
 89 |         "status": "operational",
 90 |         "features": {
 91 |             "statistics": True,
 92 |             "ml": settings.ENABLE_ML,
 93 |             "plugins": settings.ENABLE_PLUGINS,
 94 |             "gpu": settings.ENABLE_GPU
 95 |         }
 96 |     }
 97 |
 98 |
 99 | # Health check
100 | @app.get("/health")
101 | async def health_check():
102 |     return {
103 |         "status": "healthy",
104 |         "timestamp": time.time()
105 |     }
106 |
107 |
108 | # Global exception handler
109 | @app.exception_handler(Exception)
110 | async def global_exception_handler(request: Request, exc: Exception):
111 |     logger.error(f"Unhandled exception: {exc}", exc_info=True)
112 |     return JSONResponse(
113 |         status_code=500,
114 |         content={
115 |             "error": "Internal server error",
116 |             "message": str(exc) if settings.DEBUG else "An error occurred"
117 |         }
118 |     )
119 |


--------------------------------------------------------------------------------
/backend/app/models/__init__.py:
--------------------------------------------------------------------------------
 1 | """
 2 | OpenStatica Models Package
 3 | Export all Pydantic models for the application
 4 | """
 5 |
 6 | from .data_models import (
 7 |     DataUploadResponse,
 8 |     DataTransformRequest,
 9 |     DataExportRequest,
10 |     ColumnInfo,
11 |     DataPreview,
12 |     DataStatistics
13 | )
14 |
15 | from .analysis_models import (
16 |     AnalysisRequest,
17 |     AnalysisResponse,
18 |     StatisticalTestRequest,
19 |     StatisticalTestResult,
20 |     CorrelationRequest,
21 |     CorrelationResult,
22 |     RegressionRequest,
23 |     RegressionResult
24 | )
25 |
26 | from .ml_models import (
27 |     MLRequest,
28 |     MLResult,
29 |     ModelTrainingRequest,
30 |     ModelPredictionRequest,
31 |     ModelEvaluationResult,
32 |     HyperparametersConfig,
33 |     ModelMetrics,
34 |     FeatureImportance
35 | )
36 |
37 | __all__ = [
38 |     # Data models
39 |     'DataUploadResponse',
40 |     'DataTransformRequest',
41 |     'DataExportRequest',
42 |     'ColumnInfo',
43 |     'DataPreview',
44 |     'DataStatistics',
45 |
46 |     # Analysis models
47 |     'AnalysisRequest',
48 |     'AnalysisResponse',
49 |     'StatisticalTestRequest',
50 |     'StatisticalTestResult',
51 |     'CorrelationRequest',
52 |     'CorrelationResult',
53 |     'RegressionRequest',
54 |     'RegressionResult',
55 |
56 |     # ML models
57 |     'MLRequest',
58 |     'MLResult',
59 |     'ModelTrainingRequest',
60 |     'ModelPredictionRequest',
61 |     'ModelEvaluationResult',
62 |     'HyperparametersConfig',
63 |     'ModelMetrics',
64 |     'FeatureImportance'
65 | ]
66 |


--------------------------------------------------------------------------------
/backend/app/models/analysis_models.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Analysis Models for OpenStatica
  3 | Pydantic models for statistical analysis operations
  4 | """
  5 |
  6 | from pydantic import BaseModel, Field, validator
  7 | from typing import Dict, List, Optional, Any, Union
  8 | from enum import Enum
  9 | from datetime import datetime
 10 |
 11 |
 12 | class TestType(str, Enum):
 13 |     """Types of statistical tests"""
 14 |     # Parametric tests
 15 |     TTEST_ONE = "ttest_one"
 16 |     TTEST_INDEPENDENT = "ttest_independent"
 17 |     TTEST_PAIRED = "ttest_paired"
 18 |     ANOVA_ONE = "anova_one"
 19 |     ANOVA_TWO = "anova_two"
 20 |     ANOVA_REPEATED = "anova_repeated"
 21 |     ANCOVA = "ancova"
 22 |     MANOVA = "manova"
 23 |
 24 |     # Non-parametric tests
 25 |     MANN_WHITNEY = "mann_whitney"
 26 |     WILCOXON = "wilcoxon"
 27 |     KRUSKAL_WALLIS = "kruskal_wallis"
 28 |     FRIEDMAN = "friedman"
 29 |
 30 |     # Correlation tests
 31 |     PEARSON = "pearson"
 32 |     SPEARMAN = "spearman"
 33 |     KENDALL = "kendall"
 34 |     PARTIAL = "partial_correlation"
 35 |
 36 |     # Association tests
 37 |     CHI_SQUARE = "chi_square"
 38 |     FISHER_EXACT = "fisher_exact"
 39 |     MCNEMAR = "mcnemar"
 40 |     COCHRAN_Q = "cochran_q"
 41 |
 42 |     # Normality tests
 43 |     SHAPIRO_WILK = "shapiro_wilk"
 44 |     KOLMOGOROV_SMIRNOV = "kolmogorov_smirnov"
 45 |     ANDERSON_DARLING = "anderson_darling"
 46 |     JARQUE_BERA = "jarque_bera"
 47 |
 48 |
 49 | class RegressionType(str, Enum):
 50 |     """Types of regression models"""
 51 |     LINEAR = "linear"
 52 |     MULTIPLE = "multiple"
 53 |     POLYNOMIAL = "polynomial"
 54 |     LOGISTIC = "logistic"
 55 |     ORDINAL = "ordinal"
 56 |     MULTINOMIAL = "multinomial"
 57 |     POISSON = "poisson"
 58 |     NEGATIVE_BINOMIAL = "negative_binomial"
 59 |     RIDGE = "ridge"
 60 |     LASSO = "lasso"
 61 |     ELASTIC_NET = "elastic_net"
 62 |     QUANTILE = "quantile"
 63 |     ROBUST = "robust"
 64 |     COX = "cox"
 65 |     MIXED = "mixed"
 66 |
 67 |
 68 | class PostHocTest(str, Enum):
 69 |     """Post-hoc test types"""
 70 |     TUKEY = "tukey"
 71 |     BONFERRONI = "bonferroni"
 72 |     SCHEFFE = "scheffe"
 73 |     SIDAK = "sidak"
 74 |     HOLM = "holm"
 75 |     DUNNETT = "dunnett"
 76 |     GAMES_HOWELL = "games_howell"
 77 |
 78 |
 79 | class AnalysisRequest(BaseModel):
 80 |     """Base request for analysis"""
 81 |     session_id: str
 82 |     columns: List[str]
 83 |     options: Dict[str, Any] = {}
 84 |     confidence_level: float = 0.95
 85 |     missing_values: str = "exclude"  # exclude, include, impute
 86 |
 87 |     @validator('confidence_level')
 88 |     def validate_confidence(cls, v):
 89 |         if not 0 < v < 1:
 90 |             raise ValueError('Confidence level must be between 0 and 1')
 91 |         return v
 92 |
 93 |
 94 | class AnalysisResponse(BaseModel):
 95 |     """Base response for analysis"""
 96 |     test_name: str
 97 |     results: Dict[str, Any]
 98 |     interpretation: Optional[str] = None
 99 |     assumptions_met: Optional[Dict[str, bool]] = None
100 |     warnings: Optional[List[str]] = None
101 |     execution_time: float
102 |     timestamp: datetime = Field(default_factory=datetime.now)
103 |
104 |
105 | class DescriptiveStatisticsRequest(AnalysisRequest):
106 |     """Request for descriptive statistics"""
107 |     include_advanced: bool = True
108 |     percentiles: List[float] = [25, 50, 75]
109 |     include_outliers: bool = True
110 |     outlier_method: str = "iqr"  # iqr, zscore, mad
111 |
112 |
113 | class DescriptiveStatisticsResult(BaseModel):
114 |     """Result of descriptive statistics"""
115 |     column: str
116 |     count: int
117 |     missing: int
118 |     mean: Optional[float]
119 |     median: Optional[float]
120 |     mode: Optional[Union[float, str]]
121 |     std: Optional[float]
122 |     variance: Optional[float]
123 |     min: Optional[float]
124 |     max: Optional[float]
125 |     range: Optional[float]
126 |     q1: Optional[float]
127 |     q3: Optional[float]
128 |     iqr: Optional[float]
129 |     skewness: Optional[float]
130 |     kurtosis: Optional[float]
131 |     sem: Optional[float]  # Standard error of mean
132 |     cv: Optional[float]  # Coefficient of variation
133 |     percentiles: Dict[str, float]
134 |     confidence_interval: Optional[List[float]]
135 |     outliers: Optional[Dict[str, Any]]
136 |
137 |
138 | class StatisticalTestRequest(AnalysisRequest):
139 |     """Request for statistical test"""
140 |     test_type: TestType
141 |     alpha: float = 0.05
142 |     alternative: str = "two-sided"  # two-sided, less, greater
143 |     paired: bool = False
144 |     equal_variance: bool = True
145 |
146 |     # Test-specific parameters
147 |     group_column: Optional[str] = None
148 |     value_column: Optional[str] = None
149 |     test_value: Optional[float] = None
150 |
151 |     # Post-hoc analysis
152 |     post_hoc: Optional[PostHocTest] = None
153 |     correction: Optional[str] = None  # bonferroni, holm, etc.
154 |
155 |     @validator('alpha')
156 |     def validate_alpha(cls, v):
157 |         if not 0 < v < 1:
158 |             raise ValueError('Alpha must be between 0 and 1')
159 |         return v
160 |
161 |
162 | class StatisticalTestResult(AnalysisResponse):
163 |     """Result of statistical test"""
164 |     statistic: float
165 |     p_value: float
166 |     degrees_of_freedom: Optional[Union[float, List[float]]] = None
167 |     effect_size: Optional[Dict[str, float]] = None
168 |     confidence_interval: Optional[List[float]] = None
169 |     power: Optional[float] = None
170 |     sample_sizes: Optional[Dict[str, int]] = None
171 |     group_statistics: Optional[Dict[str, Dict[str, float]]] = None
172 |     post_hoc_results: Optional[Dict[str, Any]] = None
173 |     assumptions: Optional[Dict[str, Dict[str, Any]]] = None
174 |     decision: str  # reject or fail to reject
175 |
176 |
177 | class CorrelationRequest(AnalysisRequest):
178 |     """Request for correlation analysis"""
179 |     method: str = "pearson"  # pearson, spearman, kendall
180 |     pairwise: bool = True
181 |     min_periods: Optional[int] = None
182 |
183 |     # Partial correlation
184 |     control_variables: Optional[List[str]] = None
185 |
186 |     # Options
187 |     compute_p_values: bool = True
188 |     compute_confidence_intervals: bool = True
189 |     handle_multicollinearity: bool = False
190 |
191 |
192 | class CorrelationResult(AnalysisResponse):
193 |     """Result of correlation analysis"""
194 |     correlation_matrix: Dict[str, Dict[str, float]]
195 |     p_value_matrix: Optional[Dict[str, Dict[str, float]]] = None
196 |     confidence_intervals: Optional[Dict[str, Dict[str, List[float]]]] = None
197 |     n_observations: Dict[str, Dict[str, int]]
198 |     significant_correlations: List[Dict[str, Any]]
199 |     vif_scores: Optional[Dict[str, float]] = None  # Variance Inflation Factor
200 |
201 |
202 | class RegressionRequest(AnalysisRequest):
203 |     """Request for regression analysis"""
204 |     regression_type: RegressionType
205 |     dependent_variable: str
206 |     independent_variables: List[str]
207 |
208 |     # Model options
209 |     include_intercept: bool = True
210 |     standardize: bool = False
211 |
212 |     # Feature selection
213 |     feature_selection: Optional[str] = None  # forward, backward, stepwise, lasso
214 |     selection_threshold: Optional[float] = 0.05
215 |
216 |     # Regularization
217 |     alpha: Optional[float] = None  # For regularized regression
218 |     l1_ratio: Optional[float] = None  # For elastic net
219 |
220 |     # Validation
221 |     cross_validate: bool = False
222 |     cv_folds: int = 5
223 |
224 |     # Diagnostics
225 |     compute_diagnostics: bool = True
226 |     compute_influence: bool = True
227 |
228 |
229 | class RegressionResult(AnalysisResponse):
230 |     """Result of regression analysis"""
231 |     model_type: str
232 |     formula: str
233 |
234 |     # Coefficients
235 |     coefficients: Dict[str, float]
236 |     standard_errors: Dict[str, float]
237 |     t_values: Dict[str, float]
238 |     p_values: Dict[str, float]
239 |     confidence_intervals: Dict[str, List[float]]
240 |
241 |     # Model fit
242 |     r_squared: float
243 |     adjusted_r_squared: float
244 |     f_statistic: Optional[float]
245 |     f_p_value: Optional[float]
246 |     aic: float
247 |     bic: float
248 |     log_likelihood: Optional[float]
249 |
250 |     # Predictions
251 |     residuals: Optional[List[float]] = None
252 |     fitted_values: Optional[List[float]] = None
253 |
254 |     # Diagnostics
255 |     durbin_watson: Optional[float] = None
256 |     condition_number: Optional[float] = None
257 |     vif: Optional[Dict[str, float]] = None
258 |
259 |     # Assumption tests
260 |     normality_test: Optional[Dict[str, float]] = None
261 |     heteroscedasticity_test: Optional[Dict[str, float]] = None
262 |     autocorrelation_test: Optional[Dict[str, float]] = None
263 |
264 |     # Influence measures
265 |     cooks_distance: Optional[List[float]] = None
266 |     leverage: Optional[List[float]] = None
267 |     studentized_residuals: Optional[List[float]] = None
268 |
269 |     # Cross-validation results
270 |     cv_scores: Optional[List[float]] = None
271 |     cv_mean: Optional[float] = None
272 |     cv_std: Optional[float] = None
273 |
274 |
275 | class MultivariateRequest(AnalysisRequest):
276 |     """Request for multivariate analysis"""
277 |     analysis_type: str  # pca, factor, lda, cca, manova
278 |     n_components: Optional[int] = None
279 |     rotation: Optional[str] = None  # varimax, promax, oblimin
280 |     method: Optional[str] = None
281 |
282 |     # PCA/Factor specific
283 |     standardize: bool = True
284 |
285 |     # LDA specific
286 |     target_variable: Optional[str] = None
287 |
288 |     # CCA specific
289 |     x_variables: Optional[List[str]] = None
290 |     y_variables: Optional[List[str]] = None
291 |
292 |
293 | class MultivariateResult(AnalysisResponse):
294 |     """Result of multivariate analysis"""
295 |     method: str
296 |     n_components: int
297 |
298 |     # PCA/Factor Analysis
299 |     eigenvalues: Optional[List[float]] = None
300 |     explained_variance: Optional[List[float]] = None
301 |     explained_variance_ratio: Optional[List[float]] = None
302 |     cumulative_variance_ratio: Optional[List[float]] = None
303 |     loadings: Optional[Dict[str, List[float]]] = None
304 |     scores: Optional[List[List[float]]] = None
305 |     communalities: Optional[Dict[str, float]] = None
306 |     uniqueness: Optional[Dict[str, float]] = None
307 |
308 |     # Model adequacy
309 |     kmo: Optional[float] = None  # Kaiser-Meyer-Olkin
310 |     bartlett_sphericity: Optional[Dict[str, float]] = None
311 |
312 |     # Rotation
313 |     rotation_matrix: Optional[List[List[float]]] = None
314 |
315 |     # LDA
316 |     class_means: Optional[Dict[str, List[float]]] = None
317 |     coefficients: Optional[List[List[float]]] = None
318 |     classification_accuracy: Optional[float] = None
319 |
320 |     # CCA
321 |     canonical_correlations: Optional[List[float]] = None
322 |     canonical_loadings: Optional[Dict[str, List[List[float]]]] = None
323 |     wilks_lambda: Optional[float] = None
324 |
325 |
326 | class TimeSeriesRequest(AnalysisRequest):
327 |     """Request for time series analysis"""
328 |     date_column: str
329 |     value_column: str
330 |     frequency: Optional[str] = None  # D, W, M, Q, Y
331 |
332 |     # Analysis type
333 |     analysis_type: str  # decomposition, arima, forecast, etc.
334 |
335 |     # Decomposition
336 |     decomposition_type: Optional[str] = "additive"  # additive, multiplicative
337 |
338 |     # ARIMA
339 |     order: Optional[List[int]] = None  # (p, d, q)
340 |     seasonal_order: Optional[List[int]] = None  # (P, D, Q, s)
341 |     auto_arima: bool = False
342 |
343 |     # Forecasting
344 |     forecast_periods: Optional[int] = None
345 |     confidence_level: float = 0.95
346 |
347 |
348 | class TimeSeriesResult(AnalysisResponse):
349 |     """Result of time series analysis"""
350 |     analysis_type: str
351 |
352 |     # Decomposition
353 |     trend: Optional[List[float]] = None
354 |     seasonal: Optional[List[float]] = None
355 |     residual: Optional[List[float]] = None
356 |
357 |     # Stationarity tests
358 |     adf_statistic: Optional[float] = None
359 |     adf_p_value: Optional[float] = None
360 |     kpss_statistic: Optional[float] = None
361 |     kpss_p_value: Optional[float] = None
362 |
363 |     # ARIMA
364 |     model_order: Optional[List[int]] = None
365 |     aic: Optional[float] = None
366 |     bic: Optional[float] = None
367 |     coefficients: Optional[Dict[str, float]] = None
368 |
369 |     # Forecast
370 |     forecast_values: Optional[List[float]] = None
371 |     forecast_lower: Optional[List[float]] = None
372 |     forecast_upper: Optional[List[float]] = None
373 |     forecast_dates: Optional[List[str]] = None
374 |
375 |     # Accuracy metrics
376 |     mae: Optional[float] = None
377 |     mse: Optional[float] = None
378 |     rmse: Optional[float] = None
379 |     mape: Optional[float] = None
380 |
381 |
382 | class SurvivalAnalysisRequest(AnalysisRequest):
383 |     """Request for survival analysis"""
384 |     time_column: str
385 |     event_column: str
386 |     covariates: Optional[List[str]] = None
387 |
388 |     analysis_type: str = "kaplan_meier"  # kaplan_meier, cox, aft
389 |     groups: Optional[str] = None
390 |
391 |     # Cox specific
392 |     ties_method: str = "efron"  # efron, breslow
393 |
394 |     # Time points for survival probability
395 |     time_points: Optional[List[float]] = None
396 |


--------------------------------------------------------------------------------
/backend/app/models/data_models.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Data Models for OpenStatica
  3 | Pydantic models for data operations
  4 | """
  5 |
  6 | from pydantic import BaseModel, Field, validator
  7 | from typing import Dict, List, Optional, Any, Union
  8 | from datetime import datetime
  9 | from enum import Enum
 10 |
 11 |
 12 | class FileFormat(str, Enum):
 13 |     """Supported file formats"""
 14 |     CSV = "csv"
 15 |     EXCEL = "excel"
 16 |     JSON = "json"
 17 |     PARQUET = "parquet"
 18 |     STATA = "stata"
 19 |     SPSS = "spss"
 20 |     SAS = "sas"
 21 |
 22 |
 23 | class DataType(str, Enum):
 24 |     """Data types for columns"""
 25 |     NUMERIC = "numeric"
 26 |     CATEGORICAL = "categorical"
 27 |     DATETIME = "datetime"
 28 |     TEXT = "text"
 29 |     BOOLEAN = "boolean"
 30 |
 31 |
 32 | class TransformationType(str, Enum):
 33 |     """Types of data transformations"""
 34 |     NORMALIZE = "normalize"
 35 |     STANDARDIZE = "standardize"
 36 |     ENCODE = "encode"
 37 |     IMPUTE = "impute"
 38 |     BINNING = "binning"
 39 |     LOG_TRANSFORM = "log"
 40 |     SQRT_TRANSFORM = "sqrt"
 41 |     POLYNOMIAL = "polynomial"
 42 |     DIFFERENCE = "difference"
 43 |
 44 |
 45 | class ImputationStrategy(str, Enum):
 46 |     """Imputation strategies for missing values"""
 47 |     MEAN = "mean"
 48 |     MEDIAN = "median"
 49 |     MODE = "mode"
 50 |     FORWARD_FILL = "forward"
 51 |     BACKWARD_FILL = "backward"
 52 |     INTERPOLATE = "interpolate"
 53 |     DROP = "drop"
 54 |     CONSTANT = "constant"
 55 |
 56 |
 57 | class ColumnInfo(BaseModel):
 58 |     """Information about a data column"""
 59 |     name: str
 60 |     dtype: DataType
 61 |     missing_count: int = 0
 62 |     missing_percentage: float = 0.0
 63 |     unique_count: int = 0
 64 |     mean: Optional[float] = None
 65 |     median: Optional[float] = None
 66 |     std: Optional[float] = None
 67 |     min: Optional[float] = None
 68 |     max: Optional[float] = None
 69 |     q1: Optional[float] = None
 70 |     q3: Optional[float] = None
 71 |     mode: Optional[Union[str, float]] = None
 72 |     categories: Optional[List[str]] = None
 73 |
 74 |
 75 | class DataStatistics(BaseModel):
 76 |     """Overall data statistics"""
 77 |     row_count: int
 78 |     column_count: int
 79 |     numeric_columns: List[str]
 80 |     categorical_columns: List[str]
 81 |     datetime_columns: List[str]
 82 |     text_columns: List[str]
 83 |     missing_cells: int
 84 |     missing_percentage: float
 85 |     memory_usage: float  # in MB
 86 |     duplicated_rows: int
 87 |
 88 |
 89 | class DataPreview(BaseModel):
 90 |     """Data preview for display"""
 91 |     columns: List[str]
 92 |     data: List[Dict[str, Any]]
 93 |     total_rows: int
 94 |     preview_rows: int = 10
 95 |
 96 |
 97 | class DataUploadRequest(BaseModel):
 98 |     """Request for data upload"""
 99 |     filename: str
100 |     format: FileFormat
101 |     encoding: str = "utf-8"
102 |     delimiter: Optional[str] = ","
103 |     has_header: bool = True
104 |     parse_dates: bool = True
105 |     na_values: Optional[List[str]] = None
106 |
107 |
108 | class DataUploadResponse(BaseModel):
109 |     """Response after data upload"""
110 |     session_id: str
111 |     filename: str
112 |     format: FileFormat
113 |     rows: int
114 |     columns: int
115 |     column_names: List[str]
116 |     numeric_columns: List[str]
117 |     categorical_columns: List[str]
118 |     datetime_columns: Optional[List[str]] = []
119 |     preview: List[Dict[str, Any]]
120 |     statistics: Optional[DataStatistics] = None
121 |     upload_time: datetime = Field(default_factory=datetime.now)
122 |     warnings: Optional[List[str]] = None
123 |
124 |
125 | class DataTransformRequest(BaseModel):
126 |     """Request for data transformation"""
127 |     session_id: str
128 |     transformation_type: TransformationType
129 |     columns: List[str]
130 |     options: Dict[str, Any] = {}
131 |
132 |     # Transformation-specific options
133 |     imputation_strategy: Optional[ImputationStrategy] = None
134 |     constant_value: Optional[Union[str, float]] = None
135 |     encoding_type: Optional[str] = None  # onehot, label, ordinal
136 |     bins: Optional[int] = None
137 |     polynomial_degree: Optional[int] = None
138 |     lag: Optional[int] = None
139 |
140 |
141 | class DataFilterRequest(BaseModel):
142 |     """Request for data filtering"""
143 |     session_id: str
144 |     filters: List[Dict[str, Any]]
145 |     logic: str = "and"  # and/or logic for multiple filters
146 |
147 |     @validator('logic')
148 |     def validate_logic(cls, v):
149 |         if v not in ['and', 'or']:
150 |             raise ValueError('Logic must be "and" or "or"')
151 |         return v
152 |
153 |
154 | class DataAggregateRequest(BaseModel):
155 |     """Request for data aggregation"""
156 |     session_id: str
157 |     group_by: List[str]
158 |     aggregations: Dict[str, List[str]]  # column: [operations]
159 |
160 |     class Config:
161 |         json_schema_extra = {
162 |             "example": {
163 |                 "session_id": "abc123",
164 |                 "group_by": ["category", "region"],
165 |                 "aggregations": {
166 |                     "sales": ["sum", "mean", "count"],
167 |                     "profit": ["sum", "max"]
168 |                 }
169 |             }
170 |         }
171 |
172 |
173 | class DataMergeRequest(BaseModel):
174 |     """Request for merging datasets"""
175 |     left_session_id: str
176 |     right_session_id: str
177 |     how: str = "inner"  # inner, outer, left, right
178 |     left_on: List[str]
179 |     right_on: List[str]
180 |     suffixes: List[str] = ["_left", "_right"]
181 |
182 |     @validator('how')
183 |     def validate_merge_type(cls, v):
184 |         valid_types = ['inner', 'outer', 'left', 'right']
185 |         if v not in valid_types:
186 |             raise ValueError(f'Merge type must be one of {valid_types}')
187 |         return v
188 |
189 |
190 | class DataExportRequest(BaseModel):
191 |     """Request for data export"""
192 |     session_id: str
193 |     format: FileFormat
194 |     columns: Optional[List[str]] = None
195 |     include_index: bool = False
196 |     encoding: str = "utf-8"
197 |     compression: Optional[str] = None  # gzip, zip, etc.
198 |
199 |
200 | class DataValidationRule(BaseModel):
201 |     """Data validation rule"""
202 |     column: str
203 |     rule_type: str  # range, pattern, unique, not_null, etc.
204 |     parameters: Dict[str, Any]
205 |     error_message: Optional[str] = None
206 |
207 |
208 | class DataQualityReport(BaseModel):
209 |     """Data quality assessment report"""
210 |     session_id: str
211 |     overall_score: float  # 0-100
212 |     completeness_score: float
213 |     consistency_score: float
214 |     validity_score: float
215 |     uniqueness_score: float
216 |     issues: List[Dict[str, Any]]
217 |     recommendations: List[str]
218 |     column_quality: Dict[str, Dict[str, float]]
219 |
220 |
221 | class DataSplitRequest(BaseModel):
222 |     """Request for splitting data"""
223 |     session_id: str
224 |     split_type: str = "random"  # random, stratified, time-based
225 |     test_size: float = 0.2
226 |     validation_size: Optional[float] = None
227 |     stratify_column: Optional[str] = None
228 |     random_state: Optional[int] = 42
229 |     shuffle: bool = True
230 |
231 |
232 | class DataSplitResponse(BaseModel):
233 |     """Response after data split"""
234 |     train_session_id: str
235 |     test_session_id: str
236 |     validation_session_id: Optional[str] = None
237 |     train_size: int
238 |     test_size: int
239 |     validation_size: Optional[int] = None
240 |     split_proportions: Dict[str, float]
241 |
242 |
243 | class VariableMetadata(BaseModel):
244 |     """Metadata for a variable"""
245 |     name: str
246 |     label: Optional[str] = None
247 |     description: Optional[str] = None
248 |     unit: Optional[str] = None
249 |     scale: Optional[str] = None  # nominal, ordinal, interval, ratio
250 |     role: Optional[str] = None  # input, target, id, weight
251 |     value_labels: Optional[Dict[Union[int, str], str]] = None
252 |     missing_values: Optional[List[Any]] = None
253 |
254 |
255 | class DatasetMetadata(BaseModel):
256 |     """Metadata for entire dataset"""
257 |     name: str
258 |     description: Optional[str] = None
259 |     source: Optional[str] = None
260 |     created_date: datetime = Field(default_factory=datetime.now)
261 |     modified_date: datetime = Field(default_factory=datetime.now)
262 |     author: Optional[str] = None
263 |     license: Optional[str] = None
264 |     tags: Optional[List[str]] = []
265 |     variables: List[VariableMetadata] = []
266 |     notes: Optional[str] = None
267 |


--------------------------------------------------------------------------------
/backend/app/models/ml_models.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Machine Learning Models for OpenStatica
  3 | Pydantic models for ML operations
  4 | """
  5 |
  6 | from pydantic import BaseModel, Field, validator
  7 | from typing import Dict, List, Optional, Any, Union
  8 | from enum import Enum
  9 | from datetime import datetime
 10 |
 11 |
 12 | class MLTask(str, Enum):
 13 |     """Machine learning task types"""
 14 |     CLASSIFICATION = "classification"
 15 |     REGRESSION = "regression"
 16 |     CLUSTERING = "clustering"
 17 |     DIMENSIONALITY_REDUCTION = "dimensionality"
 18 |     ANOMALY_DETECTION = "anomaly"
 19 |     TIME_SERIES_FORECASTING = "time_series"
 20 |     NLP = "nlp"
 21 |     COMPUTER_VISION = "computer_vision"
 22 |     REINFORCEMENT_LEARNING = "reinforcement"
 23 |
 24 |
 25 | class Algorithm(str, Enum):
 26 |     """ML algorithms"""
 27 |     # Classification
 28 |     LOGISTIC_REGRESSION = "logistic"
 29 |     SVM = "svm"
 30 |     RANDOM_FOREST = "rf"
 31 |     GRADIENT_BOOSTING = "gb"
 32 |     XGBOOST = "xgboost"
 33 |     LIGHTGBM = "lightgbm"
 34 |     CATBOOST = "catboost"
 35 |     NAIVE_BAYES = "nb"
 36 |     KNN = "knn"
 37 |     DECISION_TREE = "dt"
 38 |
 39 |     # Deep Learning
 40 |     MLP = "mlp"
 41 |     CNN = "cnn"
 42 |     RNN = "rnn"
 43 |     LSTM = "lstm"
 44 |     GRU = "gru"
 45 |     TRANSFORMER = "transformer"
 46 |     AUTOENCODER = "autoencoder"
 47 |
 48 |     # Clustering
 49 |     KMEANS = "kmeans"
 50 |     DBSCAN = "dbscan"
 51 |     HIERARCHICAL = "hierarchical"
 52 |     GAUSSIAN_MIXTURE = "gaussian_mixture"
 53 |     MEAN_SHIFT = "mean_shift"
 54 |     SPECTRAL = "spectral"
 55 |
 56 |     # Dimensionality Reduction
 57 |     PCA = "pca"
 58 |     TSNE = "tsne"
 59 |     UMAP = "umap"
 60 |     LDA = "lda"
 61 |     ICA = "ica"
 62 |     NMF = "nmf"
 63 |
 64 |
 65 | class OptimizationMethod(str, Enum):
 66 |     """Hyperparameter optimization methods"""
 67 |     GRID_SEARCH = "grid"
 68 |     RANDOM_SEARCH = "random"
 69 |     BAYESIAN = "bayesian"
 70 |     HYPERBAND = "hyperband"
 71 |     OPTUNA = "optuna"
 72 |     GENETIC = "genetic"
 73 |
 74 |
 75 | class MetricType(str, Enum):
 76 |     """Evaluation metrics"""
 77 |     # Classification
 78 |     ACCURACY = "accuracy"
 79 |     PRECISION = "precision"
 80 |     RECALL = "recall"
 81 |     F1_SCORE = "f1"
 82 |     ROC_AUC = "roc_auc"
 83 |     PR_AUC = "pr_auc"
 84 |     LOG_LOSS = "log_loss"
 85 |     MATTHEWS_CORR = "matthews"
 86 |
 87 |     # Regression
 88 |     MSE = "mse"
 89 |     RMSE = "rmse"
 90 |     MAE = "mae"
 91 |     R2 = "r2"
 92 |     MAPE = "mape"
 93 |     EXPLAINED_VARIANCE = "explained_variance"
 94 |
 95 |     # Clustering
 96 |     SILHOUETTE = "silhouette"
 97 |     CALINSKI_HARABASZ = "calinski_harabasz"
 98 |     DAVIES_BOULDIN = "davies_bouldin"
 99 |     INERTIA = "inertia"
100 |
101 |
102 | class HyperparametersConfig(BaseModel):
103 |     """Hyperparameter configuration"""
104 |     algorithm: Algorithm
105 |     parameters: Dict[str, Any]
106 |     search_space: Optional[Dict[str, List[Any]]] = None
107 |     optimization_method: Optional[OptimizationMethod] = None
108 |     n_trials: Optional[int] = 100
109 |     cv_folds: Optional[int] = 5
110 |     scoring_metric: Optional[MetricType] = None
111 |
112 |
113 | class FeatureImportance(BaseModel):
114 |     """Feature importance scores"""
115 |     feature: str
116 |     importance: float
117 |     std: Optional[float] = None
118 |     rank: Optional[int] = None
119 |
120 |
121 | class ModelMetrics(BaseModel):
122 |     """Model evaluation metrics"""
123 |     metric_name: MetricType
124 |     value: float
125 |     std: Optional[float] = None
126 |     confidence_interval: Optional[List[float]] = None
127 |
128 |
129 | class MLRequest(BaseModel):
130 |     """Base ML request"""
131 |     session_id: str
132 |     task: MLTask
133 |     algorithm: Algorithm
134 |     features: List[str]
135 |     target: Optional[str] = None  # For supervised learning
136 |
137 |     # Data split
138 |     train_test_split: float = 0.8
139 |     validation_split: Optional[float] = None
140 |     stratify: bool = True
141 |     random_state: Optional[int] = 42
142 |
143 |     # Training options
144 |     auto_ml: bool = False
145 |     cross_validate: bool = True
146 |     cv_folds: int = 5
147 |
148 |     # Hyperparameters
149 |     hyperparameters: Optional[Dict[str, Any]] = {}
150 |     optimize_hyperparameters: bool = False
151 |     optimization_config: Optional[HyperparametersConfig] = None
152 |
153 |     # Options
154 |     options: Dict[str, Any] = {}
155 |
156 |     @validator('train_test_split')
157 |     def validate_split(cls, v):
158 |         if not 0 < v < 1:
159 |             raise ValueError('Train test split must be between 0 and 1')
160 |         return v
161 |
162 |
163 | class MLResult(BaseModel):
164 |     """ML training result"""
165 |     model_id: str
166 |     task: MLTask
167 |     algorithm: Algorithm
168 |
169 |     # Training info
170 |     training_time: float
171 |     n_samples_train: int
172 |     n_samples_test: Optional[int] = None
173 |     n_features: int
174 |
175 |     # Metrics
176 |     metrics: Dict[str, float]
177 |     cv_scores: Optional[List[float]] = None
178 |
179 |     # Feature importance
180 |     feature_importance: Optional[List[FeatureImportance]] = None
181 |
182 |     # Model specific
183 |     confusion_matrix: Optional[List[List[int]]] = None
184 |     classification_report: Optional[Dict[str, Dict[str, float]]] = None
185 |     roc_curve: Optional[Dict[str, List[float]]] = None
186 |     pr_curve: Optional[Dict[str, List[float]]] = None
187 |
188 |     # Clustering specific
189 |     cluster_centers: Optional[List[List[float]]] = None
190 |     cluster_labels: Optional[List[int]] = None
191 |     n_clusters: Optional[int] = None
192 |
193 |     # Hyperparameters
194 |     best_hyperparameters: Optional[Dict[str, Any]] = None
195 |     hyperparameter_search_results: Optional[List[Dict[str, Any]]] = None
196 |
197 |     # Timestamps
198 |     created_at: datetime = Field(default_factory=datetime.now)
199 |     execution_time: float
200 |
201 |
202 | class ModelTrainingRequest(MLRequest):
203 |     """Detailed model training request"""
204 |
205 |     # Advanced options
206 |     class_weight: Optional[Union[str, Dict[int, float]]] = None
207 |     sample_weight: Optional[List[float]] = None
208 |
209 |     # Early stopping
210 |     early_stopping: bool = False
211 |     patience: int = 10
212 |     min_delta: float = 0.001
213 |
214 |     # Ensemble
215 |     ensemble: bool = False
216 |     ensemble_method: Optional[str] = None  # voting, stacking, blending
217 |     base_models: Optional[List[Algorithm]] = None
218 |
219 |     # Feature engineering
220 |     feature_engineering: Optional[Dict[str, Any]] = None
221 |     polynomial_features: bool = False
222 |     interaction_terms: bool = False
223 |
224 |     # Preprocessing
225 |     scale_features: bool = True
226 |     encode_categorical: bool = True
227 |     handle_missing: str = "drop"  # drop, impute_mean, impute_median, impute_mode
228 |     handle_outliers: Optional[str] = None  # clip, remove, transform
229 |
230 |     # Interpretability
231 |     compute_shap: bool = False
232 |     compute_lime: bool = False
233 |     compute_permutation_importance: bool = False
234 |
235 |
236 | class ModelPredictionRequest(BaseModel):
237 |     """Model prediction request"""
238 |     session_id: str
239 |     model_id: str
240 |     data: Union[List[Dict[str, Any]], str]  # Data or session_id with data
241 |
242 |     # Prediction options
243 |     return_probabilities: bool = False
244 |     return_confidence: bool = False
245 |     return_explanations: bool = False
246 |
247 |     # Batch processing
248 |     batch_size: Optional[int] = None
249 |
250 |     # Post-processing
251 |     threshold: Optional[float] = 0.5  # For binary classification
252 |     top_k: Optional[int] = None  # For multi-class
253 |
254 |
255 | class ModelPredictionResult(BaseModel):
256 |     """Model prediction result"""
257 |     model_id: str
258 |     predictions: List[Any]
259 |     probabilities: Optional[List[List[float]]] = None
260 |     confidence_scores: Optional[List[float]] = None
261 |     explanations: Optional[List[Dict[str, Any]]] = None
262 |
263 |     # Metadata
264 |     n_predictions: int
265 |     prediction_time: float
266 |     timestamp: datetime = Field(default_factory=datetime.now)
267 |
268 |
269 | class ModelEvaluationRequest(BaseModel):
270 |     """Model evaluation request"""
271 |     session_id: str
272 |     model_id: str
273 |     test_data: Union[str, Dict[str, Any]]  # Session ID or direct data
274 |
275 |     # Evaluation options
276 |     metrics: List[MetricType]
277 |     compute_confusion_matrix: bool = True
278 |     compute_roc_curve: bool = True
279 |     compute_pr_curve: bool = True
280 |     compute_calibration: bool = False
281 |
282 |     # Cross-validation
283 |     cross_validate: bool = False
284 |     cv_folds: int = 5
285 |
286 |     # Bootstrap
287 |     bootstrap: bool = False
288 |     n_bootstrap: int = 1000
289 |
290 |
291 | class ModelEvaluationResult(BaseModel):
292 |     """Model evaluation result"""
293 |     model_id: str
294 |
295 |     # Metrics
296 |     metrics: Dict[str, ModelMetrics]
297 |
298 |     # Classification specific
299 |     confusion_matrix: Optional[List[List[int]]] = None
300 |     classification_report: Optional[Dict[str, Dict[str, float]]] = None
301 |     roc_curve: Optional[Dict[str, Any]] = None
302 |     pr_curve: Optional[Dict[str, Any]] = None
303 |     calibration_curve: Optional[Dict[str, Any]] = None
304 |
305 |     # Regression specific
306 |     residuals: Optional[List[float]] = None
307 |     qq_plot: Optional[Dict[str, Any]] = None
308 |
309 |     # Cross-validation
310 |     cv_scores: Optional[Dict[str, List[float]]] = None
311 |
312 |     # Bootstrap
313 |     bootstrap_scores: Optional[Dict[str, Dict[str, float]]] = None
314 |
315 |     # Evaluation metadata
316 |     n_samples: int
317 |     evaluation_time: float
318 |     timestamp: datetime = Field(default_factory=datetime.now)
319 |
320 |
321 | class ModelComparisonRequest(BaseModel):
322 |     """Request to compare multiple models"""
323 |     session_id: str
324 |     model_ids: List[str]
325 |     test_data: Union[str, Dict[str, Any]]
326 |     metrics: List[MetricType]
327 |
328 |     # Statistical tests
329 |     statistical_test: Optional[str] = None  # t-test, wilcoxon, friedman
330 |     significance_level: float = 0.05
331 |
332 |
333 | class ModelComparisonResult(BaseModel):
334 |     """Result of model comparison"""
335 |     comparison_table: Dict[str, Dict[str, float]]
336 |     best_model: str
337 |     ranking: List[str]
338 |
339 |     # Statistical tests
340 |     statistical_test_results: Optional[Dict[str, Any]] = None
341 |     significant_differences: Optional[Dict[str, bool]] = None
342 |
343 |     # Visualizations
344 |     performance_plot: Optional[str] = None  # Base64 encoded plot
345 |
346 |     timestamp: datetime = Field(default_factory=datetime.now)
347 |
348 |
349 | class AutoMLRequest(MLRequest):
350 |     """AutoML request"""
351 |     time_budget: Optional[int] = 3600  # seconds
352 |     max_models: Optional[int] = 20
353 |
354 |     # Search space
355 |     algorithms_to_try: Optional[List[Algorithm]] = None
356 |     include_ensembles: bool = True
357 |     include_deep_learning: bool = False
358 |
359 |     # Optimization
360 |     optimization_metric: MetricType
361 |     optimization_direction: str = "maximize"  # maximize, minimize
362 |
363 |     # Feature selection
364 |     feature_selection: bool = True
365 |     max_features: Optional[int] = None
366 |
367 |     # Model selection
368 |     refit_best_model: bool = True
369 |     keep_top_k: int = 5
370 |
371 |
372 | class AutoMLResult(BaseModel):
373 |     """AutoML result"""
374 |     best_model_id: str
375 |     best_score: float
376 |
377 |     # Leaderboard
378 |     leaderboard: List[Dict[str, Any]]
379 |
380 |     # Feature importance across models
381 |     aggregated_feature_importance: List[FeatureImportance]
382 |
383 |     # Selected features
384 |     selected_features: Optional[List[str]] = None
385 |
386 |     # Training history
387 |     training_history: List[Dict[str, Any]]
388 |
389 |     # Best hyperparameters
390 |     best_hyperparameters: Dict[str, Any]
391 |
392 |     # Ensemble details
393 |     ensemble_weights: Optional[Dict[str, float]] = None
394 |
395 |     # Metadata
396 |     total_time: float
397 |     n_models_trained: int
398 |     timestamp: datetime = Field(default_factory=datetime.now)
399 |
400 |
401 | class DeepLearningConfig(BaseModel):
402 |     """Deep learning specific configuration"""
403 |     architecture: str  # mlp, cnn, rnn, transformer, etc.
404 |     layers: List[Dict[str, Any]]
405 |
406 |     # Training
407 |     epochs: int = 100
408 |     batch_size: int = 32
409 |     learning_rate: float = 0.001
410 |     optimizer: str = "adam"
411 |     loss_function: str
412 |
413 |     # Regularization
414 |     dropout_rate: Optional[float] = None
415 |     l1_regularization: Optional[float] = None
416 |     l2_regularization: Optional[float] = None
417 |
418 |     # Callbacks
419 |     early_stopping: bool = True
420 |     reduce_lr: bool = True
421 |     checkpoint: bool = True
422 |
423 |     # GPU
424 |     use_gpu: bool = False
425 |     distributed_training: bool = False
426 |
427 |
428 | class ModelExportRequest(BaseModel):
429 |     """Request to export a model"""
430 |     model_id: str
431 |     format: str = "pickle"  # pickle, joblib, onnx, pmml, tensorflow, pytorch
432 |     include_preprocessor: bool = True
433 |     include_metadata: bool = True
434 |     compression: Optional[str] = None  # gzip, zip
435 |
436 |
437 | class ModelDeploymentRequest(BaseModel):
438 |     """Request to deploy a model"""
439 |     model_id: str
440 |     deployment_type: str  # rest_api, batch, streaming, edge
441 |
442 |     # API deployment
443 |     endpoint_name: Optional[str] = None
444 |     auto_scaling: bool = True
445 |     min_instances: int = 1
446 |     max_instances: int = 10
447 |
448 |     # Monitoring
449 |     enable_monitoring: bool = True
450 |     log_predictions: bool = True
451 |     alert_thresholds: Optional[Dict[str, float]] = None
452 |
453 |     # A/B testing
454 |     enable_ab_testing: bool = False
455 |     traffic_percentage: Optional[float] = None
456 |


--------------------------------------------------------------------------------
/backend/app/plugins/README.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/plugins/README.md


--------------------------------------------------------------------------------
/backend/app/plugins/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/plugins/__init__.py


--------------------------------------------------------------------------------
/backend/app/plugins/example_plugin/init.py:
--------------------------------------------------------------------------------
 1 | from app.core.base import BasePlugin, BaseEngine
 2 | from typing import List, Any
 3 |
 4 |
 5 | class ExampleEngine(BaseEngine):
 6 |     """Example custom engine"""
 7 |
 8 |     def __init__(self):
 9 |         super().__init__("example_engine", "custom")
10 |
11 |     async def _setup(self):
12 |         """Setup engine"""
13 |         pass
14 |
15 |     async def execute(self, data: Any, params: dict) -> Any:
16 |         """Execute custom logic"""
17 |         return {"message": "Example engine executed", "data": data}
18 |
19 |
20 | class Plugin(BasePlugin):
21 |     """Example plugin for OpenStatica"""
22 |
23 |     def __init__(self, name: str, version: str):
24 |         super().__init__(name, version)
25 |         self.engines = []
26 |
27 |     async def initialize(self):
28 |         """Initialize plugin"""
29 |         # Create and setup engines
30 |         example_engine = ExampleEngine()
31 |         await example_engine.initialize()
32 |         self.engines.append(example_engine)
33 |
34 |     def get_engines(self) -> List[BaseEngine]:
35 |         """Get engines provided by this plugin"""
36 |         return self.engines
37 |
38 |     def get_routes(self) -> List[Any]:
39 |         """Get API routes provided by this plugin"""
40 |         # Could return FastAPI routers here
41 |         return []
42 |


--------------------------------------------------------------------------------
/backend/app/plugins/example_plugin/plugin.json:
--------------------------------------------------------------------------------
 1 | {
 2 |   "name": "Example Plugin",
 3 |   "version": "1.0.0",
 4 |   "description": "Example plugin demonstrating OpenStatica plugin system",
 5 |   "author": "OpenStatica Team",
 6 |   "dependencies": [],
 7 |   "engines": [
 8 |     "example_engine"
 9 |   ],
10 |   "routes": []
11 | }


--------------------------------------------------------------------------------
/backend/app/services/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/services/__init__.py


--------------------------------------------------------------------------------
/backend/app/services/analysis_service.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Analysis Service for OpenStatica
  3 | Coordinates statistical analyses and manages results
  4 | """
  5 |
  6 | import pandas as pd
  7 | import numpy as np
  8 | from typing import Dict, Any, List, Optional, Tuple
  9 | from scipy import stats
 10 | import statsmodels.api as sm
 11 | from statsmodels.stats.multicomp import pairwise_tukeyhsd
 12 | from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox
 13 | from statsmodels.stats.stattools import durbin_watson
 14 | import pingouin as pg
 15 | from datetime import datetime
 16 | import asyncio
 17 | import logging
 18 | from app.core.base import Result
 19 |
 20 | logger = logging.getLogger(__name__)
 21 |
 22 |
 23 | class AnalysisService:
 24 |     """Service for coordinating statistical analyses"""
 25 |
 26 |     def __init__(self):
 27 |         self.analysis_cache = {}
 28 |         self.supported_tests = {
 29 |             'descriptive': self.run_descriptive_analysis,
 30 |             'frequency': self.run_frequency_analysis,
 31 |             'correlation': self.run_correlation_analysis,
 32 |             'ttest': self.run_ttest,
 33 |             'anova': self.run_anova,
 34 |             'chi_square': self.run_chi_square,
 35 |             'regression': self.run_regression,
 36 |             'factor_analysis': self.run_factor_analysis,
 37 |             'pca': self.run_pca,
 38 |             'time_series': self.run_time_series_analysis
 39 |         }
 40 |
 41 |     async def analyze(self,
 42 |                       data: pd.DataFrame,
 43 |                       analysis_type: str,
 44 |                       variables: List[str],
 45 |                       options: Dict[str, Any] = None) -> Result:
 46 |         """Main entry point for analyses"""
 47 |         options = options or {}
 48 |
 49 |         if analysis_type not in self.supported_tests:
 50 |             return Result.fail(f"Unsupported analysis type: {analysis_type}")
 51 |
 52 |         try:
 53 |             analysis_func = self.supported_tests[analysis_type]
 54 |             result = await analysis_func(data, variables, options)
 55 |
 56 |             # Cache result
 57 |             cache_key = self._generate_cache_key(analysis_type, variables, options)
 58 |             self.analysis_cache[cache_key] = result
 59 |
 60 |             return Result.ok(result)
 61 |
 62 |         except Exception as e:
 63 |             logger.error(f"Analysis error: {e}")
 64 |             return Result.fail(str(e))
 65 |
 66 |     async def run_descriptive_analysis(self,
 67 |                                        data: pd.DataFrame,
 68 |                                        variables: List[str],
 69 |                                        options: Dict[str, Any]) -> Dict[str, Any]:
 70 |         """Run comprehensive descriptive statistics"""
 71 |         include_advanced = options.get('include_advanced', True)
 72 |         confidence_level = options.get('confidence_level', 0.95)
 73 |
 74 |         results = {}
 75 |
 76 |         for var in variables:
 77 |             if var not in data.columns:
 78 |                 continue
 79 |
 80 |             col_data = data[var].dropna()
 81 |
 82 |             if pd.api.types.is_numeric_dtype(col_data):
 83 |                 # Basic statistics
 84 |                 stats_dict = {
 85 |                     'n': len(col_data),
 86 |                     'missing': data[var].isnull().sum(),
 87 |                     'mean': float(col_data.mean()),
 88 |                     'median': float(col_data.median()),
 89 |                     'mode': float(col_data.mode()[0]) if len(col_data.mode()) > 0 else None,
 90 |                     'std': float(col_data.std()),
 91 |                     'variance': float(col_data.var()),
 92 |                     'min': float(col_data.min()),
 93 |                     'max': float(col_data.max()),
 94 |                     'range': float(col_data.max() - col_data.min()),
 95 |                     'q1': float(col_data.quantile(0.25)),
 96 |                     'q3': float(col_data.quantile(0.75)),
 97 |                     'iqr': float(col_data.quantile(0.75) - col_data.quantile(0.25))
 98 |                 }
 99 |
100 |                 if include_advanced:
101 |                     # Advanced statistics
102 |                     stats_dict.update({
103 |                         'skewness': float(col_data.skew()),
104 |                         'kurtosis': float(col_data.kurtosis()),
105 |                         'sem': float(col_data.sem()),
106 |                         'mad': float(col_data.mad()),
107 |                         'cv': float(col_data.std() / col_data.mean() * 100) if col_data.mean() != 0 else None,
108 |                         'geometric_mean': float(stats.gmean(col_data[col_data > 0])) if (col_data > 0).any() else None,
109 |                         'harmonic_mean': float(stats.hmean(col_data[col_data > 0])) if (col_data > 0).any() else None,
110 |                         'trimmed_mean': float(stats.trim_mean(col_data, 0.1))
111 |                     })
112 |
113 |                     # Confidence interval
114 |                     ci = stats.t.interval(
115 |                         confidence_level,
116 |                         len(col_data) - 1,
117 |                         loc=col_data.mean(),
118 |                         scale=col_data.sem()
119 |                     )
120 |                     stats_dict['confidence_interval'] = [float(ci[0]), float(ci[1])]
121 |
122 |                     # Normality tests
123 |                     if len(col_data) >= 3:
124 |                         shapiro_stat, shapiro_p = stats.shapiro(col_data[:5000] if len(col_data) > 5000 else col_data)
125 |                         stats_dict['normality'] = {
126 |                             'shapiro_wilk': {
127 |                                 'statistic': float(shapiro_stat),
128 |                                 'p_value': float(shapiro_p),
129 |                                 'is_normal': shapiro_p > 0.05
130 |                             }
131 |                         }
132 |
133 |                     # Outliers detection
134 |                     q1 = col_data.quantile(0.25)
135 |                     q3 = col_data.quantile(0.75)
136 |                     iqr = q3 - q1
137 |                     lower_bound = q1 - 1.5 * iqr
138 |                     upper_bound = q3 + 1.5 * iqr
139 |                     outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
140 |
141 |                     stats_dict['outliers'] = {
142 |                         'count': len(outliers),
143 |                         'percentage': len(outliers) / len(col_data) * 100,
144 |                         'lower_bound': float(lower_bound),
145 |                         'upper_bound': float(upper_bound)
146 |                     }
147 |
148 |                 results[var] = stats_dict
149 |
150 |             else:
151 |                 # Categorical variable
152 |                 value_counts = data[var].value_counts()
153 |                 results[var] = {
154 |                     'type': 'categorical',
155 |                     'n': len(col_data),
156 |                     'missing': data[var].isnull().sum(),
157 |                     'unique': data[var].nunique(),
158 |                     'mode': value_counts.index[0] if len(value_counts) > 0 else None,
159 |                     'mode_freq': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
160 |                     'value_counts': value_counts.head(20).to_dict(),
161 |                     'entropy': float(stats.entropy(value_counts))
162 |                 }
163 |
164 |         return results
165 |
166 |     async def run_frequency_analysis(self,
167 |                                      data: pd.DataFrame,
168 |                                      variables: List[str],
169 |                                      options: Dict[str, Any]) -> Dict[str, Any]:
170 |         """Run frequency analysis"""
171 |         results = {}
172 |
173 |         for var in variables:
174 |             if var not in data.columns:
175 |                 continue
176 |
177 |             if pd.api.types.is_numeric_dtype(data[var]):
178 |                 # Create frequency bins for numeric data
179 |                 n_bins = options.get('n_bins', 10)
180 |                 col_data = data[var].dropna()
181 |
182 |                 counts, bin_edges = np.histogram(col_data, bins=n_bins)
183 |                 bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
184 |
185 |                 results[var] = {
186 |                     'type': 'numeric',
187 |                     'bins': bin_centers.tolist(),
188 |                     'frequencies': counts.tolist(),
189 |                     'relative_frequencies': (counts / len(col_data)).tolist(),
190 |                     'cumulative_frequencies': np.cumsum(counts).tolist(),
191 |                     'bin_edges': bin_edges.tolist()
192 |                 }
193 |             else:
194 |                 # Categorical frequency
195 |                 value_counts = data[var].value_counts()
196 |                 value_pcts = data[var].value_counts(normalize=True) * 100
197 |
198 |                 results[var] = {
199 |                     'type': 'categorical',
200 |                     'categories': value_counts.index.tolist(),
201 |                     'frequencies': value_counts.values.tolist(),
202 |                     'percentages': value_pcts.values.tolist(),
203 |                     'cumulative_percentages': value_pcts.cumsum().values.tolist()
204 |                 }
205 |
206 |         return results
207 |
208 |     async def run_correlation_analysis(self,
209 |                                        data: pd.DataFrame,
210 |                                        variables: List[str],
211 |                                        options: Dict[str, Any]) -> Dict[str, Any]:
212 |         """Run correlation analysis"""
213 |         method = options.get('method', 'pearson')
214 |         include_p_values = options.get('include_p_values', True)
215 |
216 |         # Filter numeric variables
217 |         numeric_vars = [v for v in variables if v in data.columns and pd.api.types.is_numeric_dtype(data[v])]
218 |
219 |         if len(numeric_vars) < 2:
220 |             raise ValueError("Need at least 2 numeric variables for correlation analysis")
221 |
222 |         # Calculate correlation matrix
223 |         corr_matrix = data[numeric_vars].corr(method=method)
224 |
225 |         results = {
226 |             'correlation_matrix': corr_matrix.to_dict(),
227 |             'method': method,
228 |             'n_observations': len(data[numeric_vars].dropna())
229 |         }
230 |
231 |         # Calculate p-values if requested
232 |         if include_p_values:
233 |             p_values = pd.DataFrame(
234 |                 np.zeros((len(numeric_vars), len(numeric_vars))),
235 |                 columns=numeric_vars,
236 |                 index=numeric_vars
237 |             )
238 |
239 |             for i, var1 in enumerate(numeric_vars):
240 |                 for j, var2 in enumerate(numeric_vars):
241 |                     if i != j:
242 |                         if method == 'pearson':
243 |                             _, p = stats.pearsonr(
244 |                                 data[var1].dropna(),
245 |                                 data[var2].dropna()
246 |                             )
247 |                         elif method == 'spearman':
248 |                             _, p = stats.spearmanr(
249 |                                 data[var1].dropna(),
250 |                                 data[var2].dropna()
251 |                             )
252 |                         elif method == 'kendall':
253 |                             _, p = stats.kendalltau(
254 |                                 data[var1].dropna(),
255 |                                 data[var2].dropna()
256 |                             )
257 |                         p_values.iloc[i, j] = p
258 |
259 |             results['p_values'] = p_values.to_dict()
260 |
261 |         # Find significant correlations
262 |         significant_corr = []
263 |         for i in range(len(corr_matrix.columns)):
264 |             for j in range(i + 1, len(corr_matrix.columns)):
265 |                 corr_val = corr_matrix.iloc[i, j]
266 |                 if abs(corr_val) > 0.5 and abs(corr_val) < 1:
267 |                     significant_corr.append({
268 |                         'var1': corr_matrix.columns[i],
269 |                         'var2': corr_matrix.columns[j],
270 |                         'correlation': float(corr_val)
271 |                     })
272 |
273 |         results['significant_correlations'] = significant_corr
274 |
275 |         return results
276 |
277 |     async def run_ttest(self,
278 |                         data: pd.DataFrame,
279 |                         variables: List[str],
280 |                         options: Dict[str, Any]) -> Dict[str, Any]:
281 |         """Run t-test analysis"""
282 |         test_type = options.get('test_type', 'independent')
283 |         alpha = options.get('alpha', 0.05)
284 |
285 |         results = {
286 |             'test_type': test_type,
287 |             'alpha': alpha
288 |         }
289 |
290 |         if test_type == 'one_sample':
291 |             # One-sample t-test
292 |             test_value = options.get('test_value', 0)
293 |             var = variables[0]
294 |
295 |             sample_data = data[var].dropna()
296 |             t_stat, p_value = stats.ttest_1samp(sample_data, test_value)
297 |
298 |             # Effect size (Cohen's d)
299 |             cohens_d = (sample_data.mean() - test_value) / sample_data.std()
300 |
301 |             results.update({
302 |                 'variable': var,
303 |                 'test_value': test_value,
304 |                 'sample_mean': float(sample_data.mean()),
305 |                 'sample_std': float(sample_data.std()),
306 |                 't_statistic': float(t_stat),
307 |                 'p_value': float(p_value),
308 |                 'cohens_d': float(cohens_d),
309 |                 'significant': p_value < alpha,
310 |                 'conclusion': f"{'Reject' if p_value < alpha else 'Fail to reject'} null hypothesis"
311 |             })
312 |
313 |         elif test_type == 'independent':
314 |             # Independent samples t-test
315 |             if len(variables) == 2:
316 |                 # Two numeric variables
317 |                 var1_data = data[variables[0]].dropna()
318 |                 var2_data = data[variables[1]].dropna()
319 |             else:
320 |                 # One numeric, one grouping variable
321 |                 numeric_var = options.get('numeric_var')
322 |                 group_var = options.get('group_var')
323 |
324 |                 groups = data[group_var].unique()
325 |                 if len(groups) != 2:
326 |                     raise ValueError(f"Expected 2 groups, found {len(groups)}")
327 |
328 |                 var1_data = data[data[group_var] == groups[0]][numeric_var].dropna()
329 |                 var2_data = data[data[group_var] == groups[1]][numeric_var].dropna()
330 |
331 |             # Test for equal variances
332 |             levene_stat, levene_p = stats.levene(var1_data, var2_data)
333 |             equal_var = levene_p > 0.05
334 |
335 |             # Perform t-test
336 |             t_stat, p_value = stats.ttest_ind(var1_data, var2_data, equal_var=equal_var)
337 |
338 |             # Effect size
339 |             pooled_std = np.sqrt(
340 |                 ((len(var1_data) - 1) * var1_data.var() +
341 |                  (len(var2_data) - 1) * var2_data.var()) /
342 |                 (len(var1_data) + len(var2_data) - 2)
343 |             )
344 |             cohens_d = (var1_data.mean() - var2_data.mean()) / pooled_std
345 |
346 |             results.update({
347 |                 'group1_mean': float(var1_data.mean()),
348 |                 'group1_std': float(var1_data.std()),
349 |                 'group1_n': len(var1_data),
350 |                 'group2_mean': float(var2_data.mean()),
351 |                 'group2_std': float(var2_data.std()),
352 |                 'group2_n': len(var2_data),
353 |                 'mean_difference': float(var1_data.mean() - var2_data.mean()),
354 |                 't_statistic': float(t_stat),
355 |                 'p_value': float(p_value),
356 |                 'levene_test': {
357 |                     'statistic': float(levene_stat),
358 |                     'p_value': float(levene_p),
359 |                     'equal_variances': equal_var
360 |                 },
361 |                 'cohens_d': float(cohens_d),
362 |                 'significant': p_value < alpha
363 |             })
364 |
365 |         elif test_type == 'paired':
366 |             # Paired samples t-test
367 |             if len(variables) != 2:
368 |                 raise ValueError("Paired t-test requires exactly 2 variables")
369 |
370 |             var1_data = data[variables[0]].dropna()
371 |             var2_data = data[variables[1]].dropna()
372 |
373 |             # Ensure same length
374 |             paired_data = pd.DataFrame({
375 |                 'var1': var1_data,
376 |                 'var2': var2_data
377 |             }).dropna()
378 |
379 |             differences = paired_data['var2'] - paired_data['var1']
380 |
381 |             t_stat, p_value = stats.ttest_rel(paired_data['var1'], paired_data['var2'])
382 |
383 |             # Effect size
384 |             cohens_d = differences.mean() / differences.std()
385 |
386 |             results.update({
387 |                 'var1_mean': float(paired_data['var1'].mean()),
388 |                 'var2_mean': float(paired_data['var2'].mean()),
389 |                 'mean_difference': float(differences.mean()),
390 |                 'std_difference': float(differences.std()),
391 |                 't_statistic': float(t_stat),
392 |                 'p_value': float(p_value),
393 |                 'cohens_d': float(cohens_d),
394 |                 'significant': p_value < alpha
395 |             })
396 |
397 |         return results
398 |
399 |     async def run_anova(self,
400 |                         data: pd.DataFrame,
401 |                         variables: List[str],
402 |                         options: Dict[str, Any]) -> Dict[str, Any]:
403 |         """Run ANOVA analysis"""
404 |         anova_type = options.get('type', 'one_way')
405 |         alpha = options.get('alpha', 0.05)
406 |         post_hoc = options.get('post_hoc', True)
407 |
408 |         results = {
409 |             'type': anova_type,
410 |             'alpha': alpha
411 |         }
412 |
413 |         if anova_type == 'one_way':
414 |             dependent_var = options.get('dependent')
415 |             factor_var = options.get('factor')
416 |
417 |             # Prepare data for ANOVA
418 |             groups = []
419 |             group_labels = []
420 |
421 |             for group in data[factor_var].unique():
422 |                 group_data = data[data[factor_var] == group][dependent_var].dropna()
423 |                 if len(group_data) > 0:
424 |                     groups.append(group_data.values)
425 |                     group_labels.append(str(group))
426 |
427 |             # Perform ANOVA
428 |             f_stat, p_value = stats.f_oneway(*groups)
429 |
430 |             # Calculate effect size (eta-squared)
431 |             grand_mean = data[dependent_var].mean()
432 |             ss_total = ((data[dependent_var] - grand_mean) ** 2).sum()
433 |             ss_between = sum(
434 |                 len(g) * (np.mean(g) - grand_mean) ** 2
435 |                 for g in groups
436 |             )
437 |             eta_squared = ss_between / ss_total
438 |
439 |             results.update({
440 |                 'dependent': dependent_var,
441 |                 'factor': factor_var,
442 |                 'f_statistic': float(f_stat),
443 |                 'p_value': float(p_value),
444 |                 'eta_squared': float(eta_squared),
445 |                 'significant': p_value < alpha,
446 |                 'n_groups': len(groups),
447 |                 'group_means': {
448 |                     label: float(np.mean(g))
449 |                     for label, g in zip(group_labels, groups)
450 |                 }
451 |             })
452 |
453 |             # Post-hoc tests if significant
454 |             if post_hoc and p_value < alpha and len(groups) > 2:
455 |                 # Tukey HSD
456 |                 df_long = data[[dependent_var, factor_var]].dropna()
457 |                 tukey_result = pairwise_tukeyhsd(
458 |                     df_long[dependent_var],
459 |                     df_long[factor_var],
460 |                     alpha=alpha
461 |                 )
462 |
463 |                 results['post_hoc'] = {
464 |                     'method': 'Tukey HSD',
465 |                     'results': str(tukey_result)
466 |                 }
467 |
468 |         elif anova_type == 'two_way':
469 |             dependent_var = options.get('dependent')
470 |             factor1 = options.get('factor1')
471 |             factor2 = options.get('factor2')
472 |
473 |             # Use pingouin for two-way ANOVA
474 |             aov = pg.anova(
475 |                 data=data,
476 |                 dv=dependent_var,
477 |                 between=[factor1, factor2]
478 |             )
479 |
480 |             results.update({
481 |                 'dependent': dependent_var,
482 |                 'factors': [factor1, factor2],
483 |                 'anova_table': aov.to_dict(orient='records')
484 |             })
485 |
486 |         return results
487 |
488 |     async def run_chi_square(self,
489 |                              data: pd.DataFrame,
490 |                              variables: List[str],
491 |                              options: Dict[str, Any]) -> Dict[str, Any]:
492 |         """Run chi-square test"""
493 |         if len(variables) != 2:
494 |             raise ValueError("Chi-square test requires exactly 2 categorical variables")
495 |
496 |         # Create contingency table
497 |         contingency_table = pd.crosstab(data[variables[0]], data[variables[1]])
498 |
499 |         # Perform chi-square test
500 |         chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
501 |
502 |         # Calculate Cramér's V for effect size
503 |         n = contingency_table.sum().sum()
504 |         min_dim = min(contingency_table.shape[0] - 1, contingency_table.shape[1] - 1)
505 |         cramers_v = np.sqrt(chi2 / (n * min_dim))
506 |
507 |         results = {
508 |             'variables': variables,
509 |             'chi2_statistic': float(chi2),
510 |             'p_value': float(p_value),
511 |             'degrees_of_freedom': int(dof),
512 |             'cramers_v': float(cramers_v),
513 |             'significant': p_value < options.get('alpha', 0.05),
514 |             'contingency_table': contingency_table.to_dict(),
515 |             'expected_frequencies': expected.tolist()
516 |         }
517 |
518 |         return results
519 |
520 |     async def run_regression(self,
521 |                              data: pd.DataFrame,
522 |                              variables: List[str],
523 |                              options: Dict[str, Any]) -> Dict[str, Any]:
524 |         """Run regression analysis"""
525 |         regression_type = options.get('type', 'linear')
526 |         dependent_var = options.get('dependent')
527 |         independent_vars = options.get('independent', [v for v in variables if v != dependent_var])
528 |
529 |         # Prepare data
530 |         df = data[[dependent_var] + independent_vars].dropna()
531 |         X = df[independent_vars]
532 |         y = df[dependent_var]
533 |
534 |         # Add constant
535 |         X = sm.add_constant(X)
536 |
537 |         results = {
538 |             'type': regression_type,
539 |             'dependent': dependent_var,
540 |             'independent': independent_vars,
541 |             'n_observations': len(df)
542 |         }
543 |
544 |         if regression_type == 'linear':
545 |             # OLS regression
546 |             model = sm.OLS(y, X).fit()
547 |
548 |             results.update({
549 |                 'coefficients': model.params.to_dict(),
550 |                 'std_errors': model.bse.to_dict(),
551 |                 't_values': model.tvalues.to_dict(),
552 |                 'p_values': model.pvalues.to_dict(),
553 |                 'r_squared': float(model.rsquared),
554 |                 'adjusted_r_squared': float(model.rsquared_adj),
555 |                 'f_statistic': float(model.fvalue),
556 |                 'f_pvalue': float(model.f_pvalue),
557 |                 'aic': float(model.aic),
558 |                 'bic': float(model.bic)
559 |             })
560 |
561 |             # Diagnostic tests
562 |             residuals = model.resid
563 |             fitted = model.fittedvalues
564 |
565 |             # Durbin-Watson for autocorrelation
566 |             dw = durbin_watson(residuals)
567 |
568 |             # Breusch-Pagan for heteroscedasticity
569 |             bp_test = het_breuschpagan(residuals, X)
570 |
571 |             # Jarque-Bera for normality
572 |             jb_test = stats.jarque_bera(residuals)
573 |
574 |             results['diagnostics'] = {
575 |                 'durbin_watson': float(dw),
576 |                 'breusch_pagan': {
577 |                     'statistic': float(bp_test[0]),
578 |                     'p_value': float(bp_test[1])
579 |                 },
580 |                 'jarque_bera': {
581 |                     'statistic': float(jb_test[0]),
582 |                     'p_value': float(jb_test[1])
583 |                 }
584 |             }
585 |
586 |         elif regression_type == 'logistic':
587 |             # Logistic regression
588 |             model = sm.Logit(y, X).fit()
589 |
590 |             results.update({
591 |                 'coefficients': model.params.to_dict(),
592 |                 'std_errors': model.bse.to_dict(),
593 |                 'z_values': model.tvalues.to_dict(),
594 |                 'p_values': model.pvalues.to_dict(),
595 |                 'pseudo_r_squared': float(model.prsquared),
596 |                 'log_likelihood': float(model.llf),
597 |                 'aic': float(model.aic),
598 |                 'bic': float(model.bic)
599 |             })
600 |
601 |         return results
602 |
603 |     async def run_factor_analysis(self,
604 |                                   data: pd.DataFrame,
605 |                                   variables: List[str],
606 |                                   options: Dict[str, Any]) -> Dict[str, Any]:
607 |         """Run factor analysis"""
608 |         from factor_analyzer import FactorAnalyzer
609 |         from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo
610 |
611 |         n_factors = options.get('n_factors', 3)
612 |         rotation = options.get('rotation', 'varimax')
613 |
614 |         # Prepare data
615 |         df = data[variables].dropna()
616 |
617 |         # Test assumptions
618 |         chi_square, p_value = calculate_bartlett_sphericity(df)
619 |         kmo_all, kmo_model = calculate_kmo(df)
620 |
621 |         # Perform factor analysis
622 |         fa = FactorAnalyzer(n_factors=n_factors, rotation=rotation)
623 |         fa.fit(df)
624 |
625 |         # Get results
626 |         loadings = fa.loadings_
627 |         communalities = fa.get_communalities()
628 |         eigenvalues = fa.get_eigenvalues()[0]
629 |         variance = fa.get_factor_variance()
630 |
631 |         results = {
632 |             'n_factors': n_factors,
633 |             'rotation': rotation,
634 |             'bartlett_test': {
635 |                 'chi_square': float(chi_square),
636 |                 'p_value': float(p_value)
637 |             },
638 |             'kmo': float(kmo_model),
639 |             'loadings': pd.DataFrame(
640 |                 loadings,
641 |                 columns=[f'Factor{i + 1}' for i in range(n_factors)],
642 |                 index=variables
643 |             ).to_dict(),
644 |             'communalities': dict(zip(variables, communalities)),
645 |             'eigenvalues': eigenvalues.tolist(),
646 |             'variance_explained': {
647 |                 'SS_loadings': variance[0].tolist(),
648 |                 'proportion_var': variance[1].tolist(),
649 |                 'cumulative_var': variance[2].tolist()
650 |             }
651 |         }
652 |
653 |         return results
654 |
655 |     async def run_pca(self,
656 |                       data: pd.DataFrame,
657 |                       variables: List[str],
658 |                       options: Dict[str, Any]) -> Dict[str, Any]:
659 |         """Run Principal Component Analysis"""
660 |         from sklearn.decomposition import PCA
661 |         from sklearn.preprocessing import StandardScaler
662 |
663 |         n_components = options.get('n_components', min(len(variables), 5))
664 |         standardize = options.get('standardize', True)
665 |
666 |         # Prepare data
667 |         df = data[variables].dropna()
668 |
669 |         if standardize:
670 |             scaler = StandardScaler()
671 |             df_scaled = scaler.fit_transform(df)
672 |         else:
673 |             df_scaled = df.values
674 |
675 |         # Perform PCA
676 |         pca = PCA(n_components=n_components)
677 |         scores = pca.fit_transform(df_scaled)
678 |
679 |         # Calculate loadings
680 |         loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
681 |
682 |         results = {
683 |             'n_components': n_components,
684 |             'explained_variance': pca.explained_variance_.tolist(),
685 |             'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
686 |             'cumulative_variance_ratio': np.cumsum(pca.explained_variance_ratio_).tolist(),
687 |             'loadings': pd.DataFrame(
688 |                 loadings,
689 |                 columns=[f'PC{i + 1}' for i in range(n_components)],
690 |                 index=variables
691 |             ).to_dict(),
692 |             'scores': pd.DataFrame(
693 |                 scores,
694 |                 columns=[f'PC{i + 1}' for i in range(n_components)]
695 |             ).head(100).to_dict()  # First 100 scores
696 |         }
697 |
698 |         return results
699 |
700 |     async def run_time_series_analysis(self,
701 |                                        data: pd.DataFrame,
702 |                                        variables: List[str],
703 |                                        options: Dict[str, Any]) -> Dict[str, Any]:
704 |         """Run time series analysis"""
705 |         from statsmodels.tsa.seasonal import seasonal_decompose
706 |         from statsmodels.tsa.stattools import adfuller, acf, pacf
707 |
708 |         time_var = options.get('time_var')
709 |         value_var = variables[0]
710 |         period = options.get('period', 12)
711 |
712 |         # Prepare time series
713 |         if time_var:
714 |             df = data.set_index(time_var)
715 |         else:
716 |             df = data
717 |
718 |         ts = df[value_var].dropna()
719 |
720 |         results = {
721 |             'variable': value_var,
722 |             'n_observations': len(ts)
723 |         }
724 |
725 |         # Stationarity test (ADF)
726 |         adf_result = adfuller(ts, autolag='AIC')
727 |         results['adf_test'] = {
728 |             'statistic': float(adf_result[0]),
729 |             'p_value': float(adf_result[1]),
730 |             'critical_values': adf_result[4],
731 |             'is_stationary': adf_result[1] < 0.05
732 |         }
733 |
734 |         # Decomposition
735 |         if len(ts) >= 2 * period:
736 |             decomposition = seasonal_decompose(ts, model='additive', period=period)
737 |
738 |             results['decomposition'] = {
739 |                 'trend': decomposition.trend.dropna().head(50).tolist(),
740 |                 'seasonal': decomposition.seasonal.dropna().head(50).tolist(),
741 |                 'residual': decomposition.resid.dropna().head(50).tolist()
742 |             }
743 |
744 |         # ACF and PACF
745 |         nlags = min(40, len(ts) // 4)
746 |         acf_values = acf(ts, nlags=nlags)
747 |         pacf_values = pacf(ts, nlags=nlags)
748 |
749 |         results['autocorrelation'] = {
750 |             'acf': acf_values.tolist(),
751 |             'pacf': pacf_values.tolist(),
752 |             'nlags': nlags
753 |         }
754 |
755 |         return results
756 |
757 |     def _generate_cache_key(self, analysis_type: str, variables: List[str], options: Dict) -> str:
758 |         """Generate cache key for analysis results"""
759 |         import hashlib
760 |         import json
761 |
762 |         key_data = {
763 |             'type': analysis_type,
764 |             'variables': sorted(variables),
765 |             'options': options
766 |         }
767 |
768 |         key_str = json.dumps(key_data, sort_keys=True)
769 |         return hashlib.md5(key_str.encode()).hexdigest()
770 |
771 |     async def export_results(self, results: Dict[str, Any], format: str = 'json') -> bytes:
772 |         """Export analysis results"""
773 |         import json
774 |         import io
775 |
776 |         if format == 'json':
777 |             return json.dumps(results, indent=2).encode('utf-8')
778 |
779 |         elif format == 'html':
780 |             html = self._results_to_html(results)
781 |             return html.encode('utf-8')
782 |
783 |         elif format == 'pdf':
784 |             # Would need reportlab or similar
785 |             pass
786 |
787 |         else:
788 |             raise ValueError(f"Unsupported export format: {format}")
789 |
790 |     def _results_to_html(self, results: Dict[str, Any]) -> str:
791 |         """Convert results to HTML format"""
792 |         html = """
793 |         <!DOCTYPE html>
794 |         <html>
795 |         <head>
796 |             <title>OpenStatica Analysis Results</title>
797 |             <style>
798 |                 body { font-family: Arial, sans-serif; margin: 20px; }
799 |                 h1 { color: #6366f1; }
800 |                 table { border-collapse: collapse; width: 100%; margin: 20px 0; }
801 |                 th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
802 |                 th { background-color: #f2f2f2; }
803 |             </style>
804 |         </head>
805 |         <body>
806 |             <h1>Analysis Results</h1>
807 |         """
808 |
809 |         html += self._dict_to_html_table(results)
810 |         html += "</body></html>"
811 |
812 |         return html
813 |
814 |     def _dict_to_html_table(self, d: Dict, level: int = 0) -> str:
815 |         """Convert dictionary to HTML table"""
816 |         html = "<table>"
817 |
818 |         for key, value in d.items():
819 |             html += f"<tr><td><strong>{key}</strong></td><td>"
820 |
821 |             if isinstance(value, dict):
822 |                 html += self._dict_to_html_table(value, level + 1)
823 |             elif isinstance(value, list):
824 |                 html += ", ".join(str(v) for v in value[:10])
825 |                 if len(value) > 10:
826 |                     html += "..."
827 |             else:
828 |                 html += str(value)
829 |
830 |             html += "</td></tr>"
831 |
832 |         html += "</table>"
833 |         return html
834 |


--------------------------------------------------------------------------------
/backend/app/services/data_service.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Data Service for OpenStatica
  3 | Handles all data operations including loading, transformation, and management
  4 | """
  5 |
  6 | import pandas as pd
  7 | import numpy as np
  8 | import polars as pl
  9 | from typing import Dict, Any, List, Optional, Union, Tuple
 10 | import io
 11 | import json
 12 | from pathlib import Path
 13 | import asyncio
 14 | from datetime import datetime
 15 | import hashlib
 16 | from scipy import stats
 17 | import logging
 18 |
 19 | logger = logging.getLogger(__name__)
 20 |
 21 |
 22 | class DataService:
 23 |     """Service for data operations"""
 24 |
 25 |     def __init__(self):
 26 |         self.supported_formats = {
 27 |             'csv': self.read_csv,
 28 |             'excel': self.read_excel,
 29 |             'json': self.read_json,
 30 |             'parquet': self.read_parquet,
 31 |             'stata': self.read_stata,
 32 |             'spss': self.read_spss,
 33 |             'sas': self.read_sas
 34 |         }
 35 |
 36 |     async def load_data(self, file_content: bytes, file_type: str, **kwargs) -> pd.DataFrame:
 37 |         """Load data from various file formats"""
 38 |         if file_type not in self.supported_formats:
 39 |             raise ValueError(f"Unsupported file type: {file_type}")
 40 |
 41 |         reader = self.supported_formats[file_type]
 42 |         return await reader(file_content, **kwargs)
 43 |
 44 |     async def read_csv(self, content: bytes, **kwargs) -> pd.DataFrame:
 45 |         """Read CSV file"""
 46 |         try:
 47 |             # Try to detect encoding
 48 |             encoding = kwargs.get('encoding', 'utf-8')
 49 |             delimiter = kwargs.get('delimiter', ',')
 50 |
 51 |             # Use StringIO for pandas
 52 |             text_content = content.decode(encoding)
 53 |             df = pd.read_csv(
 54 |                 io.StringIO(text_content),
 55 |                 sep=delimiter,
 56 |                 **{k: v for k, v in kwargs.items() if k not in ['encoding', 'delimiter']}
 57 |             )
 58 |
 59 |             return self.clean_dataframe(df)
 60 |
 61 |         except Exception as e:
 62 |             logger.error(f"Error reading CSV: {e}")
 63 |             raise
 64 |
 65 |     async def read_excel(self, content: bytes, **kwargs) -> pd.DataFrame:
 66 |         """Read Excel file"""
 67 |         try:
 68 |             sheet_name = kwargs.get('sheet_name', 0)
 69 |             df = pd.read_excel(
 70 |                 io.BytesIO(content),
 71 |                 sheet_name=sheet_name,
 72 |                 **{k: v for k, v in kwargs.items() if k != 'sheet_name'}
 73 |             )
 74 |
 75 |             return self.clean_dataframe(df)
 76 |
 77 |         except Exception as e:
 78 |             logger.error(f"Error reading Excel: {e}")
 79 |             raise
 80 |
 81 |     async def read_json(self, content: bytes, **kwargs) -> pd.DataFrame:
 82 |         """Read JSON file"""
 83 |         try:
 84 |             text_content = content.decode('utf-8')
 85 |             df = pd.read_json(io.StringIO(text_content), **kwargs)
 86 |             return self.clean_dataframe(df)
 87 |
 88 |         except Exception as e:
 89 |             logger.error(f"Error reading JSON: {e}")
 90 |             raise
 91 |
 92 |     async def read_parquet(self, content: bytes, **kwargs) -> pd.DataFrame:
 93 |         """Read Parquet file"""
 94 |         try:
 95 |             df = pd.read_parquet(io.BytesIO(content), **kwargs)
 96 |             return self.clean_dataframe(df)
 97 |
 98 |         except Exception as e:
 99 |             logger.error(f"Error reading Parquet: {e}")
100 |             raise
101 |
102 |     async def read_stata(self, content: bytes, **kwargs) -> pd.DataFrame:
103 |         """Read Stata file"""
104 |         try:
105 |             df = pd.read_stata(io.BytesIO(content), **kwargs)
106 |             return self.clean_dataframe(df)
107 |         except Exception as e:
108 |             logger.error(f"Error reading Stata: {e}")
109 |             raise
110 |
111 |     async def read_spss(self, content: bytes, **kwargs) -> pd.DataFrame:
112 |         """Read SPSS file"""
113 |         try:
114 |             # Use pyreadstat for SPSS files
115 |             import pyreadstat
116 |             df, meta = pyreadstat.read_sav(io.BytesIO(content))
117 |
118 |             # Store metadata
119 |             df.attrs['spss_metadata'] = meta
120 |
121 |             return self.clean_dataframe(df)
122 |         except ImportError:
123 |             logger.error("pyreadstat not installed. Install with: pip install pyreadstat")
124 |             raise
125 |         except Exception as e:
126 |             logger.error(f"Error reading SPSS: {e}")
127 |             raise
128 |
129 |     async def read_sas(self, content: bytes, **kwargs) -> pd.DataFrame:
130 |         """Read SAS file"""
131 |         try:
132 |             df = pd.read_sas(io.BytesIO(content), **kwargs)
133 |             return self.clean_dataframe(df)
134 |         except Exception as e:
135 |             logger.error(f"Error reading SAS: {e}")
136 |             raise
137 |
138 |     def clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
139 |         """Clean and standardize dataframe"""
140 |         # Remove unnamed columns
141 |         df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
142 |
143 |         # Strip whitespace from string columns
144 |         string_columns = df.select_dtypes(include=['object']).columns
145 |         for col in string_columns:
146 |             df[col] = df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)
147 |
148 |         # Convert obvious numeric columns
149 |         for col in df.columns:
150 |             try:
151 |                 df[col] = pd.to_numeric(df[col], errors='ignore')
152 |             except:
153 |                 pass
154 |
155 |         return df
156 |
157 |     async def analyze_data_types(self, df: pd.DataFrame) -> Dict[str, Any]:
158 |         """Analyze and categorize data types"""
159 |         analysis = {
160 |             'numeric': [],
161 |             'categorical': [],
162 |             'datetime': [],
163 |             'text': [],
164 |             'binary': [],
165 |             'ordinal': []
166 |         }
167 |
168 |         for col in df.columns:
169 |             dtype = df[col].dtype
170 |             unique_ratio = df[col].nunique() / len(df)
171 |
172 |             if pd.api.types.is_numeric_dtype(dtype):
173 |                 if df[col].nunique() == 2:
174 |                     analysis['binary'].append(col)
175 |                 elif unique_ratio < 0.05 and df[col].nunique() < 20:
176 |                     analysis['ordinal'].append(col)
177 |                 else:
178 |                     analysis['numeric'].append(col)
179 |
180 |             elif pd.api.types.is_datetime64_any_dtype(dtype):
181 |                 analysis['datetime'].append(col)
182 |
183 |             else:
184 |                 # String/object type
185 |                 if df[col].nunique() == 2:
186 |                     analysis['binary'].append(col)
187 |                 elif unique_ratio < 0.5:
188 |                     analysis['categorical'].append(col)
189 |                 else:
190 |                     analysis['text'].append(col)
191 |
192 |         return analysis
193 |
194 |     async def get_data_quality_report(self, df: pd.DataFrame) -> Dict[str, Any]:
195 |         """Generate comprehensive data quality report"""
196 |         report = {
197 |             'completeness': {},
198 |             'validity': {},
199 |             'consistency': {},
200 |             'uniqueness': {},
201 |             'overall_score': 0
202 |         }
203 |
204 |         # Completeness
205 |         missing_counts = df.isnull().sum()
206 |         total_cells = len(df) * len(df.columns)
207 |         missing_cells = missing_counts.sum()
208 |
209 |         report['completeness'] = {
210 |             'missing_by_column': missing_counts.to_dict(),
211 |             'total_missing': int(missing_cells),
212 |             'completeness_rate': float(1 - missing_cells / total_cells),
213 |             'columns_with_missing': list(missing_counts[missing_counts > 0].index)
214 |         }
215 |
216 |         # Validity checks
217 |         validity_issues = []
218 |         for col in df.select_dtypes(include=[np.number]).columns:
219 |             # Check for outliers using IQR method
220 |             Q1 = df[col].quantile(0.25)
221 |             Q3 = df[col].quantile(0.75)
222 |             IQR = Q3 - Q1
223 |             outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
224 |
225 |             if len(outliers) > 0:
226 |                 validity_issues.append({
227 |                     'column': col,
228 |                     'issue': 'outliers',
229 |                     'count': len(outliers),
230 |                     'percentage': len(outliers) / len(df) * 100
231 |                 })
232 |
233 |         report['validity'] = {
234 |             'issues': validity_issues,
235 |             'validity_score': 1 - (len(validity_issues) / len(df.columns))
236 |         }
237 |
238 |         # Uniqueness
239 |         duplicate_rows = df.duplicated().sum()
240 |         report['uniqueness'] = {
241 |             'duplicate_rows': int(duplicate_rows),
242 |             'duplicate_rate': float(duplicate_rows / len(df)),
243 |             'unique_identifier_candidates': []
244 |         }
245 |
246 |         # Find potential unique identifiers
247 |         for col in df.columns:
248 |             if df[col].nunique() == len(df):
249 |                 report['uniqueness']['unique_identifier_candidates'].append(col)
250 |
251 |         # Consistency checks
252 |         consistency_issues = []
253 |
254 |         # Check for mixed data types in string columns
255 |         for col in df.select_dtypes(include=['object']).columns:
256 |             mixed_types = False
257 |             has_numbers = df[col].astype(str).str.match(r'^\d+
#39;).any()
258 |             has_text = df[col].astype(str).str.match(r'^[a-zA-Z]+
#39;).any()
259 |
260 |             if has_numbers and has_text:
261 |                 consistency_issues.append({
262 |                     'column': col,
263 |                     'issue': 'mixed_data_types'
264 |                 })
265 |
266 |         report['consistency'] = {
267 |             'issues': consistency_issues,
268 |             'consistency_score': 1 - (len(consistency_issues) / len(df.columns))
269 |         }
270 |
271 |         # Calculate overall score
272 |         report['overall_score'] = np.mean([
273 |             report['completeness']['completeness_rate'],
274 |             report['validity']['validity_score'],
275 |             1 - report['uniqueness']['duplicate_rate'],
276 |             report['consistency']['consistency_score']
277 |         ]) * 100
278 |
279 |         return report
280 |
281 |     async def transform_data(self, df: pd.DataFrame, transformations: List[Dict]) -> pd.DataFrame:
282 |         """Apply transformations to dataframe"""
283 |         df_transformed = df.copy()
284 |
285 |         for transform in transformations:
286 |             transform_type = transform.get('type')
287 |             columns = transform.get('columns', [])
288 |             options = transform.get('options', {})
289 |
290 |             if transform_type == 'normalize':
291 |                 df_transformed = await self.normalize_columns(df_transformed, columns, **options)
292 |             elif transform_type == 'encode':
293 |                 df_transformed = await self.encode_categorical(df_transformed, columns, **options)
294 |             elif transform_type == 'impute':
295 |                 df_transformed = await self.impute_missing(df_transformed, columns, **options)
296 |             elif transform_type == 'bin':
297 |                 df_transformed = await self.bin_numeric(df_transformed, columns, **options)
298 |             elif transform_type == 'log':
299 |                 df_transformed = await self.log_transform(df_transformed, columns, **options)
300 |             elif transform_type == 'difference':
301 |                 df_transformed = await self.difference_transform(df_transformed, columns, **options)
302 |             elif transform_type == 'interaction':
303 |                 df_transformed = await self.create_interactions(df_transformed, columns, **options)
304 |
305 |         return df_transformed
306 |
307 |     async def normalize_columns(self, df: pd.DataFrame, columns: List[str], method: str = 'zscore') -> pd.DataFrame:
308 |         """Normalize numeric columns"""
309 |         from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
310 |
311 |         df_norm = df.copy()
312 |
313 |         if method == 'zscore':
314 |             scaler = StandardScaler()
315 |         elif method == 'minmax':
316 |             scaler = MinMaxScaler()
317 |         elif method == 'robust':
318 |             scaler = RobustScaler()
319 |         else:
320 |             raise ValueError(f"Unknown normalization method: {method}")
321 |
322 |         for col in columns:
323 |             if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
324 |                 df_norm[col] = scaler.fit_transform(df[[col]])
325 |
326 |         return df_norm
327 |
328 |     async def encode_categorical(self, df: pd.DataFrame, columns: List[str], method: str = 'onehot') -> pd.DataFrame:
329 |         """Encode categorical variables"""
330 |         df_encoded = df.copy()
331 |
332 |         for col in columns:
333 |             if col not in df.columns:
334 |                 continue
335 |
336 |             if method == 'onehot':
337 |                 dummies = pd.get_dummies(df[col], prefix=col)
338 |                 df_encoded = pd.concat([df_encoded.drop(col, axis=1), dummies], axis=1)
339 |             elif method == 'label':
340 |                 from sklearn.preprocessing import LabelEncoder
341 |                 le = LabelEncoder()
342 |                 df_encoded[col] = le.fit_transform(df[col].astype(str))
343 |             elif method == 'ordinal':
344 |                 # User should provide mapping
345 |                 pass
346 |
347 |         return df_encoded
348 |
349 |     async def impute_missing(self, df: pd.DataFrame, columns: List[str], strategy: str = 'mean') -> pd.DataFrame:
350 |         """Impute missing values"""
351 |         df_imputed = df.copy()
352 |
353 |         for col in columns:
354 |             if col not in df.columns:
355 |                 continue
356 |
357 |             if strategy == 'mean' and pd.api.types.is_numeric_dtype(df[col]):
358 |                 df_imputed[col].fillna(df[col].mean(), inplace=True)
359 |             elif strategy == 'median' and pd.api.types.is_numeric_dtype(df[col]):
360 |                 df_imputed[col].fillna(df[col].median(), inplace=True)
361 |             elif strategy == 'mode':
362 |                 df_imputed[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else None, inplace=True)
363 |             elif strategy == 'forward':
364 |                 df_imputed[col].fillna(method='ffill', inplace=True)
365 |             elif strategy == 'backward':
366 |                 df_imputed[col].fillna(method='bfill', inplace=True)
367 |             elif strategy == 'interpolate':
368 |                 df_imputed[col].interpolate(inplace=True)
369 |             elif strategy == 'drop':
370 |                 df_imputed = df_imputed.dropna(subset=[col])
371 |
372 |         return df_imputed
373 |
374 |     async def bin_numeric(self, df: pd.DataFrame, columns: List[str], n_bins: int = 5,
375 |                           strategy: str = 'quantile') -> pd.DataFrame:
376 |         """Bin numeric variables"""
377 |         df_binned = df.copy()
378 |
379 |         for col in columns:
380 |             if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):
381 |                 continue
382 |
383 |             if strategy == 'quantile':
384 |                 df_binned[f'{col}_binned'] = pd.qcut(df[col], n_bins, labels=False, duplicates='drop')
385 |             elif strategy == 'uniform':
386 |                 df_binned[f'{col}_binned'] = pd.cut(df[col], n_bins, labels=False)
387 |
388 |         return df_binned
389 |
390 |     async def log_transform(self, df: pd.DataFrame, columns: List[str], offset: float = 1.0) -> pd.DataFrame:
391 |         """Apply log transformation"""
392 |         df_log = df.copy()
393 |
394 |         for col in columns:
395 |             if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
396 |                 # Add offset to handle zero values
397 |                 df_log[f'{col}_log'] = np.log(df[col] + offset)
398 |
399 |         return df_log
400 |
401 |     async def difference_transform(self, df: pd.DataFrame, columns: List[str], periods: int = 1) -> pd.DataFrame:
402 |         """Apply differencing transformation"""
403 |         df_diff = df.copy()
404 |
405 |         for col in columns:
406 |             if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
407 |                 df_diff[f'{col}_diff'] = df[col].diff(periods)
408 |
409 |         return df_diff
410 |
411 |     async def create_interactions(self, df: pd.DataFrame, columns: List[str], degree: int = 2) -> pd.DataFrame:
412 |         """Create interaction terms"""
413 |         from itertools import combinations
414 |
415 |         df_interact = df.copy()
416 |         numeric_cols = [col for col in columns if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
417 |
418 |         for r in range(2, min(degree + 1, len(numeric_cols) + 1)):
419 |             for combo in combinations(numeric_cols, r):
420 |                 interaction_name = '_x_'.join(combo)
421 |                 df_interact[interaction_name] = df[list(combo)].prod(axis=1)
422 |
423 |         return df_interact
424 |
425 |     async def detect_outliers(self, df: pd.DataFrame, columns: List[str] = None, method: str = 'iqr') -> Dict[str, Any]:
426 |         """Detect outliers in data"""
427 |         if columns is None:
428 |             columns = df.select_dtypes(include=[np.number]).columns.tolist()
429 |
430 |         outliers = {}
431 |
432 |         for col in columns:
433 |             if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):
434 |                 continue
435 |
436 |             if method == 'iqr':
437 |                 Q1 = df[col].quantile(0.25)
438 |                 Q3 = df[col].quantile(0.75)
439 |                 IQR = Q3 - Q1
440 |                 lower_bound = Q1 - 1.5 * IQR
441 |                 upper_bound = Q3 + 1.5 * IQR
442 |                 outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()
443 |
444 |             elif method == 'zscore':
445 |                 z_scores = np.abs(stats.zscore(df[col].dropna()))
446 |                 outlier_indices = df.index[z_scores > 3].tolist()
447 |
448 |             elif method == 'isolation':
449 |                 from sklearn.ensemble import IsolationForest
450 |                 iso_forest = IsolationForest(contamination=0.1)
451 |                 outlier_labels = iso_forest.fit_predict(df[[col]].dropna())
452 |                 outlier_indices = df.index[outlier_labels == -1].tolist()
453 |
454 |             outliers[col] = {
455 |                 'indices': outlier_indices,
456 |                 'count': len(outlier_indices),
457 |                 'percentage': len(outlier_indices) / len(df) * 100
458 |             }
459 |
460 |         return outliers
461 |
462 |     async def generate_summary_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
463 |         """Generate comprehensive summary statistics"""
464 |         summary = {
465 |             'shape': df.shape,
466 |             'columns': df.columns.tolist(),
467 |             'dtypes': df.dtypes.astype(str).to_dict(),
468 |             'numeric_summary': {},
469 |             'categorical_summary': {}
470 |         }
471 |
472 |         # Numeric columns summary
473 |         numeric_cols = df.select_dtypes(include=[np.number]).columns
474 |         if len(numeric_cols) > 0:
475 |             summary['numeric_summary'] = df[numeric_cols].describe().to_dict()
476 |
477 |             # Add additional statistics
478 |             for col in numeric_cols:
479 |                 summary['numeric_summary'][col].update({
480 |                     'skewness': float(df[col].skew()),
481 |                     'kurtosis': float(df[col].kurtosis()),
482 |                     'variance': float(df[col].var()),
483 |                     'sem': float(df[col].sem()),
484 |                     'mad': float(df[col].mad()),
485 |                     'range': float(df[col].max() - df[col].min()),
486 |                     'iqr': float(df[col].quantile(0.75) - df[col].quantile(0.25)),
487 |                     'cv': float(df[col].std() / df[col].mean() * 100) if df[col].mean() != 0 else None
488 |                 })
489 |
490 |         # Categorical columns summary
491 |         categorical_cols = df.select_dtypes(include=['object', 'category']).columns
492 |         for col in categorical_cols:
493 |             value_counts = df[col].value_counts()
494 |             summary['categorical_summary'][col] = {
495 |                 'unique': df[col].nunique(),
496 |                 'top': value_counts.index[0] if len(value_counts) > 0 else None,
497 |                 'freq': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
498 |                 'missing': int(df[col].isnull().sum()),
499 |                 'value_counts': value_counts.head(10).to_dict()
500 |             }
501 |
502 |         return summary
503 |
504 |     async def export_data(self, df: pd.DataFrame, format: str, **kwargs) -> bytes:
505 |         """Export dataframe to various formats"""
506 |         output = io.BytesIO()
507 |
508 |         if format == 'csv':
509 |             csv_string = df.to_csv(**kwargs)
510 |             output.write(csv_string.encode('utf-8'))
511 |
512 |         elif format == 'excel':
513 |             with pd.ExcelWriter(output, engine='openpyxl') as writer:
514 |                 df.to_excel(writer, **kwargs)
515 |
516 |         elif format == 'json':
517 |             json_string = df.to_json(**kwargs)
518 |             output.write(json_string.encode('utf-8'))
519 |
520 |         elif format == 'parquet':
521 |             df.to_parquet(output, **kwargs)
522 |
523 |         elif format == 'html':
524 |             html_string = df.to_html(**kwargs)
525 |             output.write(html_string.encode('utf-8'))
526 |
527 |         elif format == 'latex':
528 |             latex_string = df.to_latex(**kwargs)
529 |             output.write(latex_string.encode('utf-8'))
530 |
531 |         else:
532 |             raise ValueError(f"Unsupported export format: {format}")
533 |
534 |         output.seek(0)
535 |         return output.read()
536 |
537 |     async def merge_datasets(self, df1: pd.DataFrame, df2: pd.DataFrame, **merge_kwargs) -> pd.DataFrame:
538 |         """Merge two datasets"""
539 |         return pd.merge(df1, df2, **merge_kwargs)
540 |
541 |     async def pivot_data(self, df: pd.DataFrame, **pivot_kwargs) -> pd.DataFrame:
542 |         """Pivot dataframe"""
543 |         return df.pivot_table(**pivot_kwargs)
544 |
545 |     async def melt_data(self, df: pd.DataFrame, **melt_kwargs) -> pd.DataFrame:
546 |         """Melt dataframe from wide to long format"""
547 |         return df.melt(**melt_kwargs)
548 |


--------------------------------------------------------------------------------
/backend/app/services/export_service.py:
--------------------------------------------------------------------------------
  1 | import pandas as pd
  2 | import numpy as np
  3 | from typing import Any, Dict, Optional
  4 | import io
  5 | import json
  6 | from datetime import datetime
  7 | import zipfile
  8 | from reportlab.lib import colors
  9 | from reportlab.lib.pagesizes import letter, A4
 10 | from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
 11 | from reportlab.lib.styles import getSampleStyleSheet
 12 | from reportlab.lib.units import inch
 13 |
 14 |
 15 | class ExportService:
 16 |     """Service for exporting data and results"""
 17 |
 18 |     async def export_data(self, data: pd.DataFrame, format: str, **kwargs) -> bytes:
 19 |         """Export data in various formats"""
 20 |         if format == "csv":
 21 |             return self._export_csv(data, **kwargs)
 22 |         elif format == "excel":
 23 |             return self._export_excel(data, **kwargs)
 24 |         elif format == "json":
 25 |             return self._export_json(data, **kwargs)
 26 |         elif format == "parquet":
 27 |             return self._export_parquet(data, **kwargs)
 28 |         elif format == "html":
 29 |             return self._export_html(data, **kwargs)
 30 |         else:
 31 |             raise ValueError(f"Unsupported export format: {format}")
 32 |
 33 |     async def export_results(self, results: Dict[str, Any], format: str, **kwargs) -> bytes:
 34 |         """Export analysis results"""
 35 |         if format == "pdf":
 36 |             return self._export_pdf(results, **kwargs)
 37 |         elif format == "html":
 38 |             return self._export_results_html(results, **kwargs)
 39 |         elif format == "json":
 40 |             return self._export_results_json(results, **kwargs)
 41 |         elif format == "markdown":
 42 |             return self._export_markdown(results, **kwargs)
 43 |         else:
 44 |             raise ValueError(f"Unsupported export format: {format}")
 45 |
 46 |     async def export_model(self, model: Any, format: str, **kwargs) -> bytes:
 47 |         """Export trained model"""
 48 |         if format == "pickle":
 49 |             import pickle
 50 |             buffer = io.BytesIO()
 51 |             pickle.dump(model, buffer)
 52 |             return buffer.getvalue()
 53 |         elif format == "joblib":
 54 |             import joblib
 55 |             buffer = io.BytesIO()
 56 |             joblib.dump(model, buffer)
 57 |             return buffer.getvalue()
 58 |         elif format == "onnx":
 59 |             # Convert to ONNX format
 60 |             return self._export_onnx(model, **kwargs)
 61 |         else:
 62 |             raise ValueError(f"Unsupported model export format: {format}")
 63 |
 64 |     def _export_csv(self, data: pd.DataFrame, **kwargs) -> bytes:
 65 |         """Export to CSV"""
 66 |         buffer = io.StringIO()
 67 |         data.to_csv(buffer, index=kwargs.get('index', False))
 68 |         return buffer.getvalue().encode('utf-8')
 69 |
 70 |     def _export_excel(self, data: pd.DataFrame, **kwargs) -> bytes:
 71 |         """Export to Excel"""
 72 |         buffer = io.BytesIO()
 73 |         with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
 74 |             data.to_excel(writer, sheet_name='Data', index=kwargs.get('index', False))
 75 |
 76 |             # Add metadata sheet if provided
 77 |             if 'metadata' in kwargs:
 78 |                 metadata_df = pd.DataFrame(kwargs['metadata'].items(), columns=['Key', 'Value'])
 79 |                 metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
 80 |
 81 |         return buffer.getvalue()
 82 |
 83 |     def _export_json(self, data: pd.DataFrame, **kwargs) -> bytes:
 84 |         """Export to JSON"""
 85 |         orient = kwargs.get('orient', 'records')
 86 |         json_str = data.to_json(orient=orient, indent=2)
 87 |         return json_str.encode('utf-8')
 88 |
 89 |     def _export_parquet(self, data: pd.DataFrame, **kwargs) -> bytes:
 90 |         """Export to Parquet"""
 91 |         buffer = io.BytesIO()
 92 |         data.to_parquet(buffer, index=kwargs.get('index', False))
 93 |         return buffer.getvalue()
 94 |
 95 |     def _export_html(self, data: pd.DataFrame, **kwargs) -> bytes:
 96 |         """Export to HTML"""
 97 |         html = f"""
 98 |         <!DOCTYPE html>
 99 |         <html>
100 |         <head>
101 |             <title>OpenStatica Data Export</title>
102 |             <style>
103 |                 body {{ font-family: Arial, sans-serif; margin: 20px; }}
104 |                 h1 {{ color: #6366f1; }}
105 |                 table {{ border-collapse: collapse; width: 100%; }}
106 |                 th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
107 |                 th {{ background-color: #6366f1; color: white; }}
108 |                 tr:nth-child(even) {{ background-color: #f2f2f2; }}
109 |             </style>
110 |         </head>
111 |         <body>
112 |             <h1>Data Export</h1>
113 |             <p>Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
114 |             {data.to_html(index=kwargs.get('index', False), classes='data-table')}
115 |         </body>
116 |         </html>
117 |         """
118 |         return html.encode('utf-8')
119 |
120 |     def _export_pdf(self, results: Dict[str, Any], **kwargs) -> bytes:
121 |         """Export results to PDF"""
122 |         buffer = io.BytesIO()
123 |         doc = SimpleDocTemplate(buffer, pagesize=letter)
124 |         story = []
125 |         styles = getSampleStyleSheet()
126 |
127 |         # Title
128 |         title = Paragraph("OpenStatica Analysis Report", styles['Title'])
129 |         story.append(title)
130 |         story.append(Spacer(1, 12))
131 |
132 |         # Metadata
133 |         metadata = Paragraph(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", styles['Normal'])
134 |         story.append(metadata)
135 |         story.append(Spacer(1, 12))
136 |
137 |         # Results
138 |         for key, value in results.items():
139 |             section_title = Paragraph(key.replace('_', ' ').title(), styles['Heading2'])
140 |             story.append(section_title)
141 |
142 |             if isinstance(value, dict):
143 |                 # Create table for dictionary data
144 |                 table_data = [[k, str(v)] for k, v in value.items()]
145 |                 table = Table(table_data)
146 |                 table.setStyle(TableStyle([
147 |                     ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
148 |                     ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
149 |                     ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
150 |                     ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
151 |                     ('FONTSIZE', (0, 0), (-1, 0), 14),
152 |                     ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
153 |                     ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
154 |                     ('GRID', (0, 0), (-1, -1), 1, colors.black)
155 |                 ]))
156 |                 story.append(table)
157 |             else:
158 |                 content = Paragraph(str(value), styles['Normal'])
159 |                 story.append(content)
160 |
161 |             story.append(Spacer(1, 12))
162 |
163 |         doc.build(story)
164 |         return buffer.getvalue()
165 |
166 |     def _export_results_html(self, results: Dict[str, Any], **kwargs) -> bytes:
167 |         """Export results to HTML"""
168 |         html = f"""
169 |         <!DOCTYPE html>
170 |         <html>
171 |         <head>
172 |             <title>OpenStatica Analysis Results</title>
173 |             <style>
174 |                 body {{
175 |                     font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
176 |                     margin: 40px;
177 |                     background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
178 |                     min-height: 100vh;
179 |                 }}
180 |                 .container {{
181 |                     background: white;
182 |                     border-radius: 12px;
183 |                     padding: 30px;
184 |                     box-shadow: 0 10px 30px rgba(0,0,0,0.2);
185 |                 }}
186 |                 h1 {{
187 |                     color: #6366f1;
188 |                     border-bottom: 3px solid #6366f1;
189 |                     padding-bottom: 10px;
190 |                 }}
191 |                 h2 {{
192 |                     color: #4f46e5;
193 |                     margin-top: 30px;
194 |                 }}
195 |                 .result-section {{
196 |                     background: #f8f9fa;
197 |                     padding: 20px;
198 |                     border-radius: 8px;
199 |                     margin: 20px 0;
200 |                 }}
201 |                 table {{
202 |                     border-collapse: collapse;
203 |                     width: 100%;
204 |                     margin: 15px 0;
205 |                 }}
206 |                 th, td {{
207 |                     border: 1px solid #e5e7eb;
208 |                     padding: 12px;
209 |                     text-align: left;
210 |                 }}
211 |                 th {{
212 |                     background-color: #6366f1;
213 |                     color: white;
214 |                     font-weight: 600;
215 |                 }}
216 |                 tr:nth-child(even) {{
217 |                     background-color: #f3f4f6;
218 |                 }}
219 |                 .metadata {{
220 |                     color: #6b7280;
221 |                     font-size: 14px;
222 |                     margin-bottom: 20px;
223 |                 }}
224 |             </style>
225 |         </head>
226 |         <body>
227 |             <div class="container">
228 |                 <h1>OpenStatica Analysis Results</h1>
229 |                 <div class="metadata">
230 |                     Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
231 |                 </div>
232 |         """
233 |
234 |         for key, value in results.items():
235 |             html += f"""
236 |                 <div class="result-section">
237 |                     <h2>{key.replace('_', ' ').title()}</h2>
238 |                     {self._format_value_html(value)}
239 |                 </div>
240 |             """
241 |
242 |         html += """
243 |             </div>
244 |         </body>
245 |         </html>
246 |         """
247 |         return html.encode('utf-8')
248 |
249 |     def _format_value_html(self, value: Any) -> str:
250 |         """Format value for HTML output"""
251 |         if isinstance(value, dict):
252 |             html = "<table>"
253 |             for k, v in value.items():
254 |                 html += f"<tr><td><strong>{k}</strong></td><td>{v}</td></tr>"
255 |             html += "</table>"
256 |             return html
257 |         elif isinstance(value, list):
258 |             return "<ul>" + "".join(f"<li>{item}</li>" for item in value) + "</ul>"
259 |         else:
260 |             return f"<p>{value}</p>"
261 |
262 |     def _export_results_json(self, results: Dict[str, Any], **kwargs) -> bytes:
263 |         """Export results to JSON"""
264 |
265 |         # Convert numpy arrays and other non-serializable types
266 |         def convert(obj):
267 |             if isinstance(obj, np.ndarray):
268 |                 return obj.tolist()
269 |             elif isinstance(obj, pd.DataFrame):
270 |                 return obj.to_dict(orient='records')
271 |             elif isinstance(obj, pd.Series):
272 |                 return obj.tolist()
273 |             elif hasattr(obj, '__dict__'):
274 |                 return obj.__dict__
275 |             return obj
276 |
277 |         json_str = json.dumps(results, default=convert, indent=2)
278 |         return json_str.encode('utf-8')
279 |
280 |     def _export_markdown(self, results: Dict[str, Any], **kwargs) -> bytes:
281 |         """Export results to Markdown"""
282 |         md = f"""# OpenStatica Analysis Results
283 |
284 | Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
285 |
286 | ---
287 |
288 | """
289 |
290 |         for key, value in results.items():
291 |             md += f"## {key.replace('_', ' ').title()}\n\n"
292 |             md += self._format_value_markdown(value)
293 |             md += "\n---\n\n"
294 |
295 |         return md.encode('utf-8')
296 |
297 |     def _format_value_markdown(self, value: Any) -> str:
298 |         """Format value for Markdown output"""
299 |         if isinstance(value, dict):
300 |             md = "| Key | Value |\n|-----|-------|\n"
301 |             for k, v in value.items():
302 |                 md += f"| {k} | {v} |\n"
303 |             return md
304 |         elif isinstance(value, list):
305 |             return "\n".join(f"- {item}" for item in value) + "\n"
306 |         else:
307 |             return f"{value}\n"
308 |
309 |     def _export_onnx(self, model: Any, **kwargs) -> bytes:
310 |         """Export model to ONNX format"""
311 |         try:
312 |             import torch
313 |             import onnx
314 |
315 |             # Convert model to ONNX
316 |             dummy_input = kwargs.get('dummy_input')
317 |             if dummy_input is None:
318 |                 raise ValueError("dummy_input required for ONNX export")
319 |
320 |             buffer = io.BytesIO()
321 |             torch.onnx.export(
322 |                 model,
323 |                 dummy_input,
324 |                 buffer,
325 |                 export_params=True,
326 |                 opset_version=11,
327 |                 do_constant_folding=True,
328 |                 input_names=['input'],
329 |                 output_names=['output'],
330 |                 dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
331 |             )
332 |
333 |             return buffer.getvalue()
334 |
335 |         except ImportError:
336 |             raise ImportError("ONNX export requires torch and onnx packages")
337 |


--------------------------------------------------------------------------------
/backend/app/services/ml_service.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Machine Learning Service for OpenStatica
  3 | Manages ML model training, evaluation, and deployment
  4 | """
  5 |
  6 | import pandas as pd
  7 | import numpy as np
  8 | from typing import Dict, Any, List, Optional, Tuple, Union
  9 | from sklearn.model_selection import (
 10 |     train_test_split, cross_val_score, GridSearchCV,
 11 |     RandomizedSearchCV, StratifiedKFold, KFold
 12 | )
 13 | from sklearn.preprocessing import StandardScaler, LabelEncoder
 14 | from sklearn.metrics import (
 15 |     accuracy_score, precision_score, recall_score, f1_score,
 16 |     roc_auc_score, confusion_matrix, classification_report,
 17 |     mean_squared_error, mean_absolute_error, r2_score,
 18 |     silhouette_score, calinski_harabasz_score, davies_bouldin_score
 19 | )
 20 | from sklearn.feature_selection import (
 21 |     SelectKBest, f_classif, f_regression,
 22 |     RFE, RFECV, SelectFromModel
 23 | )
 24 | import joblib
 25 | import pickle
 26 | import asyncio
 27 | import logging
 28 | from datetime import datetime
 29 | import hashlib
 30 | import json
 31 | from pathlib import Path
 32 |
 33 | logger = logging.getLogger(__name__)
 34 |
 35 |
 36 | class MLService:
 37 |     """Service for machine learning operations"""
 38 |
 39 |     def __init__(self, model_cache_path: str = "./models"):
 40 |         self.model_cache_path = Path(model_cache_path)
 41 |         self.model_cache_path.mkdir(exist_ok=True)
 42 |         self.trained_models = {}
 43 |         self.model_metadata = {}
 44 |
 45 |     async def train_model(self,
 46 |                           data: pd.DataFrame,
 47 |                           task: str,
 48 |                           algorithm: str,
 49 |                           features: List[str],
 50 |                           target: Optional[str] = None,
 51 |                           **kwargs) -> Dict[str, Any]:
 52 |         """Train a machine learning model"""
 53 |
 54 |         # Prepare data
 55 |         X, y = await self._prepare_data(data, features, target, task)
 56 |
 57 |         # Split data if supervised
 58 |         if task in ['classification', 'regression']:
 59 |             X_train, X_test, y_train, y_test = train_test_split(
 60 |                 X, y,
 61 |                 test_size=1 - kwargs.get('train_test_split', 0.8),
 62 |                 random_state=kwargs.get('random_state', 42),
 63 |                 stratify=y if task == 'classification' else None
 64 |             )
 65 |         else:
 66 |             X_train, X_test = X, None
 67 |             y_train, y_test = None, None
 68 |
 69 |         # Get model
 70 |         model = await self._get_model(task, algorithm)
 71 |
 72 |         # Hyperparameter tuning if requested
 73 |         if kwargs.get('auto_ml', False):
 74 |             model = await self._tune_hyperparameters(
 75 |                 model, X_train, y_train, task,
 76 |                 cv_folds=kwargs.get('cv_folds', 5)
 77 |             )
 78 |
 79 |         # Train model
 80 |         if task in ['classification', 'regression']:
 81 |             model.fit(X_train, y_train)
 82 |         else:
 83 |             model.fit(X_train)
 84 |
 85 |         # Generate model ID
 86 |         model_id = self._generate_model_id(task, algorithm)
 87 |
 88 |         # Store model
 89 |         self.trained_models[model_id] = model
 90 |
 91 |         # Evaluate model
 92 |         evaluation = await self._evaluate_model(
 93 |             model, X_test, y_test, task,
 94 |             cross_validate=kwargs.get('cross_validate', True)
 95 |         )
 96 |
 97 |         # Get feature importance
 98 |         feature_importance = await self._get_feature_importance(model, features)
 99 |
100 |         # Store metadata
101 |         self.model_metadata[model_id] = {
102 |             'task': task,
103 |             'algorithm': algorithm,
104 |             'features': features,
105 |             'target': target,
106 |             'trained_at': datetime.now().isoformat(),
107 |             'training_size': len(X_train) if X_train is not None else len(X),
108 |             'parameters': model.get_params() if hasattr(model, 'get_params') else {}
109 |         }
110 |
111 |         # Save model to disk
112 |         await self._save_model(model, model_id)
113 |
114 |         return {
115 |             'model_id': model_id,
116 |             'task': task,
117 |             'algorithm': algorithm,
118 |             'metrics': evaluation.get('metrics', {}),
119 |             'feature_importance': feature_importance,
120 |             'confusion_matrix': evaluation.get('confusion_matrix'),
121 |             'training_time': evaluation.get('training_time', 0),
122 |             'cv_scores': evaluation.get('cv_scores')
123 |         }
124 |
125 |     async def _prepare_data(self,
126 |                             data: pd.DataFrame,
127 |                             features: List[str],
128 |                             target: Optional[str],
129 |                             task: str) -> Tuple[pd.DataFrame, Optional[pd.Series]]:
130 |         """Prepare data for training"""
131 |
132 |         # Select features
133 |         X = data[features].copy()
134 |
135 |         # Handle categorical variables
136 |         categorical_cols = X.select_dtypes(include=['object', 'category']).columns
137 |         for col in categorical_cols:
138 |             # Use label encoding for now (could be improved with one-hot encoding)
139 |             le = LabelEncoder()
140 |             X[col] = le.fit_transform(X[col].astype(str))
141 |
142 |         # Handle missing values
143 |         X = X.fillna(X.mean())
144 |
145 |         # Get target if supervised
146 |         y = None
147 |         if target and task in ['classification', 'regression']:
148 |             y = data[target].copy()
149 |
150 |             # Encode target if classification
151 |             if task == 'classification' and y.dtype == 'object':
152 |                 le = LabelEncoder()
153 |                 y = le.fit_transform(y)
154 |
155 |         return X, y
156 |
157 |     async def _get_model(self, task: str, algorithm: str):
158 |         """Get model instance based on task and algorithm"""
159 |
160 |         if task == 'classification':
161 |             if algorithm == 'logistic':
162 |                 from sklearn.linear_model import LogisticRegression
163 |                 return LogisticRegression(max_iter=1000)
164 |             elif algorithm == 'svm':
165 |                 from sklearn.svm import SVC
166 |                 return SVC(probability=True)
167 |             elif algorithm == 'rf':
168 |                 from sklearn.ensemble import RandomForestClassifier
169 |                 return RandomForestClassifier(n_estimators=100, random_state=42)
170 |             elif algorithm == 'xgboost':
171 |                 try:
172 |                     import xgboost as xgb
173 |                     return xgb.XGBClassifier(random_state=42)
174 |                 except ImportError:
175 |                     from sklearn.ensemble import GradientBoostingClassifier
176 |                     return GradientBoostingClassifier(random_state=42)
177 |             elif algorithm == 'nn':
178 |                 from sklearn.neural_network import MLPClassifier
179 |                 return MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)
180 |             elif algorithm == 'knn':
181 |                 from sklearn.neighbors import KNeighborsClassifier
182 |                 return KNeighborsClassifier()
183 |             elif algorithm == 'nb':
184 |                 from sklearn.naive_bayes import GaussianNB
185 |                 return GaussianNB()
186 |             elif algorithm == 'dt':
187 |                 from sklearn.tree import DecisionTreeClassifier
188 |                 return DecisionTreeClassifier(random_state=42)
189 |
190 |         elif task == 'regression':
191 |             if algorithm == 'linear':
192 |                 from sklearn.linear_model import LinearRegression
193 |                 return LinearRegression()
194 |             elif algorithm == 'ridge':
195 |                 from sklearn.linear_model import Ridge
196 |                 return Ridge()
197 |             elif algorithm == 'lasso':
198 |                 from sklearn.linear_model import Lasso
199 |                 return Lasso()
200 |             elif algorithm == 'elastic':
201 |                 from sklearn.linear_model import ElasticNet
202 |                 return ElasticNet()
203 |             elif algorithm == 'svr':
204 |                 from sklearn.svm import SVR
205 |                 return SVR()
206 |             elif algorithm == 'rf_reg':
207 |                 from sklearn.ensemble import RandomForestRegressor
208 |                 return RandomForestRegressor(n_estimators=100, random_state=42)
209 |             elif algorithm == 'xgb_reg':
210 |                 try:
211 |                     import xgboost as xgb
212 |                     return xgb.XGBRegressor(random_state=42)
213 |                 except ImportError:
214 |                     from sklearn.ensemble import GradientBoostingRegressor
215 |                     return GradientBoostingRegressor(random_state=42)
216 |             elif algorithm == 'nn_reg':
217 |                 from sklearn.neural_network import MLPRegressor
218 |                 return MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)
219 |
220 |         elif task == 'clustering':
221 |             if algorithm == 'kmeans':
222 |                 from sklearn.cluster import KMeans
223 |                 return KMeans(n_clusters=3, random_state=42)
224 |             elif algorithm == 'dbscan':
225 |                 from sklearn.cluster import DBSCAN
226 |                 return DBSCAN(eps=0.5, min_samples=5)
227 |             elif algorithm == 'hierarchical':
228 |                 from sklearn.cluster import AgglomerativeClustering
229 |                 return AgglomerativeClustering(n_clusters=3)
230 |             elif algorithm == 'gaussian':
231 |                 from sklearn.mixture import GaussianMixture
232 |                 return GaussianMixture(n_components=3, random_state=42)
233 |             elif algorithm == 'meanshift':
234 |                 from sklearn.cluster import MeanShift
235 |                 return MeanShift()
236 |             elif algorithm == 'spectral':
237 |                 from sklearn.cluster import SpectralClustering
238 |                 return SpectralClustering(n_clusters=3, random_state=42)
239 |
240 |         elif task == 'dimensionality':
241 |             if algorithm == 'pca':
242 |                 from sklearn.decomposition import PCA
243 |                 return PCA(n_components=2)
244 |             elif algorithm == 'tsne':
245 |                 from sklearn.manifold import TSNE
246 |                 return TSNE(n_components=2, random_state=42)
247 |             elif algorithm == 'umap':
248 |                 try:
249 |                     import umap
250 |                     return umap.UMAP(n_components=2, random_state=42)
251 |                 except ImportError:
252 |                     from sklearn.decomposition import PCA
253 |                     return PCA(n_components=2)
254 |             elif algorithm == 'lda':
255 |                 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
256 |                 return LinearDiscriminantAnalysis()
257 |             elif algorithm == 'ica':
258 |                 from sklearn.decomposition import FastICA
259 |                 return FastICA(n_components=2, random_state=42)
260 |
261 |         elif task == 'anomaly':
262 |             if algorithm == 'isolation':
263 |                 from sklearn.ensemble import IsolationForest
264 |                 return IsolationForest(random_state=42)
265 |             elif algorithm == 'lof':
266 |                 from sklearn.neighbors import LocalOutlierFactor
267 |                 return LocalOutlierFactor(novelty=True)
268 |             elif algorithm == 'ocsvm':
269 |                 from sklearn.svm import OneClassSVM
270 |                 return OneClassSVM()
271 |             elif algorithm == 'elliptic':
272 |                 from sklearn.covariance import EllipticEnvelope
273 |                 return EllipticEnvelope(random_state=42)
274 |
275 |         raise ValueError(f"Unknown task/algorithm combination: {task}/{algorithm}")
276 |
277 |     async def _tune_hyperparameters(self,
278 |                                     model,
279 |                                     X_train: pd.DataFrame,
280 |                                     y_train: Optional[pd.Series],
281 |                                     task: str,
282 |                                     cv_folds: int = 5) -> Any:
283 |         """Tune model hyperparameters"""
284 |
285 |         # Define parameter grids for different models
286 |         param_grids = {
287 |             'LogisticRegression': {
288 |                 'C': [0.01, 0.1, 1, 10],
289 |                 'penalty': ['l1', 'l2'],
290 |                 'solver': ['liblinear', 'saga']
291 |             },
292 |             'RandomForestClassifier': {
293 |                 'n_estimators': [50, 100, 200],
294 |                 'max_depth': [None, 10, 20],
295 |                 'min_samples_split': [2, 5, 10]
296 |             },
297 |             'RandomForestRegressor': {
298 |                 'n_estimators': [50, 100, 200],
299 |                 'max_depth': [None, 10, 20],
300 |                 'min_samples_split': [2, 5, 10]
301 |             },
302 |             'SVC': {
303 |                 'C': [0.1, 1, 10],
304 |                 'kernel': ['linear', 'rbf', 'poly'],
305 |                 'gamma': ['scale', 'auto']
306 |             },
307 |             'KMeans': {
308 |                 'n_clusters': [2, 3, 4, 5, 6],
309 |                 'init': ['k-means++', 'random']
310 |             }
311 |         }
312 |
313 |         model_name = type(model).__name__
314 |
315 |         if model_name in param_grids:
316 |             if task in ['classification', 'regression']:
317 |                 # Use GridSearchCV for supervised learning
318 |                 grid_search = GridSearchCV(
319 |                     model,
320 |                     param_grids[model_name],
321 |                     cv=cv_folds,
322 |                     scoring='accuracy' if task == 'classification' else 'neg_mean_squared_error',
323 |                     n_jobs=-1
324 |                 )
325 |                 grid_search.fit(X_train, y_train)
326 |                 return grid_search.best_estimator_
327 |             else:
328 |                 # For unsupervised, we might need different approach
329 |                 # For now, return the original model
330 |                 return model
331 |
332 |         return model
333 |
334 |     async def _evaluate_model(self,
335 |                               model,
336 |                               X_test: Optional[pd.DataFrame],
337 |                               y_test: Optional[pd.Series],
338 |                               task: str,
339 |                               cross_validate: bool = True) -> Dict[str, Any]:
340 |         """Evaluate model performance"""
341 |         import time
342 |
343 |         start_time = time.time()
344 |         evaluation = {'task': task}
345 |
346 |         if task == 'classification' and X_test is not None:
347 |             y_pred = model.predict(X_test)
348 |
349 |             evaluation['metrics'] = {
350 |                 'accuracy': float(accuracy_score(y_test, y_pred)),
351 |                 'precision': float(precision_score(y_test, y_pred, average='weighted')),
352 |                 'recall': float(recall_score(y_test, y_pred, average='weighted')),
353 |                 'f1_score': float(f1_score(y_test, y_pred, average='weighted'))
354 |             }
355 |
356 |             # Add AUC-ROC for binary classification
357 |             if len(np.unique(y_test)) == 2 and hasattr(model, 'predict_proba'):
358 |                 y_proba = model.predict_proba(X_test)[:, 1]
359 |                 evaluation['metrics']['auc_roc'] = float(roc_auc_score(y_test, y_proba))
360 |
361 |             evaluation['confusion_matrix'] = confusion_matrix(y_test, y_pred).tolist()
362 |
363 |         elif task == 'regression' and X_test is not None:
364 |             y_pred = model.predict(X_test)
365 |
366 |             evaluation['metrics'] = {
367 |                 'mse': float(mean_squared_error(y_test, y_pred)),
368 |                 'rmse': float(np.sqrt(mean_squared_error(y_test, y_pred))),
369 |                 'mae': float(mean_absolute_error(y_test, y_pred)),
370 |                 'r2': float(r2_score(y_test, y_pred))
371 |             }
372 |
373 |         elif task == 'clustering':
374 |             if hasattr(model, 'labels_'):
375 |                 labels = model.labels_
376 |
377 |                 # Calculate clustering metrics
378 |                 if len(np.unique(labels)) > 1:
379 |                     evaluation['metrics'] = {
380 |                         'silhouette_score': float(silhouette_score(X_test or model.cluster_centers_, labels)),
381 |                         'calinski_harabasz': float(calinski_harabasz_score(X_test or model.cluster_centers_, labels)),
382 |                         'davies_bouldin': float(davies_bouldin_score(X_test or model.cluster_centers_, labels)),
383 |                         'n_clusters': len(np.unique(labels))
384 |                     }
385 |
386 |         evaluation['training_time'] = time.time() - start_time
387 |
388 |         return evaluation
389 |
390 |     async def _get_feature_importance(self,
391 |                                       model,
392 |                                       features: List[str]) -> Optional[Dict[str, float]]:
393 |         """Extract feature importance from model"""
394 |
395 |         importance = None
396 |
397 |         if hasattr(model, 'feature_importances_'):
398 |             # Tree-based models
399 |             importance = dict(zip(features, model.feature_importances_))
400 |         elif hasattr(model, 'coef_'):
401 |             # Linear models
402 |             coef = model.coef_
403 |             if coef.ndim > 1:
404 |                 coef = np.abs(coef[0])
405 |             else:
406 |                 coef = np.abs(coef)
407 |             importance = dict(zip(features, coef))
408 |
409 |         # Normalize importance values
410 |         if importance:
411 |             total = sum(importance.values())
412 |             if total > 0:
413 |                 importance = {k: v / total for k, v in importance.items()}
414 |
415 |         return importance
416 |
417 |     async def predict(self,
418 |                       model_id: str,
419 |                       data: Union[pd.DataFrame, Dict, List]) -> np.ndarray:
420 |         """Make predictions with trained model"""
421 |
422 |         if model_id not in self.trained_models:
423 |             # Try to load from disk
424 |             model = await self._load_model(model_id)
425 |             if model is None:
426 |                 raise ValueError(f"Model {model_id} not found")
427 |             self.trained_models[model_id] = model
428 |
429 |         model = self.trained_models[model_id]
430 |         metadata = self.model_metadata.get(model_id, {})
431 |
432 |         # Prepare input data
433 |         if isinstance(data, dict):
434 |             data = pd.DataFrame([data])
435 |         elif isinstance(data, list):
436 |             data = pd.DataFrame(data)
437 |
438 |         # Select features used during training
439 |         if 'features' in metadata:
440 |             data = data[metadata['features']]
441 |
442 |         # Handle categorical variables (same as training)
443 |         categorical_cols = data.select_dtypes(include=['object', 'category']).columns
444 |         for col in categorical_cols:
445 |             le = LabelEncoder()
446 |             data[col] = le.fit_transform(data[col].astype(str))
447 |
448 |         # Handle missing values
449 |         data = data.fillna(data.mean())
450 |
451 |         # Make predictions
452 |         predictions = model.predict(data)
453 |
454 |         return predictions
455 |
456 |     async def evaluate(self,
457 |                        model_id: str,
458 |                        X_test: pd.DataFrame,
459 |                        y_test: pd.Series) -> Dict[str, Any]:
460 |         """Evaluate existing model on new data"""
461 |
462 |         if model_id not in self.trained_models:
463 |             model = await self._load_model(model_id)
464 |             if model is None:
465 |                 raise ValueError(f"Model {model_id} not found")
466 |             self.trained_models[model_id] = model
467 |
468 |         model = self.trained_models[model_id]
469 |         metadata = self.model_metadata.get(model_id, {})
470 |         task = metadata.get('task', 'classification')
471 |
472 |         return await self._evaluate_model(model, X_test, y_test, task)
473 |
474 |     async def feature_selection(self,
475 |                                 X: pd.DataFrame,
476 |                                 y: pd.Series,
477 |                                 method: str = 'univariate',
478 |                                 n_features: int = 10) -> List[str]:
479 |         """Select best features"""
480 |
481 |         if method == 'univariate':
482 |             # Univariate feature selection
483 |             selector = SelectKBest(f_classif if y.dtype == 'object' else f_regression, k=n_features)
484 |             selector.fit(X, y)
485 |             selected_features = X.columns[selector.get_support()].tolist()
486 |
487 |         elif method == 'rfe':
488 |             # Recursive Feature Elimination
489 |             from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
490 |
491 |             if y.dtype == 'object':
492 |                 estimator = RandomForestClassifier(n_estimators=50, random_state=42)
493 |             else:
494 |                 estimator = RandomForestRegressor(n_estimators=50, random_state=42)
495 |
496 |             selector = RFE(estimator, n_features_to_select=n_features)
497 |             selector.fit(X, y)
498 |             selected_features = X.columns[selector.support_].tolist()
499 |
500 |         elif method == 'l1':
501 |             # L1-based feature selection
502 |             from sklearn.linear_model import LogisticRegression, Lasso
503 |
504 |             if y.dtype == 'object':
505 |                 estimator = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)
506 |             else:
507 |                 estimator = Lasso(alpha=0.01, random_state=42)
508 |
509 |             selector = SelectFromModel(estimator, max_features=n_features)
510 |             selector.fit(X, y)
511 |             selected_features = X.columns[selector.get_support()].tolist()
512 |
513 |         else:
514 |             raise ValueError(f"Unknown feature selection method: {method}")
515 |
516 |         return selected_features
517 |
518 |     async def cross_validate_model(self,
519 |                                    model,
520 |                                    X: pd.DataFrame,
521 |                                    y: pd.Series,
522 |                                    cv: int = 5,
523 |                                    scoring: str = None) -> Dict[str, Any]:
524 |         """Perform cross-validation"""
525 |
526 |         if scoring is None:
527 |             # Auto-detect scoring based on target type
528 |             if y.dtype == 'object' or len(np.unique(y)) < 10:
529 |                 scoring = 'accuracy'
530 |             else:
531 |                 scoring = 'neg_mean_squared_error'
532 |
533 |         scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
534 |
535 |         return {
536 |             'scores': scores.tolist(),
537 |             'mean': float(scores.mean()),
538 |             'std': float(scores.std()),
539 |             'cv_folds': cv,
540 |             'scoring': scoring
541 |         }
542 |
543 |     async def automl(self,
544 |                      data: pd.DataFrame,
545 |                      target: str,
546 |                      task: str = 'auto',
547 |                      time_budget: int = 300,
548 |                      **kwargs) -> Dict[str, Any]:
549 |         """Automated machine learning"""
550 |
551 |         # Auto-detect task if not specified
552 |         if task == 'auto':
553 |             if data[target].dtype == 'object' or data[target].nunique() < 10:
554 |                 task = 'classification'
555 |             else:
556 |                 task = 'regression'
557 |
558 |         # Try different algorithms
559 |         algorithms = {
560 |             'classification': ['rf', 'xgboost', 'logistic', 'svm'],
561 |             'regression': ['rf_reg', 'xgb_reg', 'linear', 'ridge']
562 |         }
563 |
564 |         best_model = None
565 |         best_score = -np.inf if task == 'regression' else 0
566 |         best_algorithm = None
567 |         results = []
568 |
569 |         features = [col for col in data.columns if col != target]
570 |
571 |         for algorithm in algorithms.get(task, []):
572 |             try:
573 |                 # Train model
574 |                 result = await self.train_model(
575 |                     data=data,
576 |                     task=task,
577 |                     algorithm=algorithm,
578 |                     features=features,
579 |                     target=target,
580 |                     auto_ml=False,  # Don't tune within automl
581 |                     **kwargs
582 |                 )
583 |
584 |                 # Get primary metric
585 |                 if task == 'classification':
586 |                     score = result['metrics'].get('accuracy', 0)
587 |                 else:
588 |                     score = -result['metrics'].get('mse', np.inf)
589 |
590 |                 results.append({
591 |                     'algorithm': algorithm,
592 |                     'score': score,
593 |                     'metrics': result['metrics']
594 |                 })
595 |
596 |                 if score > best_score:
597 |                     best_score = score
598 |                     best_algorithm = algorithm
599 |                     best_model = result
600 |
601 |             except Exception as e:
602 |                 logger.warning(f"Failed to train {algorithm}: {e}")
603 |                 continue
604 |
605 |         return {
606 |             'best_model': best_model,
607 |             'best_algorithm': best_algorithm,
608 |             'best_score': best_score,
609 |             'all_results': results,
610 |             'task': task
611 |         }
612 |
613 |     def _generate_model_id(self, task: str, algorithm: str) -> str:
614 |         """Generate unique model ID"""
615 |         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
616 |         hash_input = f"{task}_{algorithm}_{timestamp}"
617 |         model_hash = hashlib.md5(hash_input.encode()).hexdigest()[:8]
618 |         return f"{task}_{algorithm}_{model_hash}"
619 |
620 |     async def _save_model(self, model, model_id: str):
621 |         """Save model to disk"""
622 |         try:
623 |             model_path = self.model_cache_path / f"{model_id}.pkl"
624 |             joblib.dump(model, model_path)
625 |
626 |             # Save metadata
627 |             metadata_path = self.model_cache_path / f"{model_id}_metadata.json"
628 |             with open(metadata_path, 'w') as f:
629 |                 json.dump(self.model_metadata.get(model_id, {}), f)
630 |
631 |         except Exception as e:
632 |             logger.error(f"Failed to save model {model_id}: {e}")
633 |
634 |     async def _load_model(self, model_id: str):
635 |         """Load model from disk"""
636 |         try:
637 |             model_path = self.model_cache_path / f"{model_id}.pkl"
638 |             if model_path.exists():
639 |                 model = joblib.load(model_path)
640 |
641 |                 # Load metadata
642 |                 metadata_path = self.model_cache_path / f"{model_id}_metadata.json"
643 |                 if metadata_path.exists():
644 |                     with open(metadata_path, 'r') as f:
645 |                         self.model_metadata[model_id] = json.load(f)
646 |
647 |                 return model
648 |
649 |         except Exception as e:
650 |             logger.error(f"Failed to load model {model_id}: {e}")
651 |
652 |         return None
653 |
654 |     async def export_model(self, model_id: str, format: str = 'pickle') -> bytes:
655 |         """Export model in various formats"""
656 |
657 |         if model_id not in self.trained_models:
658 |             model = await self._load_model(model_id)
659 |             if model is None:
660 |                 raise ValueError(f"Model {model_id} not found")
661 |             self.trained_models[model_id] = model
662 |
663 |         model = self.trained_models[model_id]
664 |
665 |         if format == 'pickle':
666 |             import io
667 |             buffer = io.BytesIO()
668 |             pickle.dump(model, buffer)
669 |             return buffer.getvalue()
670 |
671 |         elif format == 'joblib':
672 |             import io
673 |             buffer = io.BytesIO()
674 |             joblib.dump(model, buffer)
675 |             return buffer.getvalue()
676 |
677 |         elif format == 'onnx':
678 |             # Would need skl2onnx
679 |             pass
680 |
681 |         elif format == 'pmml':
682 |             # Would need sklearn2pmml
683 |             pass
684 |
685 |         else:
686 |             raise ValueError(f"Unsupported export format: {format}")
687 |
688 |     async def get_model_info(self, model_id: str) -> Dict[str, Any]:
689 |         """Get model information"""
690 |
691 |         if model_id in self.model_metadata:
692 |             return self.model_metadata[model_id]
693 |
694 |         # Try to load metadata from disk
695 |         metadata_path = self.model_cache_path / f"{model_id}_metadata.json"
696 |         if metadata_path.exists():
697 |             with open(metadata_path, 'r') as f:
698 |                 return json.load(f)
699 |
700 |         return {}
701 |
702 |     async def list_models(self) -> List[Dict[str, Any]]:
703 |         """List all available models"""
704 |         models = []
705 |
706 |         # Add loaded models
707 |         for model_id in self.trained_models.keys():
708 |             info = await self.get_model_info(model_id)
709 |             models.append({
710 |                 'model_id': model_id,
711 |                 'loaded': True,
712 |                 **info
713 |             })
714 |
715 |         # Add saved models
716 |         for model_file in self.model_cache_path.glob("*.pkl"):
717 |             model_id = model_file.stem
718 |             if model_id not in self.trained_models:
719 |                 info = await self.get_model_info(model_id)
720 |                 models.append({
721 |                     'model_id': model_id,
722 |                     'loaded': False,
723 |                     **info
724 |                 })
725 |
726 |         return models
727 |
728 |     async def delete_model(self, model_id: str) -> bool:
729 |         """Delete a model"""
730 |
731 |         # Remove from memory
732 |         if model_id in self.trained_models:
733 |             del self.trained_models[model_id]
734 |
735 |         if model_id in self.model_metadata:
736 |             del self.model_metadata[model_id]
737 |
738 |         # Remove from disk
739 |         model_path = self.model_cache_path / f"{model_id}.pkl"
740 |         metadata_path = self.model_cache_path / f"{model_id}_metadata.json"
741 |
742 |         deleted = False
743 |         if model_path.exists():
744 |             model_path.unlink()
745 |             deleted = True
746 |
747 |         if metadata_path.exists():
748 |             metadata_path.unlink()
749 |
750 |         return deleted
751 |


--------------------------------------------------------------------------------
/backend/app/services/visualization_service.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Visualization Service for OpenStatica
  3 | Creates comprehensive data visualizations similar to SPSS
  4 | """
  5 |
  6 | import pandas as pd
  7 | import numpy as np
  8 | import matplotlib
  9 |
 10 | matplotlib.use('Agg')  # Use non-interactive backend
 11 | import matplotlib.pyplot as plt
 12 | import seaborn as sns
 13 | import plotly.graph_objects as go
 14 | import plotly.express as px
 15 | from plotly.subplots import make_subplots
 16 | from typing import Dict, Any, List, Optional, Union
 17 | import io
 18 | import base64
 19 | from scipy import stats
 20 | import warnings
 21 |
 22 | warnings.filterwarnings('ignore')
 23 |
 24 |
 25 | class VisualizationService:
 26 |     """Service for creating data visualizations"""
 27 |
 28 |     def __init__(self):
 29 |         # Set style
 30 |         sns.set_style("whitegrid")
 31 |         plt.rcParams['figure.figsize'] = (10, 6)
 32 |         plt.rcParams['font.size'] = 10
 33 |         plt.rcParams['axes.labelsize'] = 12
 34 |         plt.rcParams['axes.titlesize'] = 14
 35 |         plt.rcParams['xtick.labelsize'] = 10
 36 |         plt.rcParams['ytick.labelsize'] = 10
 37 |
 38 |         # Color palettes
 39 |         self.colors = px.colors.qualitative.Set2
 40 |         self.sequential_colors = px.colors.sequential.Viridis
 41 |
 42 |     async def create_visualization(self,
 43 |                                    viz_type: str,
 44 |                                    data: pd.DataFrame,
 45 |                                    **kwargs) -> Dict[str, Any]:
 46 |         """Create a visualization based on type"""
 47 |
 48 |         viz_methods = {
 49 |             'histogram': self.create_histogram,
 50 |             'boxplot': self.create_boxplot,
 51 |             'scatter': self.create_scatter_plot,
 52 |             'line': self.create_line_plot,
 53 |             'bar': self.create_bar_chart,
 54 |             'pie': self.create_pie_chart,
 55 |             'heatmap': self.create_heatmap,
 56 |             'correlation_matrix': self.create_correlation_matrix,
 57 |             'pairplot': self.create_pairplot,
 58 |             'distribution': self.create_distribution_plot,
 59 |             'qq': self.create_qq_plot,
 60 |             'residuals': self.create_residual_plots,
 61 |             'roc': self.create_roc_curve,
 62 |             'confusion_matrix': self.create_confusion_matrix_plot,
 63 |             'feature_importance': self.create_feature_importance_plot,
 64 |             'parallel_coordinates': self.create_parallel_coordinates,
 65 |             'violin': self.create_violin_plot,
 66 |             '3d_scatter': self.create_3d_scatter,
 67 |             'surface': self.create_surface_plot,
 68 |             'dendogram': self.create_dendrogram,
 69 |             'error_bars': self.create_error_bar_plot
 70 |         }
 71 |
 72 |         method = viz_methods.get(viz_type)
 73 |         if not method:
 74 |             raise ValueError(f"Unknown visualization type: {viz_type}")
 75 |
 76 |         return await method(data, **kwargs)
 77 |
 78 |     async def create_histogram(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
 79 |         """Create histogram with normal distribution overlay"""
 80 |         column = kwargs.get('column')
 81 |         bins = kwargs.get('bins', 'auto')
 82 |         show_kde = kwargs.get('show_kde', True)
 83 |         show_normal = kwargs.get('show_normal', True)
 84 |
 85 |         if column not in data.columns:
 86 |             raise ValueError(f"Column {column} not found")
 87 |
 88 |         values = data[column].dropna()
 89 |
 90 |         # Create figure with plotly
 91 |         fig = go.Figure()
 92 |
 93 |         # Add histogram
 94 |         fig.add_trace(go.Histogram(
 95 |             x=values,
 96 |             nbinsx=30 if bins == 'auto' else bins,
 97 |             name='Frequency',
 98 |             marker_color='rgba(99, 102, 241, 0.7)',
 99 |             opacity=0.7
100 |         ))
101 |
102 |         # Add KDE if requested
103 |         if show_kde:
104 |             from scipy.stats import gaussian_kde
105 |             kde = gaussian_kde(values)
106 |             x_range = np.linspace(values.min(), values.max(), 100)
107 |             kde_values = kde(x_range) * len(values) * (values.max() - values.min()) / 30
108 |
109 |             fig.add_trace(go.Scatter(
110 |                 x=x_range,
111 |                 y=kde_values,
112 |                 mode='lines',
113 |                 name='KDE',
114 |                 line=dict(color='red', width=2)
115 |             ))
116 |
117 |         # Add normal distribution if requested
118 |         if show_normal:
119 |             mean = values.mean()
120 |             std = values.std()
121 |             x_range = np.linspace(values.min(), values.max(), 100)
122 |             normal_values = stats.norm.pdf(x_range, mean, std) * len(values) * (values.max() - values.min()) / 30
123 |
124 |             fig.add_trace(go.Scatter(
125 |                 x=x_range,
126 |                 y=normal_values,
127 |                 mode='lines',
128 |                 name='Normal',
129 |                 line=dict(color='green', width=2, dash='dash')
130 |             ))
131 |
132 |         # Update layout
133 |         fig.update_layout(
134 |             title=f'Histogram of {column}',
135 |             xaxis_title=column,
136 |             yaxis_title='Frequency',
137 |             bargap=0.1,
138 |             showlegend=True,
139 |             hovermode='x unified'
140 |         )
141 |
142 |         # Add statistics annotation
143 |         stats_text = f"Mean: {values.mean():.2f}<br>"
144 |         stats_text += f"Std: {values.std():.2f}<br>"
145 |         stats_text += f"Skew: {values.skew():.2f}<br>"
146 |         stats_text += f"Kurt: {values.kurtosis():.2f}"
147 |
148 |         fig.add_annotation(
149 |             xref="paper", yref="paper",
150 |             x=0.95, y=0.95,
151 |             text=stats_text,
152 |             showarrow=False,
153 |             bgcolor="white",
154 |             bordercolor="black",
155 |             borderwidth=1
156 |         )
157 |
158 |         return {
159 |             'type': 'histogram',
160 |             'plotly': fig.to_dict(),
161 |             'statistics': {
162 |                 'mean': float(values.mean()),
163 |                 'median': float(values.median()),
164 |                 'std': float(values.std()),
165 |                 'skewness': float(values.skew()),
166 |                 'kurtosis': float(values.kurtosis()),
167 |                 'normality_test': self._test_normality(values)
168 |             }
169 |         }
170 |
171 |     async def create_boxplot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
172 |         """Create box plot with outliers"""
173 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns)
174 |         group_by = kwargs.get('group_by')
175 |         show_points = kwargs.get('show_points', False)
176 |
177 |         fig = go.Figure()
178 |
179 |         if group_by and group_by in data.columns:
180 |             # Grouped boxplot
181 |             for group in data[group_by].unique():
182 |                 group_data = data[data[group_by] == group]
183 |                 for col in columns:
184 |                     fig.add_trace(go.Box(
185 |                         y=group_data[col],
186 |                         name=f'{col} ({group})',
187 |                         boxpoints='outliers' if show_points else False,
188 |                         marker_color=self.colors[len(fig.data) % len(self.colors)]
189 |                     ))
190 |         else:
191 |             # Simple boxplot
192 |             for col in columns:
193 |                 fig.add_trace(go.Box(
194 |                     y=data[col],
195 |                     name=col,
196 |                     boxpoints='outliers' if show_points else False,
197 |                     marker_color=self.colors[len(fig.data) % len(self.colors)],
198 |                     boxmean='sd'  # Show mean and std
199 |                 ))
200 |
201 |         fig.update_layout(
202 |             title='Box Plot Analysis',
203 |             yaxis_title='Value',
204 |             showlegend=True,
205 |             hovermode='x unified'
206 |         )
207 |
208 |         # Calculate outliers
209 |         outliers_info = {}
210 |         for col in columns:
211 |             q1 = data[col].quantile(0.25)
212 |             q3 = data[col].quantile(0.75)
213 |             iqr = q3 - q1
214 |             lower_bound = q1 - 1.5 * iqr
215 |             upper_bound = q3 + 1.5 * iqr
216 |             outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]
217 |             outliers_info[col] = {
218 |                 'count': len(outliers),
219 |                 'percentage': len(outliers) / len(data[col].dropna()) * 100
220 |             }
221 |
222 |         return {
223 |             'type': 'boxplot',
224 |             'plotly': fig.to_dict(),
225 |             'outliers': outliers_info
226 |         }
227 |
228 |     async def create_scatter_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
229 |         """Create scatter plot with regression line"""
230 |         x_col = kwargs.get('x')
231 |         y_col = kwargs.get('y')
232 |         color = kwargs.get('color')
233 |         size = kwargs.get('size')
234 |         trendline = kwargs.get('trendline', 'ols')  # ols, lowess, or None
235 |
236 |         fig = px.scatter(
237 |             data,
238 |             x=x_col,
239 |             y=y_col,
240 |             color=color,
241 |             size=size,
242 |             trendline=trendline if trendline else None,
243 |             title=f'{y_col} vs {x_col}'
244 |         )
245 |
246 |         # Add correlation coefficient
247 |         if x_col and y_col:
248 |             corr = data[[x_col, y_col]].corr().iloc[0, 1]
249 |             fig.add_annotation(
250 |                 xref="paper", yref="paper",
251 |                 x=0.05, y=0.95,
252 |                 text=f"r = {corr:.3f}<br>r² = {corr ** 2:.3f}",
253 |                 showarrow=False,
254 |                 bgcolor="white",
255 |                 bordercolor="black",
256 |                 borderwidth=1
257 |             )
258 |
259 |         fig.update_layout(
260 |             hovermode='closest',
261 |             showlegend=True if color else False
262 |         )
263 |
264 |         return {
265 |             'type': 'scatter',
266 |             'plotly': fig.to_dict(),
267 |             'correlation': float(corr) if x_col and y_col else None
268 |         }
269 |
270 |     async def create_correlation_matrix(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
271 |         """Create correlation matrix heatmap"""
272 |         method = kwargs.get('method', 'pearson')
273 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns)
274 |
275 |         # Calculate correlation matrix
276 |         corr_matrix = data[columns].corr(method=method)
277 |
278 |         # Create heatmap
279 |         fig = go.Figure(data=go.Heatmap(
280 |             z=corr_matrix.values,
281 |             x=corr_matrix.columns,
282 |             y=corr_matrix.columns,
283 |             colorscale='RdBu',
284 |             zmid=0,
285 |             zmin=-1,
286 |             zmax=1,
287 |             text=np.round(corr_matrix.values, 2),
288 |             texttemplate='%{text}',
289 |             textfont={"size": 10},
290 |             reversescale=True,
291 |             colorbar=dict(title="Correlation")
292 |         ))
293 |
294 |         fig.update_layout(
295 |             title=f'{method.capitalize()} Correlation Matrix',
296 |             xaxis={'side': 'bottom'},
297 |             width=700,
298 |             height=700
299 |         )
300 |
301 |         # Find significant correlations
302 |         significant_corr = []
303 |         for i in range(len(corr_matrix.columns)):
304 |             for j in range(i + 1, len(corr_matrix.columns)):
305 |                 corr_val = corr_matrix.iloc[i, j]
306 |                 if abs(corr_val) > 0.5 and abs(corr_val) < 1:
307 |                     significant_corr.append({
308 |                         'var1': corr_matrix.columns[i],
309 |                         'var2': corr_matrix.columns[j],
310 |                         'correlation': float(corr_val)
311 |                     })
312 |
313 |         return {
314 |             'type': 'heatmap',
315 |             'plotly': fig.to_dict(),
316 |             'significant_correlations': significant_corr
317 |         }
318 |
319 |     async def create_pairplot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
320 |         """Create pairwise scatter plot matrix"""
321 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns[:5])
322 |         hue = kwargs.get('hue')
323 |
324 |         # Create scatter matrix
325 |         fig = px.scatter_matrix(
326 |             data,
327 |             dimensions=columns,
328 |             color=hue,
329 |             title="Pairwise Relationships"
330 |         )
331 |
332 |         fig.update_traces(diagonal_visible=False)
333 |         fig.update_layout(
334 |             height=800,
335 |             width=800
336 |         )
337 |
338 |         return {
339 |             'type': 'pairplot',
340 |             'plotly': fig.to_dict()
341 |         }
342 |
343 |     async def create_qq_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
344 |         """Create Q-Q plot for normality assessment"""
345 |         column = kwargs.get('column')
346 |
347 |         values = data[column].dropna().values
348 |
349 |         # Calculate theoretical quantiles
350 |         qq_data = stats.probplot(values, dist="norm")
351 |
352 |         fig = go.Figure()
353 |
354 |         # Add Q-Q plot points
355 |         fig.add_trace(go.Scatter(
356 |             x=qq_data[0][0],
357 |             y=qq_data[0][1],
358 |             mode='markers',
359 |             name='Data',
360 |             marker=dict(color='blue', size=5)
361 |         ))
362 |
363 |         # Add reference line
364 |         x_range = [qq_data[0][0].min(), qq_data[0][0].max()]
365 |         fig.add_trace(go.Scatter(
366 |             x=x_range,
367 |             y=[qq_data[1][1] + qq_data[1][0] * x for x in x_range],
368 |             mode='lines',
369 |             name='Normal',
370 |             line=dict(color='red', dash='dash')
371 |         ))
372 |
373 |         fig.update_layout(
374 |             title=f'Q-Q Plot: {column}',
375 |             xaxis_title='Theoretical Quantiles',
376 |             yaxis_title='Sample Quantiles',
377 |             showlegend=True
378 |         )
379 |
380 |         # Perform normality tests
381 |         shapiro_stat, shapiro_p = stats.shapiro(values[:5000] if len(values) > 5000 else values)
382 |         ks_stat, ks_p = stats.kstest(values, 'norm', args=(values.mean(), values.std()))
383 |
384 |         return {
385 |             'type': 'qq_plot',
386 |             'plotly': fig.to_dict(),
387 |             'normality_tests': {
388 |                 'shapiro': {'statistic': float(shapiro_stat), 'p_value': float(shapiro_p)},
389 |                 'kolmogorov_smirnov': {'statistic': float(ks_stat), 'p_value': float(ks_p)}
390 |             }
391 |         }
392 |
393 |     async def create_residual_plots(self, actual: np.ndarray, predicted: np.ndarray, **kwargs) -> Dict[str, Any]:
394 |         """Create residual diagnostic plots"""
395 |         residuals = actual - predicted
396 |
397 |         # Create subplots
398 |         fig = make_subplots(
399 |             rows=2, cols=2,
400 |             subplot_titles=('Residuals vs Fitted', 'Q-Q Plot',
401 |                             'Scale-Location', 'Residuals vs Leverage')
402 |         )
403 |
404 |         # 1. Residuals vs Fitted
405 |         fig.add_trace(go.Scatter(
406 |             x=predicted,
407 |             y=residuals,
408 |             mode='markers',
409 |             marker=dict(color='blue', size=5),
410 |             name='Residuals'
411 |         ), row=1, col=1)
412 |
413 |         fig.add_trace(go.Scatter(
414 |             x=[predicted.min(), predicted.max()],
415 |             y=[0, 0],
416 |             mode='lines',
417 |             line=dict(color='red', dash='dash'),
418 |             name='Zero line'
419 |         ), row=1, col=1)
420 |
421 |         # 2. Q-Q plot of residuals
422 |         qq_data = stats.probplot(residuals, dist="norm")
423 |         fig.add_trace(go.Scatter(
424 |             x=qq_data[0][0],
425 |             y=qq_data[0][1],
426 |             mode='markers',
427 |             marker=dict(color='blue', size=5),
428 |             name='Q-Q'
429 |         ), row=1, col=2)
430 |
431 |         # 3. Scale-Location (sqrt of standardized residuals)
432 |         standardized_residuals = residuals / residuals.std()
433 |         sqrt_abs_residuals = np.sqrt(np.abs(standardized_residuals))
434 |
435 |         fig.add_trace(go.Scatter(
436 |             x=predicted,
437 |             y=sqrt_abs_residuals,
438 |             mode='markers',
439 |             marker=dict(color='blue', size=5),
440 |             name='Scale-Location'
441 |         ), row=2, col=1)
442 |
443 |         # 4. Cook's distance (simplified)
444 |         n = len(residuals)
445 |         p = 1  # number of parameters (simplified)
446 |         leverage = 1 / n + (predicted - predicted.mean()) ** 2 / ((predicted - predicted.mean()) ** 2).sum()
447 |         cooks_d = (residuals ** 2 / (p * residuals.var())) * (leverage / (1 - leverage) ** 2)
448 |
449 |         fig.add_trace(go.Scatter(
450 |             x=leverage,
451 |             y=standardized_residuals,
452 |             mode='markers',
453 |             marker=dict(color='blue', size=5),
454 |             name='Leverage'
455 |         ), row=2, col=2)
456 |
457 |         fig.update_layout(
458 |             title="Residual Diagnostic Plots",
459 |             showlegend=False,
460 |             height=800
461 |         )
462 |
463 |         # Diagnostic tests
464 |         durbin_watson = self._durbin_watson(residuals)
465 |
466 |         return {
467 |             'type': 'residual_plots',
468 |             'plotly': fig.to_dict(),
469 |             'diagnostics': {
470 |                 'durbin_watson': float(durbin_watson),
471 |                 'mean_residual': float(residuals.mean()),
472 |                 'std_residual': float(residuals.std()),
473 |                 'normality_test': self._test_normality(residuals)
474 |             }
475 |         }
476 |
477 |     async def create_roc_curve(self, y_true: np.ndarray, y_scores: np.ndarray, **kwargs) -> Dict[str, Any]:
478 |         """Create ROC curve"""
479 |         from sklearn.metrics import roc_curve, auc
480 |
481 |         fpr, tpr, thresholds = roc_curve(y_true, y_scores)
482 |         roc_auc = auc(fpr, tpr)
483 |
484 |         fig = go.Figure()
485 |
486 |         # ROC curve
487 |         fig.add_trace(go.Scatter(
488 |             x=fpr,
489 |             y=tpr,
490 |             mode='lines',
491 |             name=f'ROC (AUC = {roc_auc:.3f})',
492 |             line=dict(color='blue', width=2)
493 |         ))
494 |
495 |         # Diagonal reference line
496 |         fig.add_trace(go.Scatter(
497 |             x=[0, 1],
498 |             y=[0, 1],
499 |             mode='lines',
500 |             name='Random',
501 |             line=dict(color='red', dash='dash')
502 |         ))
503 |
504 |         fig.update_layout(
505 |             title='ROC Curve',
506 |             xaxis_title='False Positive Rate',
507 |             yaxis_title='True Positive Rate',
508 |             showlegend=True,
509 |             hovermode='closest'
510 |         )
511 |
512 |         return {
513 |             'type': 'roc_curve',
514 |             'plotly': fig.to_dict(),
515 |             'auc': float(roc_auc),
516 |             'optimal_threshold': float(thresholds[np.argmax(tpr - fpr)])
517 |         }
518 |
519 |     async def create_confusion_matrix_plot(self, y_true: np.ndarray, y_pred: np.ndarray, **kwargs) -> Dict[str, Any]:
520 |         """Create confusion matrix visualization"""
521 |         from sklearn.metrics import confusion_matrix
522 |
523 |         labels = kwargs.get('labels', sorted(np.unique(np.concatenate([y_true, y_pred]))))
524 |         cm = confusion_matrix(y_true, y_pred, labels=labels)
525 |
526 |         # Normalize
527 |         cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
528 |
529 |         fig = go.Figure(data=go.Heatmap(
530 |             z=cm_normalized,
531 |             x=labels,
532 |             y=labels,
533 |             text=cm,
534 |             texttemplate='%{text}',
535 |             textfont={"size": 12},
536 |             colorscale='Blues',
537 |             reversescale=False,
538 |             showscale=True,
539 |             colorbar=dict(title="Proportion")
540 |         ))
541 |
542 |         fig.update_layout(
543 |             title='Confusion Matrix',
544 |             xaxis_title='Predicted',
545 |             yaxis_title='Actual',
546 |             xaxis={'side': 'bottom'},
547 |             width=500,
548 |             height=500
549 |         )
550 |
551 |         return {
552 |             'type': 'confusion_matrix',
553 |             'plotly': fig.to_dict(),
554 |             'matrix': cm.tolist()
555 |         }
556 |
557 |     async def create_feature_importance_plot(self, features: List[str], importance: List[float], **kwargs) -> Dict[
558 |         str, Any]:
559 |         """Create feature importance bar plot"""
560 |         # Sort by importance
561 |         sorted_idx = np.argsort(importance)[::-1]
562 |         top_n = min(kwargs.get('top_n', 20), len(features))
563 |
564 |         fig = go.Figure(go.Bar(
565 |             x=importance[sorted_idx][:top_n],
566 |             y=features[sorted_idx][:top_n],
567 |             orientation='h',
568 |             marker_color='rgba(99, 102, 241, 0.7)'
569 |         ))
570 |
571 |         fig.update_layout(
572 |             title='Feature Importance',
573 |             xaxis_title='Importance',
574 |             yaxis_title='Feature',
575 |             height=max(400, top_n * 20)
576 |         )
577 |
578 |         return {
579 |             'type': 'feature_importance',
580 |             'plotly': fig.to_dict()
581 |         }
582 |
583 |     async def create_parallel_coordinates(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
584 |         """Create parallel coordinates plot"""
585 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns[:7])
586 |         color = kwargs.get('color')
587 |
588 |         fig = px.parallel_coordinates(
589 |             data,
590 |             dimensions=columns,
591 |             color=color,
592 |             title="Parallel Coordinates Plot"
593 |         )
594 |
595 |         fig.update_layout(height=500)
596 |
597 |         return {
598 |             'type': 'parallel_coordinates',
599 |             'plotly': fig.to_dict()
600 |         }
601 |
602 |     def _test_normality(self, values: np.ndarray) -> Dict[str, Any]:
603 |         """Test for normality"""
604 |         shapiro_stat, shapiro_p = stats.shapiro(values[:5000] if len(values) > 5000 else values)
605 |         return {
606 |             'shapiro_wilk': {
607 |                 'statistic': float(shapiro_stat),
608 |                 'p_value': float(shapiro_p),
609 |                 'is_normal': shapiro_p > 0.05
610 |             }
611 |         }
612 |
613 |     def _durbin_watson(self, residuals: np.ndarray) -> float:
614 |         """Calculate Durbin-Watson statistic"""
615 |         diff = np.diff(residuals)
616 |         return np.sum(diff ** 2) / np.sum(residuals ** 2)
617 |
618 |     # Additional visualization methods...
619 |     async def create_violin_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
620 |         """Create violin plot"""
621 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns)
622 |
623 |         fig = go.Figure()
624 |
625 |         for col in columns:
626 |             fig.add_trace(go.Violin(
627 |                 y=data[col],
628 |                 name=col,
629 |                 box_visible=True,
630 |                 meanline_visible=True
631 |             ))
632 |
633 |         fig.update_layout(
634 |             title='Violin Plot',
635 |             yaxis_title='Value',
636 |             showlegend=True
637 |         )
638 |
639 |         return {
640 |             'type': 'violin',
641 |             'plotly': fig.to_dict()
642 |         }
643 |
644 |     async def create_3d_scatter(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
645 |         """Create 3D scatter plot"""
646 |         x_col = kwargs.get('x')
647 |         y_col = kwargs.get('y')
648 |         z_col = kwargs.get('z')
649 |         color = kwargs.get('color')
650 |
651 |         fig = px.scatter_3d(
652 |             data,
653 |             x=x_col,
654 |             y=y_col,
655 |             z=z_col,
656 |             color=color,
657 |             title='3D Scatter Plot'
658 |         )
659 |
660 |         return {
661 |             'type': '3d_scatter',
662 |             'plotly': fig.to_dict()
663 |         }
664 |
665 |     async def create_surface_plot(self, x: np.ndarray, y: np.ndarray, z: np.ndarray, **kwargs) -> Dict[str, Any]:
666 |         """Create 3D surface plot"""
667 |         fig = go.Figure(data=[go.Surface(x=x, y=y, z=z)])
668 |
669 |         fig.update_layout(
670 |             title='3D Surface Plot',
671 |             scene=dict(
672 |                 xaxis_title='X',
673 |                 yaxis_title='Y',
674 |                 zaxis_title='Z'
675 |             )
676 |         )
677 |
678 |         return {
679 |             'type': 'surface',
680 |             'plotly': fig.to_dict()
681 |         }
682 |
683 |     async def create_dendrogram(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
684 |         """Create dendrogram for hierarchical clustering"""
685 |         from scipy.cluster.hierarchy import dendrogram, linkage
686 |
687 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns)
688 |         method = kwargs.get('method', 'ward')
689 |
690 |         # Calculate linkage
691 |         Z = linkage(data[columns].dropna(), method=method)
692 |
693 |         # Create dendrogram
694 |         fig = plt.figure(figsize=(10, 6))
695 |         dendrogram(Z, labels=data.index)
696 |         plt.title('Hierarchical Clustering Dendrogram')
697 |         plt.xlabel('Sample Index')
698 |         plt.ylabel('Distance')
699 |
700 |         # Convert to base64
701 |         buf = io.BytesIO()
702 |         plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')
703 |         buf.seek(0)
704 |         img_base64 = base64.b64encode(buf.read()).decode('utf-8')
705 |         plt.close()
706 |
707 |         return {
708 |             'type': 'dendrogram',
709 |             'image': f'data:image/png;base64,{img_base64}'
710 |         }
711 |
712 |     async def create_line_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
713 |         """Create line plot"""
714 |         x_col = kwargs.get('x')
715 |         y_cols = kwargs.get('y', [])
716 |
717 |         if not isinstance(y_cols, list):
718 |             y_cols = [y_cols]
719 |
720 |         fig = go.Figure()
721 |
722 |         for y_col in y_cols:
723 |             fig.add_trace(go.Scatter(
724 |                 x=data[x_col] if x_col else data.index,
725 |                 y=data[y_col],
726 |                 mode='lines+markers',
727 |                 name=y_col
728 |             ))
729 |
730 |         fig.update_layout(
731 |             title='Line Plot',
732 |             xaxis_title=x_col if x_col else 'Index',
733 |             yaxis_title='Value',
734 |             hovermode='x unified'
735 |         )
736 |
737 |         return {
738 |             'type': 'line',
739 |             'plotly': fig.to_dict()
740 |         }
741 |
742 |     async def create_bar_chart(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
743 |         """Create bar chart"""
744 |         x_col = kwargs.get('x')
745 |         y_col = kwargs.get('y')
746 |         orientation = kwargs.get('orientation', 'v')
747 |
748 |         fig = px.bar(
749 |             data,
750 |             x=x_col if orientation == 'v' else y_col,
751 |             y=y_col if orientation == 'v' else x_col,
752 |             orientation=orientation,
753 |             title='Bar Chart'
754 |         )
755 |
756 |         return {
757 |             'type': 'bar',
758 |             'plotly': fig.to_dict()
759 |         }
760 |
761 |     async def create_pie_chart(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
762 |         """Create pie chart"""
763 |         values_col = kwargs.get('values')
764 |         names_col = kwargs.get('names')
765 |
766 |         fig = px.pie(
767 |             data,
768 |             values=values_col,
769 |             names=names_col,
770 |             title='Pie Chart'
771 |         )
772 |
773 |         return {
774 |             'type': 'pie',
775 |             'plotly': fig.to_dict()
776 |         }
777 |
778 |     async def create_distribution_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
779 |         """Create distribution comparison plot"""
780 |         columns = kwargs.get('columns', data.select_dtypes(include=[np.number]).columns)
781 |
782 |         fig = go.Figure()
783 |
784 |         for col in columns:
785 |             fig.add_trace(go.Histogram(
786 |                 x=data[col],
787 |                 name=col,
788 |                 opacity=0.7,
789 |                 nbinsx=30
790 |             ))
791 |
792 |         fig.update_layout(
793 |             title='Distribution Comparison',
794 |             xaxis_title='Value',
795 |             yaxis_title='Frequency',
796 |             barmode='overlay'
797 |         )
798 |
799 |         return {
800 |             'type': 'distribution',
801 |             'plotly': fig.to_dict()
802 |         }
803 |
804 |     async def create_error_bar_plot(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
805 |         """Create error bar plot"""
806 |         x_col = kwargs.get('x')
807 |         y_col = kwargs.get('y')
808 |         error_col = kwargs.get('error')
809 |
810 |         fig = go.Figure(data=go.Scatter(
811 |             x=data[x_col],
812 |             y=data[y_col],
813 |             error_y=dict(
814 |                 type='data',
815 |                 array=data[error_col] if error_col else data[y_col].std(),
816 |                 visible=True
817 |             ),
818 |             mode='markers+lines',
819 |             marker=dict(size=8)
820 |         ))
821 |
822 |         fig.update_layout(
823 |             title='Error Bar Plot',
824 |             xaxis_title=x_col,
825 |             yaxis_title=y_col
826 |         )
827 |
828 |         return {
829 |             'type': 'error_bar',
830 |             'plotly': fig.to_dict()
831 |         }
832 |
833 |     async def create_heatmap(self, data: pd.DataFrame, **kwargs) -> Dict[str, Any]:
834 |         """Create generic heatmap"""
835 |         x_col = kwargs.get('x')
836 |         y_col = kwargs.get('y')
837 |         z_col = kwargs.get('z')
838 |
839 |         if z_col:
840 |             # Pivot data for heatmap
841 |             pivot_data = data.pivot(index=y_col, columns=x_col, values=z_col)
842 |         else:
843 |             # Use data directly
844 |             pivot_data = data
845 |
846 |         fig = go.Figure(data=go.Heatmap(
847 |             z=pivot_data.values,
848 |             x=pivot_data.columns,
849 |             y=pivot_data.index,
850 |             colorscale='Viridis'
851 |         ))
852 |
853 |         fig.update_layout(
854 |             title='Heatmap',
855 |             xaxis_title=x_col,
856 |             yaxis_title=y_col
857 |         )
858 |
859 |         return {
860 |             'type': 'heatmap',
861 |             'plotly': fig.to_dict()
862 |         }
863 |


--------------------------------------------------------------------------------
/backend/app/utils/__init__.py:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/app/utils/__init__.py


--------------------------------------------------------------------------------
/backend/app/utils/decorators.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Decorators for OpenStatica
  3 | Utility decorators for validation, caching, and performance
  4 | """
  5 |
  6 | import functools
  7 | import time
  8 | import asyncio
  9 | from typing import Callable, Any, Optional
 10 | import logging
 11 | from fastapi import HTTPException, status
 12 | import hashlib
 13 | import json
 14 |
 15 | logger = logging.getLogger(__name__)
 16 |
 17 |
 18 | def timer(func: Callable) -> Callable:
 19 |     """Decorator to measure function execution time"""
 20 |
 21 |     @functools.wraps(func)
 22 |     async def async_wrapper(*args, **kwargs):
 23 |         start = time.perf_counter()
 24 |         result = await func(*args, **kwargs)
 25 |         end = time.perf_counter()
 26 |         logger.info(f"{func.__name__} took {end - start:.4f} seconds")
 27 |         return result
 28 |
 29 |     @functools.wraps(func)
 30 |     def sync_wrapper(*args, **kwargs):
 31 |         start = time.perf_counter()
 32 |         result = func(*args, **kwargs)
 33 |         end = time.perf_counter()
 34 |         logger.info(f"{func.__name__} took {end - start:.4f} seconds")
 35 |         return result
 36 |
 37 |     if asyncio.iscoroutinefunction(func):
 38 |         return async_wrapper
 39 |     else:
 40 |         return sync_wrapper
 41 |
 42 |
 43 | def cache(ttl: int = 300):
 44 |     """Simple in-memory cache decorator with TTL"""
 45 |
 46 |     def decorator(func: Callable) -> Callable:
 47 |         cache_data = {}
 48 |         cache_time = {}
 49 |
 50 |         @functools.wraps(func)
 51 |         async def async_wrapper(*args, **kwargs):
 52 |             # Create cache key
 53 |             key = hashlib.md5(
 54 |                 f"{args}{kwargs}".encode()
 55 |             ).hexdigest()
 56 |
 57 |             # Check cache
 58 |             if key in cache_data:
 59 |                 if time.time() - cache_time[key] < ttl:
 60 |                     logger.debug(f"Cache hit for {func.__name__}")
 61 |                     return cache_data[key]
 62 |
 63 |             # Execute function
 64 |             result = await func(*args, **kwargs)
 65 |
 66 |             # Store in cache
 67 |             cache_data[key] = result
 68 |             cache_time[key] = time.time()
 69 |
 70 |             return result
 71 |
 72 |         @functools.wraps(func)
 73 |         def sync_wrapper(*args, **kwargs):
 74 |             # Create cache key
 75 |             key = hashlib.md5(
 76 |                 f"{args}{kwargs}".encode()
 77 |             ).hexdigest()
 78 |
 79 |             # Check cache
 80 |             if key in cache_data:
 81 |                 if time.time() - cache_time[key] < ttl:
 82 |                     logger.debug(f"Cache hit for {func.__name__}")
 83 |                     return cache_data[key]
 84 |
 85 |             # Execute function
 86 |             result = func(*args, **kwargs)
 87 |
 88 |             # Store in cache
 89 |             cache_data[key] = result
 90 |             cache_time[key] = time.time()
 91 |
 92 |             return result
 93 |
 94 |         if asyncio.iscoroutinefunction(func):
 95 |             return async_wrapper
 96 |         else:
 97 |             return sync_wrapper
 98 |
 99 |     return decorator
100 |
101 |
102 | def validate_session(func: Callable) -> Callable:
103 |     """Decorator to validate session exists"""
104 |
105 |     @functools.wraps(func)
106 |     async def wrapper(*args, **kwargs):
107 |         # Extract session_id from kwargs or first positional arg
108 |         session_id = kwargs.get('session_id')
109 |         if not session_id and len(args) > 1:
110 |             # Assuming first arg is self/request, second is session_id
111 |             session_id = args[1] if isinstance(args[1], str) else None
112 |
113 |         if not session_id:
114 |             raise HTTPException(
115 |                 status_code=status.HTTP_400_BAD_REQUEST,
116 |                 detail="Session ID required"
117 |             )
118 |
119 |         return await func(*args, **kwargs)
120 |
121 |     return wrapper
122 |
123 |
124 | def require_data(func: Callable) -> Callable:
125 |     """Decorator to ensure data is loaded in session"""
126 |
127 |     @functools.wraps(func)
128 |     async def wrapper(*args, **kwargs):
129 |         # This would need access to session_manager
130 |         # For now, just pass through
131 |         return await func(*args, **kwargs)
132 |
133 |     return wrapper
134 |
135 |
136 | def retry(max_attempts: int = 3, delay: float = 1.0):
137 |     """Decorator to retry failed operations"""
138 |
139 |     def decorator(func: Callable) -> Callable:
140 |         @functools.wraps(func)
141 |         async def async_wrapper(*args, **kwargs):
142 |             last_exception = None
143 |
144 |             for attempt in range(max_attempts):
145 |                 try:
146 |                     return await func(*args, **kwargs)
147 |                 except Exception as e:
148 |                     last_exception = e
149 |                     if attempt < max_attempts - 1:
150 |                         await asyncio.sleep(delay * (2 ** attempt))
151 |                         logger.warning(f"Retry {attempt + 1}/{max_attempts} for {func.__name__}")
152 |                     else:
153 |                         logger.error(f"All retries failed for {func.__name__}")
154 |
155 |             raise last_exception
156 |
157 |         @functools.wraps(func)
158 |         def sync_wrapper(*args, **kwargs):
159 |             last_exception = None
160 |
161 |             for attempt in range(max_attempts):
162 |                 try:
163 |                     return func(*args, **kwargs)
164 |                 except Exception as e:
165 |                     last_exception = e
166 |                     if attempt < max_attempts - 1:
167 |                         time.sleep(delay * (2 ** attempt))
168 |                         logger.warning(f"Retry {attempt + 1}/{max_attempts} for {func.__name__}")
169 |                     else:
170 |                         logger.error(f"All retries failed for {func.__name__}")
171 |
172 |             raise last_exception
173 |
174 |         if asyncio.iscoroutinefunction(func):
175 |             return async_wrapper
176 |         else:
177 |             return sync_wrapper
178 |
179 |     return decorator
180 |
181 |
182 | def rate_limit(calls: int = 10, period: int = 60):
183 |     """Simple rate limiting decorator"""
184 |
185 |     def decorator(func: Callable) -> Callable:
186 |         call_times = []
187 |
188 |         @functools.wraps(func)
189 |         async def wrapper(*args, **kwargs):
190 |             now = time.time()
191 |
192 |             # Remove old calls outside the period
193 |             nonlocal call_times
194 |             call_times = [t for t in call_times if now - t < period]
195 |
196 |             # Check rate limit
197 |             if len(call_times) >= calls:
198 |                 raise HTTPException(
199 |                     status_code=status.HTTP_429_TOO_MANY_REQUESTS,
200 |                     detail=f"Rate limit exceeded: {calls} calls per {period} seconds"
201 |                 )
202 |
203 |             # Record this call
204 |             call_times.append(now)
205 |
206 |             return await func(*args, **kwargs)
207 |
208 |         return wrapper
209 |
210 |     return decorator
211 |
212 |
213 | def log_activity(level: str = "INFO"):
214 |     """Decorator to log function calls"""
215 |
216 |     def decorator(func: Callable) -> Callable:
217 |         @functools.wraps(func)
218 |         async def async_wrapper(*args, **kwargs):
219 |             log_func = getattr(logger, level.lower(), logger.info)
220 |             log_func(f"Calling {func.__name__} with args={args[1:]} kwargs={kwargs}")
221 |
222 |             try:
223 |                 result = await func(*args, **kwargs)
224 |                 log_func(f"{func.__name__} completed successfully")
225 |                 return result
226 |             except Exception as e:
227 |                 logger.error(f"{func.__name__} failed with error: {e}")
228 |                 raise
229 |
230 |         @functools.wraps(func)
231 |         def sync_wrapper(*args, **kwargs):
232 |             log_func = getattr(logger, level.lower(), logger.info)
233 |             log_func(f"Calling {func.__name__} with args={args[1:]} kwargs={kwargs}")
234 |
235 |             try:
236 |                 result = func(*args, **kwargs)
237 |                 log_func(f"{func.__name__} completed successfully")
238 |                 return result
239 |             except Exception as e:
240 |                 logger.error(f"{func.__name__} failed with error: {e}")
241 |                 raise
242 |
243 |         if asyncio.iscoroutinefunction(func):
244 |             return async_wrapper
245 |         else:
246 |             return sync_wrapper
247 |
248 |     return decorator
249 |
250 |
251 | def validate_input(**validators):
252 |     """Decorator to validate input parameters"""
253 |
254 |     def decorator(func: Callable) -> Callable:
255 |         @functools.wraps(func)
256 |         async def wrapper(*args, **kwargs):
257 |             # Validate each parameter
258 |             for param_name, validator in validators.items():
259 |                 if param_name in kwargs:
260 |                     value = kwargs[param_name]
261 |                     if not validator(value):
262 |                         raise HTTPException(
263 |                             status_code=status.HTTP_400_BAD_REQUEST,
264 |                             detail=f"Invalid value for parameter: {param_name}"
265 |                         )
266 |
267 |             return await func(*args, **kwargs)
268 |
269 |         return wrapper
270 |
271 |     return decorator
272 |


--------------------------------------------------------------------------------
/backend/app/utils/helpers.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Helper functions for OpenStatica
  3 | Common utility functions used across the application
  4 | """
  5 |
  6 | import hashlib
  7 | import json
  8 | import uuid
  9 | from typing import Any, Dict, List, Optional, Union
 10 | from datetime import datetime, timedelta
 11 | import numpy as np
 12 | import pandas as pd
 13 | import io
 14 | import base64
 15 | import re
 16 | from pathlib import Path
 17 |
 18 |
 19 | def generate_session_id() -> str:
 20 |     """Generate unique session ID"""
 21 |     return str(uuid.uuid4())
 22 |
 23 |
 24 | def generate_model_id(prefix: str = "model") -> str:
 25 |     """Generate unique model ID"""
 26 |     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
 27 |     unique_id = str(uuid.uuid4())[:8]
 28 |     return f"{prefix}_{timestamp}_{unique_id}"
 29 |
 30 |
 31 | def hash_data(data: Any) -> str:
 32 |     """Generate hash for data caching"""
 33 |     if isinstance(data, pd.DataFrame):
 34 |         data_str = pd.util.hash_pandas_object(data).values.tobytes()
 35 |     elif isinstance(data, np.ndarray):
 36 |         data_str = data.tobytes()
 37 |     else:
 38 |         data_str = json.dumps(data, sort_keys=True).encode()
 39 |
 40 |     return hashlib.sha256(data_str).hexdigest()
 41 |
 42 |
 43 | def sanitize_filename(filename: str) -> str:
 44 |     """Sanitize filename for safe storage"""
 45 |     # Remove any path components
 46 |     filename = Path(filename).name
 47 |
 48 |     # Replace unsafe characters
 49 |     filename = re.sub(r'[^\w\s\-\.]', '', filename)
 50 |     filename = re.sub(r'[-\s]+', '-', filename)
 51 |
 52 |     # Limit length
 53 |     name, ext = filename.rsplit('.', 1) if '.' in filename else (filename, '')
 54 |     if len(name) > 50:
 55 |         name = name[:50]
 56 |
 57 |     return f"{name}.{ext}" if ext else name
 58 |
 59 |
 60 | def format_bytes(bytes_value: int) -> str:
 61 |     """Format bytes to human readable string"""
 62 |     for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
 63 |         if bytes_value < 1024.0:
 64 |             return f"{bytes_value:.2f} {unit}"
 65 |         bytes_value /= 1024.0
 66 |     return f"{bytes_value:.2f} PB"
 67 |
 68 |
 69 | def format_duration(seconds: float) -> str:
 70 |     """Format duration to human readable string"""
 71 |     if seconds < 1:
 72 |         return f"{seconds * 1000:.2f}ms"
 73 |     elif seconds < 60:
 74 |         return f"{seconds:.2f}s"
 75 |     elif seconds < 3600:
 76 |         minutes = int(seconds / 60)
 77 |         secs = seconds % 60
 78 |         return f"{minutes}m {secs:.0f}s"
 79 |     else:
 80 |         hours = int(seconds / 3600)
 81 |         minutes = int((seconds % 3600) / 60)
 82 |         return f"{hours}h {minutes}m"
 83 |
 84 |
 85 | def encode_dataframe_to_json(df: pd.DataFrame) -> str:
 86 |     """Encode DataFrame to JSON string"""
 87 |     return df.to_json(orient='split', date_format='iso')
 88 |
 89 |
 90 | def decode_json_to_dataframe(json_str: str) -> pd.DataFrame:
 91 |     """Decode JSON string to DataFrame"""
 92 |     return pd.read_json(json_str, orient='split')
 93 |
 94 |
 95 | def dataframe_to_base64(df: pd.DataFrame) -> str:
 96 |     """Convert DataFrame to base64 encoded string"""
 97 |     buffer = io.BytesIO()
 98 |     df.to_pickle(buffer)
 99 |     buffer.seek(0)
100 |     return base64.b64encode(buffer.read()).decode('utf-8')
101 |
102 |
103 | def base64_to_dataframe(b64_str: str) -> pd.DataFrame:
104 |     """Convert base64 string back to DataFrame"""
105 |     buffer = io.BytesIO(base64.b64decode(b64_str))
106 |     return pd.read_pickle(buffer)
107 |
108 |
109 | def flatten_dict(d: Dict, parent_key: str = '', sep: str = '.') -> Dict:
110 |     """Flatten nested dictionary"""
111 |     items = []
112 |     for k, v in d.items():
113 |         new_key = f"{parent_key}{sep}{k}" if parent_key else k
114 |         if isinstance(v, dict):
115 |             items.extend(flatten_dict(v, new_key, sep=sep).items())
116 |         else:
117 |             items.append((new_key, v))
118 |     return dict(items)
119 |
120 |
121 | def unflatten_dict(d: Dict, sep: str = '.') -> Dict:
122 |     """Unflatten dictionary"""
123 |     result = {}
124 |     for key, value in d.items():
125 |         parts = key.split(sep)
126 |         target = result
127 |         for part in parts[:-1]:
128 |             if part not in target:
129 |                 target[part] = {}
130 |             target = target[part]
131 |         target[parts[-1]] = value
132 |     return result
133 |
134 |
135 | def chunk_list(lst: List, chunk_size: int) -> List[List]:
136 |     """Split list into chunks"""
137 |     return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]
138 |
139 |
140 | def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:
141 |     """Safe division with default value for division by zero"""
142 |     if denominator == 0:
143 |         return default
144 |     return numerator / denominator
145 |
146 |
147 | def get_column_types(df: pd.DataFrame) -> Dict[str, str]:
148 |     """Get column types for a DataFrame"""
149 |     type_mapping = {
150 |         'int64': 'integer',
151 |         'float64': 'numeric',
152 |         'object': 'categorical',
153 |         'bool': 'boolean',
154 |         'datetime64': 'datetime',
155 |         'timedelta64': 'timedelta',
156 |         'category': 'categorical'
157 |     }
158 |
159 |     return {
160 |         col: type_mapping.get(str(df[col].dtype), 'unknown')
161 |         for col in df.columns
162 |     }
163 |
164 |
165 | def infer_delimiter(sample: str) -> str:
166 |     """Infer CSV delimiter from sample"""
167 |     delimiters = [',', '\t', ';', '|']
168 |     delimiter_counts = {}
169 |
170 |     for delimiter in delimiters:
171 |         delimiter_counts[delimiter] = sample.count(delimiter)
172 |
173 |     # Return delimiter with most occurrences
174 |     return max(delimiter_counts, key=delimiter_counts.get)
175 |
176 |
177 | def is_numeric_dtype(dtype: Any) -> bool:
178 |     """Check if dtype is numeric"""
179 |     return pd.api.types.is_numeric_dtype(dtype)
180 |
181 |
182 | def is_categorical_dtype(dtype: Any) -> bool:
183 |     """Check if dtype is categorical"""
184 |     return pd.api.types.is_categorical_dtype(dtype) or pd.api.types.is_object_dtype(dtype)
185 |
186 |
187 | def is_datetime_dtype(dtype: Any) -> bool:
188 |     """Check if dtype is datetime"""
189 |     return pd.api.types.is_datetime64_any_dtype(dtype)
190 |
191 |
192 | def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
193 |     """Clean column names for compatibility"""
194 |     df = df.copy()
195 |
196 |     # Replace spaces and special characters
197 |     df.columns = df.columns.str.replace(' ', '_')
198 |     df.columns = df.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True)
199 |
200 |     # Ensure column names don't start with numbers
201 |     df.columns = ['col_' + col if col[0].isdigit() else col for col in df.columns]
202 |
203 |     # Handle duplicate columns
204 |     cols = pd.Series(df.columns)
205 |     for dup in cols[cols.duplicated()].unique():
206 |         cols[cols == dup] = [f"{dup}_{i}" if i != 0 else dup for i in range(sum(cols == dup))]
207 |     df.columns = cols
208 |
209 |     return df
210 |
211 |
212 | def calculate_memory_usage(df: pd.DataFrame) -> Dict[str, Any]:
213 |     """Calculate memory usage of DataFrame"""
214 |     memory_usage = df.memory_usage(deep=True)
215 |     total_memory = memory_usage.sum()
216 |
217 |     return {
218 |         'total': format_bytes(total_memory),
219 |         'total_bytes': int(total_memory),
220 |         'per_column': {
221 |             col: format_bytes(memory_usage[col])
222 |             for col in df.columns
223 |         }
224 |     }
225 |
226 |
227 | def validate_json(json_str: str) -> bool:
228 |     """Validate JSON string"""
229 |     try:
230 |         json.loads(json_str)
231 |         return True
232 |     except (json.JSONDecodeError, TypeError):
233 |         return False
234 |
235 |
236 | def merge_dicts(dict1: Dict, dict2: Dict) -> Dict:
237 |     """Deep merge two dictionaries"""
238 |     result = dict1.copy()
239 |
240 |     for key, value in dict2.items():
241 |         if key in result and isinstance(result[key], dict) and isinstance(value, dict):
242 |             result[key] = merge_dicts(result[key], value)
243 |         else:
244 |             result[key] = value
245 |
246 |     return result
247 |
248 |
249 | def format_number(value: float, precision: int = 4) -> str:
250 |     """Format number for display"""
251 |     if pd.isna(value):
252 |         return "N/A"
253 |
254 |     if abs(value) < 1e-4 or abs(value) > 1e6:
255 |         return f"{value:.{precision}e}"
256 |     else:
257 |         return f"{value:.{precision}f}"
258 |
259 |
260 | def create_summary_stats(series: pd.Series) -> Dict[str, Any]:
261 |     """Create summary statistics for a series"""
262 |     if is_numeric_dtype(series.dtype):
263 |         return {
264 |             'count': int(series.count()),
265 |             'mean': float(series.mean()),
266 |             'std': float(series.std()),
267 |             'min': float(series.min()),
268 |             'q1': float(series.quantile(0.25)),
269 |             'median': float(series.median()),
270 |             'q3': float(series.quantile(0.75)),
271 |             'max': float(series.max()),
272 |             'missing': int(series.isna().sum())
273 |         }
274 |     else:
275 |         value_counts = series.value_counts()
276 |         return {
277 |             'count': int(series.count()),
278 |             'unique': int(series.nunique()),
279 |             'top': str(value_counts.index[0]) if len(value_counts) > 0 else None,
280 |             'freq': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
281 |             'missing': int(series.isna().sum())
282 |         }
283 |


--------------------------------------------------------------------------------
/backend/app/utils/validators.py:
--------------------------------------------------------------------------------
  1 | """
  2 | Validators for OpenStatica
  3 | Input validation and data quality checks
  4 | """
  5 |
  6 | from typing import Any, Dict, List, Optional, Union
  7 | import pandas as pd
  8 | import numpy as np
  9 | import re
 10 | from pydantic import BaseModel, validator
 11 | from datetime import datetime
 12 | import logging
 13 |
 14 | logger = logging.getLogger(__name__)
 15 |
 16 |
 17 | class DataValidator:
 18 |     """Validator for data inputs"""
 19 |
 20 |     @staticmethod
 21 |     def validate_dataframe(df: pd.DataFrame) -> Dict[str, Any]:
 22 |         """Validate dataframe and return validation results"""
 23 |         issues = []
 24 |         warnings = []
 25 |
 26 |         # Check if empty
 27 |         if df.empty:
 28 |             issues.append("Dataframe is empty")
 29 |             return {'valid': False, 'issues': issues, 'warnings': warnings}
 30 |
 31 |         # Check dimensions
 32 |         if len(df.columns) == 0:
 33 |             issues.append("No columns in dataframe")
 34 |
 35 |         if len(df) > 1000000:
 36 |             warnings.append(f"Large dataset: {len(df)} rows may impact performance")
 37 |
 38 |         # Check for duplicate columns
 39 |         duplicate_cols = df.columns[df.columns.duplicated()].tolist()
 40 |         if duplicate_cols:
 41 |             issues.append(f"Duplicate column names: {duplicate_cols}")
 42 |
 43 |         # Check for completely empty columns
 44 |         empty_cols = df.columns[df.isna().all()].tolist()
 45 |         if empty_cols:
 46 |             warnings.append(f"Empty columns: {empty_cols}")
 47 |
 48 |         # Check for high missing data
 49 |         missing_pct = df.isna().sum() / len(df)
 50 |         high_missing = missing_pct[missing_pct > 0.5].to_dict()
 51 |         if high_missing:
 52 |             warnings.append(f"Columns with >50% missing: {list(high_missing.keys())}")
 53 |
 54 |         # Check data types
 55 |         mixed_types = []
 56 |         for col in df.select_dtypes(include='object').columns:
 57 |             if df[col].apply(type).nunique() > 1:
 58 |                 mixed_types.append(col)
 59 |
 60 |         if mixed_types:
 61 |             warnings.append(f"Columns with mixed types: {mixed_types}")
 62 |
 63 |         return {
 64 |             'valid': len(issues) == 0,
 65 |             'issues': issues,
 66 |             'warnings': warnings,
 67 |             'shape': df.shape,
 68 |             'dtypes': df.dtypes.astype(str).to_dict()
 69 |         }
 70 |
 71 |     @staticmethod
 72 |     def validate_column_names(columns: List[str]) -> Dict[str, Any]:
 73 |         """Validate column names"""
 74 |         issues = []
 75 |         cleaned = []
 76 |
 77 |         for col in columns:
 78 |             # Check for invalid characters
 79 |             if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*
#39;, col):
 80 |                 issues.append(f"Invalid column name: {col}")
 81 |                 # Clean the name
 82 |                 clean_col = re.sub(r'[^a-zA-Z0-9_]', '_', col)
 83 |                 if not clean_col[0].isalpha():
 84 |                     clean_col = 'col_' + clean_col
 85 |                 cleaned.append(clean_col)
 86 |             else:
 87 |                 cleaned.append(col)
 88 |
 89 |         return {
 90 |             'valid': len(issues) == 0,
 91 |             'issues': issues,
 92 |             'cleaned': cleaned
 93 |         }
 94 |
 95 |     @staticmethod
 96 |     def validate_numeric_data(data: Union[pd.Series, np.ndarray]) -> Dict[str, Any]:
 97 |         """Validate numeric data"""
 98 |         issues = []
 99 |         warnings = []
100 |
101 |         if isinstance(data, pd.Series):
102 |             data = data.values
103 |
104 |         # Check for infinite values
105 |         if np.any(np.isinf(data)):
106 |             issues.append("Data contains infinite values")
107 |
108 |         # Check for NaN
109 |         nan_count = np.sum(np.isnan(data))
110 |         if nan_count > 0:
111 |             warnings.append(f"Data contains {nan_count} NaN values")
112 |
113 |         # Check range
114 |         if len(data) > 0:
115 |             data_clean = data[~np.isnan(data)]
116 |             if len(data_clean) > 0:
117 |                 data_range = np.ptp(data_clean)
118 |                 if data_range == 0:
119 |                     warnings.append("Data has zero variance")
120 |                 elif data_range > 1e10:
121 |                     warnings.append("Very large data range may cause numerical issues")
122 |
123 |         return {
124 |             'valid': len(issues) == 0,
125 |             'issues': issues,
126 |             'warnings': warnings
127 |         }
128 |
129 |     @staticmethod
130 |     def validate_categorical_data(data: pd.Series) -> Dict[str, Any]:
131 |         """Validate categorical data"""
132 |         issues = []
133 |         warnings = []
134 |
135 |         # Check cardinality
136 |         unique_ratio = data.nunique() / len(data)
137 |
138 |         if unique_ratio > 0.9:
139 |             warnings.append("High cardinality - might be better as numeric or ID")
140 |
141 |         if unique_ratio == 1:
142 |             warnings.append("Each value is unique - not suitable for categorical analysis")
143 |
144 |         # Check for empty strings
145 |         if data.dtype == 'object':
146 |             empty_count = (data == '').sum()
147 |             if empty_count > 0:
148 |                 warnings.append(f"Contains {empty_count} empty strings")
149 |
150 |         return {
151 |             'valid': len(issues) == 0,
152 |             'issues': issues,
153 |             'warnings': warnings,
154 |             'n_unique': data.nunique(),
155 |             'unique_ratio': unique_ratio
156 |         }
157 |
158 |
159 | class FileValidator:
160 |     """Validator for file uploads"""
161 |
162 |     ALLOWED_EXTENSIONS = {'.csv', '.xlsx', '.xls', '.json', '.parquet', '.sav', '.dta', '.sas7bdat'}
163 |     MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
164 |
165 |     @classmethod
166 |     def validate_file(cls, filename: str, file_size: int) -> Dict[str, Any]:
167 |         """Validate uploaded file"""
168 |         issues = []
169 |
170 |         # Check extension
171 |         import os
172 |         _, ext = os.path.splitext(filename.lower())
173 |
174 |         if ext not in cls.ALLOWED_EXTENSIONS:
175 |             issues.append(f"File type {ext} not supported")
176 |
177 |         # Check size
178 |         if file_size > cls.MAX_FILE_SIZE:
179 |             issues.append(f"File too large: {file_size / 1024 / 1024:.2f}MB (max: 100MB)")
180 |
181 |         if file_size == 0:
182 |             issues.append("File is empty")
183 |
184 |         return {
185 |             'valid': len(issues) == 0,
186 |             'issues': issues,
187 |             'extension': ext,
188 |             'size_mb': file_size / 1024 / 1024
189 |         }
190 |
191 |
192 | class AnalysisValidator:
193 |     """Validator for analysis parameters"""
194 |
195 |     @staticmethod
196 |     def validate_test_assumptions(test_type: str, data: pd.DataFrame, **params) -> Dict[str, Any]:
197 |         """Validate statistical test assumptions"""
198 |         assumptions = {}
199 |
200 |         if test_type == 'ttest':
201 |             # Check normality
202 |             from scipy import stats
203 |
204 |             var = params.get('variable')
205 |             if var and var in data.columns:
206 |                 _, p_value = stats.shapiro(data[var].dropna()[:5000])
207 |                 assumptions['normality'] = {
208 |                     'test': 'Shapiro-Wilk',
209 |                     'p_value': p_value,
210 |                     'passed': p_value > 0.05
211 |                 }
212 |
213 |             # Check sample size
214 |             assumptions['sample_size'] = {
215 |                 'n': len(data),
216 |                 'adequate': len(data) >= 30
217 |             }
218 |
219 |         elif test_type == 'anova':
220 |             # Check homogeneity of variance
221 |             groups = params.get('groups', [])
222 |             if len(groups) >= 2:
223 |                 from scipy.stats import levene
224 |                 group_data = [data[g].dropna() for g in groups if g in data.columns]
225 |                 if len(group_data) >= 2:
226 |                     stat, p_value = levene(*group_data)
227 |                     assumptions['homogeneity'] = {
228 |                         'test': 'Levene',
229 |                         'p_value': p_value,
230 |                         'passed': p_value > 0.05
231 |                     }
232 |
233 |         elif test_type == 'regression':
234 |             # Check multicollinearity
235 |             from statsmodels.stats.outliers_influence import variance_inflation_factor
236 |
237 |             predictors = params.get('predictors', [])
238 |             if len(predictors) > 1:
239 |                 X = data[predictors].dropna()
240 |                 vif_data = pd.DataFrame()
241 |                 vif_data["Variable"] = predictors
242 |                 vif_data["VIF"] = [variance_inflation_factor(X.values, i)
243 |                                    for i in range(X.shape[1])]
244 |
245 |                 assumptions['multicollinearity'] = {
246 |                     'vif': vif_data.to_dict(),
247 |                     'passed': all(vif_data["VIF"] < 10)
248 |                 }
249 |
250 |         return assumptions
251 |
252 |     @staticmethod
253 |     def validate_ml_parameters(task: str, algorithm: str, params: Dict[str, Any]) -> Dict[str, Any]:
254 |         """Validate ML parameters"""
255 |         issues = []
256 |         warnings = []
257 |
258 |         # Check train/test split
259 |         train_size = params.get('train_test_split', 0.8)
260 |         if not 0.5 <= train_size <= 0.95:
261 |             warnings.append(f"Unusual train/test split: {train_size}")
262 |
263 |         # Check cross-validation folds
264 |         cv_folds = params.get('cv_folds', 5)
265 |         if cv_folds < 2:
266 |             issues.append("Cross-validation requires at least 2 folds")
267 |         elif cv_folds > 20:
268 |             warnings.append("High number of CV folds may be slow")
269 |
270 |         # Algorithm-specific checks
271 |         if algorithm == 'kmeans':
272 |             n_clusters = params.get('n_clusters', 3)
273 |             if n_clusters < 2:
274 |                 issues.append("K-means requires at least 2 clusters")
275 |             elif n_clusters > 20:
276 |                 warnings.append("High number of clusters may lead to overfitting")
277 |
278 |         elif algorithm in ['rf', 'random_forest']:
279 |             n_estimators = params.get('n_estimators', 100)
280 |             if n_estimators < 10:
281 |                 warnings.append("Low number of trees may underfit")
282 |             elif n_estimators > 1000:
283 |                 warnings.append("High number of trees may be slow")
284 |
285 |         return {
286 |             'valid': len(issues) == 0,
287 |             'issues': issues,
288 |             'warnings': warnings
289 |         }
290 |
291 |
292 | class RequestValidator:
293 |     """Validator for API requests"""
294 |
295 |     @staticmethod
296 |     def validate_session_id(session_id: str) -> bool:
297 |         """Validate session ID format"""
298 |         # UUID format
299 |         uuid_pattern = re.compile(
300 |             r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}
#39;,
301 |             re.IGNORECASE
302 |         )
303 |         return bool(uuid_pattern.match(session_id))
304 |
305 |     @staticmethod
306 |     def validate_variable_selection(variables: List[str],
307 |                                     available: List[str]) -> Dict[str, Any]:
308 |         """Validate variable selection"""
309 |         issues = []
310 |
311 |         if not variables:
312 |             issues.append("No variables selected")
313 |
314 |         missing = set(variables) - set(available)
315 |         if missing:
316 |             issues.append(f"Variables not found: {list(missing)}")
317 |
318 |         if len(variables) > 100:
319 |             issues.append("Too many variables selected (max: 100)")
320 |
321 |         return {
322 |             'valid': len(issues) == 0,
323 |             'issues': issues,
324 |             'selected': len(variables),
325 |             'missing': list(missing)
326 |         }
327 |
328 |
329 | class OutputValidator:
330 |     """Validator for outputs and results"""
331 |
332 |     @staticmethod
333 |     def validate_results(results: Dict[str, Any], expected_keys: List[str]) -> Dict[str, Any]:
334 |         """Validate analysis results structure"""
335 |         issues = []
336 |
337 |         missing_keys = set(expected_keys) - set(results.keys())
338 |         if missing_keys:
339 |             issues.append(f"Missing expected keys: {list(missing_keys)}")
340 |
341 |         # Check for None values in critical fields
342 |         for key in expected_keys:
343 |             if key in results and results[key] is None:
344 |                 issues.append(f"Null value for required field: {key}")
345 |
346 |         return {
347 |             'valid': len(issues) == 0,
348 |             'issues': issues
349 |         }
350 |
351 |     @staticmethod
352 |     def sanitize_output(data: Any) -> Any:
353 |         """Sanitize output data"""
354 |         if isinstance(data, dict):
355 |             return {k: OutputValidator.sanitize_output(v) for k, v in data.items()}
356 |         elif isinstance(data, list):
357 |             return [OutputValidator.sanitize_output(v) for v in data]
358 |         elif isinstance(data, float):
359 |             if np.isnan(data):
360 |                 return None
361 |             elif np.isinf(data):
362 |                 return str(data)
363 |             return data
364 |         elif isinstance(data, np.ndarray):
365 |             return OutputValidator.sanitize_output(data.tolist())
366 |         elif pd.isna(data):
367 |             return None
368 |         return data
369 |
370 |
371 | # Pydantic models for request validation
372 | class DataUploadValidator(BaseModel):
373 |     """Validator for data upload requests"""
374 |     filename: str
375 |     format: str
376 |
377 |     @validator('format')
378 |     def validate_format(cls, v):
379 |         allowed = ['csv', 'excel', 'json', 'parquet', 'spss', 'stata', 'sas']
380 |         if v not in allowed:
381 |             raise ValueError(f'Format must be one of {allowed}')
382 |         return v
383 |
384 |
385 | class AnalysisRequestValidator(BaseModel):
386 |     """Validator for analysis requests"""
387 |     session_id: str
388 |     analysis_type: str
389 |     variables: List[str]
390 |     options: Dict[str, Any] = {}
391 |
392 |     @validator('session_id')
393 |     def validate_session(cls, v):
394 |         if not RequestValidator.validate_session_id(v):
395 |             raise ValueError('Invalid session ID format')
396 |         return v
397 |
398 |     @validator('variables')
399 |     def validate_variables(cls, v):
400 |         if not v:
401 |             raise ValueError('At least one variable must be selected')
402 |         if len(v) > 100:
403 |             raise ValueError('Too many variables (max: 100)')
404 |         return v
405 |
406 |
407 | class MLRequestValidator(BaseModel):
408 |     """Validator for ML requests"""
409 |     session_id: str
410 |     task: str
411 |     algorithm: str
412 |     features: List[str]
413 |     target: Optional[str] = None
414 |
415 |     @validator('task')
416 |     def validate_task(cls, v):
417 |         allowed = ['classification', 'regression', 'clustering',
418 |                    'dimensionality', 'anomaly']
419 |         if v not in allowed:
420 |             raise ValueError(f'Task must be one of {allowed}')
421 |         return v
422 |
423 |     @validator('features')
424 |     def validate_features(cls, v):
425 |         if not v:
426 |             raise ValueError('At least one feature must be selected')
427 |         return v
428 |
429 |     @validator('target')
430 |     def validate_target(cls, v, values):
431 |         task = values.get('task')
432 |         if task in ['classification', 'regression'] and not v:
433 |             raise ValueError(f'Target variable required for {task}')
434 |         return v
435 |
436 |
437 | def validate_input(validator_class: BaseModel, data: Dict[str, Any]) -> Dict[str, Any]:
438 |     """Generic input validation using Pydantic models"""
439 |     try:
440 |         validated = validator_class(**data)
441 |         return {'valid': True, 'data': validated.dict()}
442 |     except Exception as e:
443 |         return {'valid': False, 'errors': str(e)}
444 |


--------------------------------------------------------------------------------
/backend/constraints.txt:
--------------------------------------------------------------------------------
1 | numpy==1.26.4
2 | scipy==1.12.0
3 | pmdarima==2.0.4
4 |


--------------------------------------------------------------------------------
/backend/requirements-dev.txt:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/backend/requirements-dev.txt


--------------------------------------------------------------------------------
/backend/requirements-ml.txt:
--------------------------------------------------------------------------------
 1 | # Deep Learning Frameworks (optional)
 2 | torch
 3 | tensorflow
 4 | jax[cpu]
 5 |
 6 | # ML Libraries
 7 | xgboost
 8 | lightgbm
 9 | catboost
10 |
11 | # NLP & Vision
12 | transformers
13 | tokenizers
14 | datasets
15 | Pillow
16 |
17 | # Model Management
18 | mlflow
19 | optuna
20 | wandb
21 |
22 | # Additional ML Tools
23 | imbalanced-learn
24 | shap
25 | lime
26 |


--------------------------------------------------------------------------------
/backend/requirements.txt:
--------------------------------------------------------------------------------
 1 | # Core
 2 | fastapi
 3 | uvicorn[standard]
 4 | pydantic
 5 | pydantic_settings
 6 | python-multipart
 7 |
 8 | # Data Processing
 9 | pandas
10 | polars
11 | pyarrow
12 | openpyxl
13 |
14 | # Statistics
15 | statsmodels
16 | pingouin
17 | scikit-learn
18 | factor-analyzer
19 |
20 | # Visualization
21 | matplotlib
22 | seaborn
23 | plotly
24 |
25 | # API & Database
26 | redis
27 | sqlalchemy
28 | alembic
29 | asyncpg
30 |
31 | # Utilities
32 | python-dotenv
33 | pyyaml
34 | click
35 | rich
36 |
37 | # Testing
38 | pytest
39 | pytest-asyncio
40 | httpx
41 |


--------------------------------------------------------------------------------
/docker-compose.yml:
--------------------------------------------------------------------------------
 1 | version: '3.8'
 2 |
 3 | services:
 4 |   backend:
 5 |     build:
 6 |       context: ./backend
 7 |       dockerfile: Dockerfile
 8 |     container_name: openstatica-backend
 9 |     ports:
10 |       - "8000:8000"
11 |     environment:
12 |       - DATABASE_URL=postgresql://openstatica:password@db:5432/openstatica
13 |       - REDIS_URL=redis://redis:6379
14 |       - SECRET_KEY=${SECRET_KEY:-your-secret-key-here}
15 |       - ENABLE_ML=true
16 |       - ENABLE_GPU=false
17 |     volumes:
18 |       - ./backend:/app
19 |       - ./uploads:/app/uploads
20 |       - ./models:/app/models
21 |       - ./plugins:/app/plugins
22 |     depends_on:
23 |       - db
24 |       - redis
25 |     command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
26 |
27 |   frontend:
28 |     image: nginx:alpine
29 |     container_name: openstatica-frontend
30 |     ports:
31 |       - "3000:80"
32 |     volumes:
33 |       - ./frontend:/usr/share/nginx/html
34 |       - ./nginx.conf:/etc/nginx/nginx.conf
35 |     depends_on:
36 |       - backend
37 |
38 |   db:
39 |     image: postgres:14-alpine
40 |     container_name: openstatica-db
41 |     environment:
42 |       - POSTGRES_USER=openstatica
43 |       - POSTGRES_PASSWORD=password
44 |       - POSTGRES_DB=openstatica
45 |     volumes:
46 |       - postgres_data:/var/lib/postgresql/data
47 |     ports:
48 |       - "5432:5432"
49 |
50 |   redis:
51 |     image: redis:7-alpine
52 |     container_name: openstatica-redis
53 |     ports:
54 |       - "6379:6379"
55 |     volumes:
56 |       - redis_data:/data
57 |
58 | volumes:
59 |   postgres_data:
60 |   redis_data:


--------------------------------------------------------------------------------
/docs/API.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/docs/API.md


--------------------------------------------------------------------------------
/docs/ARCHITECTURE.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/docs/ARCHITECTURE.md


--------------------------------------------------------------------------------
/docs/ML_GUIDE.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/docs/ML_GUIDE.md


--------------------------------------------------------------------------------
/docs/PLUGINS.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/docs/PLUGINS.md


--------------------------------------------------------------------------------
/frontend/assets/logo.svg:
--------------------------------------------------------------------------------
 1 | <svg width="512" height="512" viewBox="0 0 512 512" fill="none" xmlns="http://www.w3.org/2000/svg">
 2 |     <rect width="512" height="512" rx="100" fill="url(#gradient)"/>
 3 |     <path d="M100 256 L150 200 L200 350 L250 150 L300 350 L350 200 L400 256"
 4 |           stroke="white" stroke-width="20" stroke-linecap="round" stroke-linejoin="round"/>
 5 |     <circle cx="100" cy="256" r="15" fill="white"/>
 6 |     <circle cx="150" cy="200" r="15" fill="white"/>
 7 |     <circle cx="200" cy="350" r="15" fill="white"/>
 8 |     <circle cx="250" cy="150" r="15" fill="white"/>
 9 |     <circle cx="300" cy="350" r="15" fill="white"/>
10 |     <circle cx="350" cy="200" r="15" fill="white"/>
11 |     <circle cx="400" cy="256" r="15" fill="white"/>
12 |     <defs>
13 |         <linearGradient id="gradient" x1="0" y1="0" x2="512" y2="512">
14 |             <stop offset="0%" stop-color="#6366f1"/>
15 |             <stop offset="100%" stop-color="#8b5cf6"/>
16 |         </linearGradient>
17 |     </defs>
18 | </svg>


--------------------------------------------------------------------------------
/frontend/css/styles.css:
--------------------------------------------------------------------------------
   1 | /* OpenStatica - Complete Styles */
   2 |
   3 | :root {
   4 |     /* Colors */
   5 |     --primary: #6366f1;
   6 |     --primary-dark: #4f46e5;
   7 |     --primary-light: #818cf8;
   8 |     --secondary: #8b5cf6;
   9 |     --success: #10b981;
  10 |     --warning: #f59e0b;
  11 |     --danger: #ef4444;
  12 |     --info: #06b6d4;
  13 |
  14 |     /* Grays */
  15 |     --gray-50: #f9fafb;
  16 |     --gray-100: #f3f4f6;
  17 |     --gray-200: #e5e7eb;
  18 |     --gray-300: #d1d5db;
  19 |     --gray-400: #9ca3af;
  20 |     --gray-500: #6b7280;
  21 |     --gray-600: #4b5563;
  22 |     --gray-700: #374151;
  23 |     --gray-800: #1f2937;
  24 |     --gray-900: #111827;
  25 |
  26 |     /* Layout */
  27 |     --sidebar-width: 320px;
  28 |     --header-height: 64px;
  29 |     --border-radius: 8px;
  30 |     --transition: all 0.3s ease;
  31 |
  32 |     /* Shadows */
  33 |     --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
  34 |     --shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1), 0 1px 2px 0 rgba(0, 0, 0, 0.06);
  35 |     --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
  36 |     --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
  37 |     --shadow-xl: 0 20px 25px -5px rgba(0, 0, 0, 0.1), 0 10px 10px -5px rgba(0, 0, 0, 0.04);
  38 | }
  39 |
  40 | /* Reset & Base */
  41 | * {
  42 |     margin: 0;
  43 |     padding: 0;
  44 |     box-sizing: border-box;
  45 | }
  46 |
  47 | html {
  48 |     font-size: 16px;
  49 |     -webkit-font-smoothing: antialiased;
  50 |     -moz-osx-font-smoothing: grayscale;
  51 | }
  52 |
  53 | body {
  54 |     font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
  55 |     background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  56 |     color: var(--gray-900);
  57 |     line-height: 1.6;
  58 |     min-height: 100vh;
  59 | }
  60 |
  61 | /* Typography */
  62 | h1, h2, h3, h4, h5, h6 {
  63 |     font-weight: 600;
  64 |     line-height: 1.2;
  65 |     margin-bottom: 1rem;
  66 | }
  67 |
  68 | h1 {
  69 |     font-size: 2.5rem;
  70 | }
  71 |
  72 | h2 {
  73 |     font-size: 2rem;
  74 | }
  75 |
  76 | h3 {
  77 |     font-size: 1.5rem;
  78 | }
  79 |
  80 | h4 {
  81 |     font-size: 1.25rem;
  82 | }
  83 |
  84 | h5 {
  85 |     font-size: 1rem;
  86 | }
  87 |
  88 | h6 {
  89 |     font-size: 0.875rem;
  90 | }
  91 |
  92 | p {
  93 |     margin-bottom: 1rem;
  94 | }
  95 |
  96 | /* Layout Components */
  97 | .app-container {
  98 |     display: flex;
  99 |     flex-direction: column;
 100 |     height: 100vh;
 101 |     background: white;
 102 | }
 103 |
 104 | /* Header */
 105 | .header {
 106 |     background: white;
 107 |     border-bottom: 1px solid var(--gray-200);
 108 |     height: var(--header-height);
 109 |     position: sticky;
 110 |     top: 0;
 111 |     z-index: 1000;
 112 |     box-shadow: var(--shadow);
 113 | }
 114 |
 115 | .header-content {
 116 |     display: flex;
 117 |     align-items: center;
 118 |     justify-content: space-between;
 119 |     height: 100%;
 120 |     padding: 0 2rem;
 121 |     max-width: 100%;
 122 | }
 123 |
 124 | .logo {
 125 |     display: flex;
 126 |     align-items: center;
 127 |     gap: 0.75rem;
 128 |     color: var(--primary);
 129 |     font-weight: 700;
 130 |     font-size: 1.5rem;
 131 | }
 132 |
 133 | .logo svg {
 134 |     width: 32px;
 135 |     height: 32px;
 136 | }
 137 |
 138 | /* Navigation */
 139 | .nav {
 140 |     display: flex;
 141 |     gap: 0.5rem;
 142 |     flex: 1;
 143 |     justify-content: center;
 144 | }
 145 |
 146 | .nav-btn {
 147 |     padding: 0.5rem 1rem;
 148 |     background: transparent;
 149 |     border: none;
 150 |     border-radius: var(--border-radius);
 151 |     color: var(--gray-600);
 152 |     font-weight: 500;
 153 |     cursor: pointer;
 154 |     transition: var(--transition);
 155 |     position: relative;
 156 | }
 157 |
 158 | .nav-btn:hover {
 159 |     background: var(--gray-100);
 160 |     color: var(--gray-900);
 161 | }
 162 |
 163 | .nav-btn.active {
 164 |     background: var(--primary);
 165 |     color: white;
 166 | }
 167 |
 168 | .nav-btn.active::after {
 169 |     content: '';
 170 |     position: absolute;
 171 |     bottom: -1px;
 172 |     left: 0;
 173 |     right: 0;
 174 |     height: 2px;
 175 |     background: var(--primary);
 176 | }
 177 |
 178 | /* Main Layout */
 179 | .main-layout {
 180 |     display: flex;
 181 |     flex: 1;
 182 |     overflow: hidden;
 183 | }
 184 |
 185 | /* Sidebar */
 186 | .sidebar {
 187 |     width: var(--sidebar-width);
 188 |     background: var(--gray-50);
 189 |     border-right: 1px solid var(--gray-200);
 190 |     overflow-y: auto;
 191 |     padding: 1.5rem;
 192 |     flex-shrink: 0;
 193 | }
 194 |
 195 | .sidebar-section {
 196 |     margin-bottom: 2rem;
 197 | }
 198 |
 199 | .sidebar-section h3 {
 200 |     font-size: 0.875rem;
 201 |     font-weight: 600;
 202 |     text-transform: uppercase;
 203 |     color: var(--gray-500);
 204 |     margin-bottom: 1rem;
 205 |     letter-spacing: 0.5px;
 206 | }
 207 |
 208 | /* Upload Area */
 209 | .upload-area {
 210 |     border: 2px dashed var(--gray-300);
 211 |     border-radius: var(--border-radius);
 212 |     padding: 2rem;
 213 |     text-align: center;
 214 |     cursor: pointer;
 215 |     transition: var(--transition);
 216 |     background: white;
 217 | }
 218 |
 219 | .upload-area:hover {
 220 |     border-color: var(--primary);
 221 |     background: rgba(99, 102, 241, 0.05);
 222 | }
 223 |
 224 | .upload-area.dragover {
 225 |     border-color: var(--primary);
 226 |     background: rgba(99, 102, 241, 0.1);
 227 |     transform: scale(1.02);
 228 | }
 229 |
 230 | .upload-area svg {
 231 |     width: 48px;
 232 |     height: 48px;
 233 |     color: var(--gray-400);
 234 |     margin-bottom: 1rem;
 235 | }
 236 |
 237 | .upload-area p {
 238 |     color: var(--gray-600);
 239 |     font-size: 0.875rem;
 240 | }
 241 |
 242 | /* Content Area */
 243 | .content {
 244 |     flex: 1;
 245 |     background: white;
 246 |     overflow-y: auto;
 247 |     padding: 2rem;
 248 | }
 249 |
 250 | .view-panel {
 251 |     display: none;
 252 |     animation: fadeIn 0.3s ease;
 253 | }
 254 |
 255 | .view-panel.active {
 256 |     display: block;
 257 | }
 258 |
 259 | @keyframes fadeIn {
 260 |     from {
 261 |         opacity: 0;
 262 |         transform: translateY(10px);
 263 |     }
 264 |     to {
 265 |         opacity: 1;
 266 |         transform: translateY(0);
 267 |     }
 268 | }
 269 |
 270 | .panel-header {
 271 |     display: flex;
 272 |     justify-content: space-between;
 273 |     align-items: center;
 274 |     margin-bottom: 2rem;
 275 |     padding-bottom: 1rem;
 276 |     border-bottom: 2px solid var(--gray-100);
 277 | }
 278 |
 279 | .panel-header h2 {
 280 |     font-size: 1.5rem;
 281 |     font-weight: 600;
 282 |     color: var(--gray-900);
 283 |     margin: 0;
 284 | }
 285 |
 286 | /* Data Table */
 287 | .data-table-container {
 288 |     background: white;
 289 |     border-radius: var(--border-radius);
 290 |     overflow: hidden;
 291 |     box-shadow: var(--shadow);
 292 | }
 293 |
 294 | .data-table-wrapper {
 295 |     overflow-x: auto;
 296 | }
 297 |
 298 | .data-table {
 299 |     width: 100%;
 300 |     border-collapse: collapse;
 301 | }
 302 |
 303 | .data-table thead {
 304 |     background: var(--gray-50);
 305 |     border-bottom: 2px solid var(--gray-200);
 306 | }
 307 |
 308 | .data-table th {
 309 |     padding: 0.75rem 1rem;
 310 |     text-align: left;
 311 |     font-weight: 600;
 312 |     font-size: 0.875rem;
 313 |     color: var(--gray-700);
 314 |     white-space: nowrap;
 315 | }
 316 |
 317 | .data-table tbody tr {
 318 |     border-bottom: 1px solid var(--gray-100);
 319 |     transition: var(--transition);
 320 | }
 321 |
 322 | .data-table tbody tr:hover {
 323 |     background: var(--gray-50);
 324 | }
 325 |
 326 | .data-table td {
 327 |     padding: 0.75rem 1rem;
 328 |     font-size: 0.875rem;
 329 |     color: var(--gray-800);
 330 | }
 331 |
 332 | .column-header {
 333 |     display: flex;
 334 |     flex-direction: column;
 335 |     gap: 0.25rem;
 336 | }
 337 |
 338 | .column-type {
 339 |     font-size: 0.75rem;
 340 |     color: var(--gray-500);
 341 |     font-weight: normal;
 342 | }
 343 |
 344 | .row-number {
 345 |     background: var(--gray-50);
 346 |     font-weight: 500;
 347 |     color: var(--gray-500);
 348 |     text-align: center;
 349 | }
 350 |
 351 | /* Cell Types */
 352 | .null-value {
 353 |     color: var(--gray-400);
 354 |     font-style: italic;
 355 | }
 356 |
 357 | .numeric-value {
 358 |     text-align: right;
 359 |     font-variant-numeric: tabular-nums;
 360 | }
 361 |
 362 | .boolean-value {
 363 |     text-align: center;
 364 | }
 365 |
 366 | /* Variables */
 367 | .variable-list {
 368 |     max-height: 400px;
 369 |     overflow-y: auto;
 370 | }
 371 |
 372 | .variable-group {
 373 |     margin-bottom: 1.5rem;
 374 | }
 375 |
 376 | .variable-group-header {
 377 |     display: flex;
 378 |     justify-content: space-between;
 379 |     align-items: center;
 380 |     margin-bottom: 0.5rem;
 381 |     font-weight: 500;
 382 |     color: var(--gray-700);
 383 | }
 384 |
 385 | .variable-item {
 386 |     display: flex;
 387 |     align-items: center;
 388 |     padding: 0.5rem;
 389 |     margin-bottom: 0.25rem;
 390 |     border-radius: 0.25rem;
 391 |     cursor: pointer;
 392 |     transition: var(--transition);
 393 | }
 394 |
 395 | .variable-item:hover {
 396 |     background: var(--gray-100);
 397 | }
 398 |
 399 | .variable-item input[type="checkbox"] {
 400 |     margin-right: 0.5rem;
 401 | }
 402 |
 403 | .variable-item label {
 404 |     flex: 1;
 405 |     cursor: pointer;
 406 |     font-size: 0.875rem;
 407 | }
 408 |
 409 | .variable-type {
 410 |     font-size: 0.75rem;
 411 |     padding: 0.125rem 0.5rem;
 412 |     border-radius: 0.25rem;
 413 |     background: var(--gray-200);
 414 |     color: var(--gray-600);
 415 | }
 416 |
 417 | /* Forms */
 418 | .form-group {
 419 |     margin-bottom: 1.5rem;
 420 | }
 421 |
 422 | .form-label {
 423 |     display: block;
 424 |     font-size: 0.875rem;
 425 |     font-weight: 500;
 426 |     color: var(--gray-700);
 427 |     margin-bottom: 0.5rem;
 428 | }
 429 |
 430 | .form-control {
 431 |     width: 100%;
 432 |     padding: 0.625rem 0.75rem;
 433 |     border: 1px solid var(--gray-300);
 434 |     border-radius: 0.375rem;
 435 |     font-size: 0.875rem;
 436 |     transition: var(--transition);
 437 |     background: white;
 438 | }
 439 |
 440 | .form-control:focus {
 441 |     outline: none;
 442 |     border-color: var(--primary);
 443 |     box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1);
 444 | }
 445 |
 446 | .form-checkbox {
 447 |     display: flex;
 448 |     align-items: center;
 449 |     gap: 0.5rem;
 450 |     cursor: pointer;
 451 | }
 452 |
 453 | .form-checkbox input[type="checkbox"] {
 454 |     width: 1.25rem;
 455 |     height: 1.25rem;
 456 |     cursor: pointer;
 457 | }
 458 |
 459 | .form-range {
 460 |     width: 100%;
 461 |     -webkit-appearance: none;
 462 |     appearance: none;
 463 |     height: 0.5rem;
 464 |     border-radius: 0.25rem;
 465 |     background: var(--gray-200);
 466 |     outline: none;
 467 | }
 468 |
 469 | .form-range::-webkit-slider-thumb {
 470 |     -webkit-appearance: none;
 471 |     appearance: none;
 472 |     width: 1.25rem;
 473 |     height: 1.25rem;
 474 |     border-radius: 50%;
 475 |     background: var(--primary);
 476 |     cursor: pointer;
 477 | }
 478 |
 479 | .form-range::-moz-range-thumb {
 480 |     width: 1.25rem;
 481 |     height: 1.25rem;
 482 |     border-radius: 50%;
 483 |     background: var(--primary);
 484 |     cursor: pointer;
 485 | }
 486 |
 487 | /* Buttons */
 488 | .btn {
 489 |     padding: 0.625rem 1.25rem;
 490 |     border: none;
 491 |     border-radius: var(--border-radius);
 492 |     font-weight: 500;
 493 |     cursor: pointer;
 494 |     transition: var(--transition);
 495 |     font-size: 0.875rem;
 496 |     display: inline-flex;
 497 |     align-items: center;
 498 |     gap: 0.5rem;
 499 | }
 500 |
 501 | .btn-primary {
 502 |     background: var(--primary);
 503 |     color: white;
 504 | }
 505 |
 506 | .btn-primary:hover {
 507 |     background: var(--primary-dark);
 508 |     transform: translateY(-1px);
 509 |     box-shadow: var(--shadow-md);
 510 | }
 511 |
 512 | .btn-secondary {
 513 |     background: var(--gray-200);
 514 |     color: var(--gray-700);
 515 | }
 516 |
 517 | .btn-secondary:hover {
 518 |     background: var(--gray-300);
 519 | }
 520 |
 521 | .btn-success {
 522 |     background: var(--success);
 523 |     color: white;
 524 | }
 525 |
 526 | .btn-danger {
 527 |     background: var(--danger);
 528 |     color: white;
 529 | }
 530 |
 531 | .btn-text {
 532 |     background: none;
 533 |     border: none;
 534 |     color: var(--primary);
 535 |     font-weight: 500;
 536 |     cursor: pointer;
 537 |     padding: 0.25rem 0.5rem;
 538 |     transition: var(--transition);
 539 | }
 540 |
 541 | .btn-text:hover {
 542 |     text-decoration: underline;
 543 | }
 544 |
 545 | .btn-icon {
 546 |     width: 2.5rem;
 547 |     height: 2.5rem;
 548 |     display: flex;
 549 |     align-items: center;
 550 |     justify-content: center;
 551 |     background: transparent;
 552 |     border: 1px solid var(--gray-300);
 553 |     border-radius: var(--border-radius);
 554 |     color: var(--gray-600);
 555 |     cursor: pointer;
 556 |     transition: var(--transition);
 557 | }
 558 |
 559 | .btn-icon:hover {
 560 |     background: var(--gray-100);
 561 |     border-color: var(--gray-400);
 562 | }
 563 |
 564 | .btn-icon-sm {
 565 |     width: 1.5rem;
 566 |     height: 1.5rem;
 567 |     padding: 0;
 568 |     border: none;
 569 |     background: transparent;
 570 |     cursor: pointer;
 571 | }
 572 |
 573 | .btn:disabled {
 574 |     opacity: 0.5;
 575 |     cursor: not-allowed;
 576 | }
 577 |
 578 | .btn-sm {
 579 |     padding: 0.375rem 0.75rem;
 580 |     font-size: 0.75rem;
 581 | }
 582 |
 583 | /* Results */
 584 | .results-container {
 585 |     display: grid;
 586 |     gap: 1.5rem;
 587 | }
 588 |
 589 | .result-card {
 590 |     background: white;
 591 |     border-radius: var(--border-radius);
 592 |     padding: 1.5rem;
 593 |     box-shadow: var(--shadow);
 594 |     border: 1px solid var(--gray-200);
 595 | }
 596 |
 597 | .result-card h3 {
 598 |     font-size: 1.125rem;
 599 |     font-weight: 600;
 600 |     margin-bottom: 1rem;
 601 |     color: var(--gray-900);
 602 |     padding-bottom: 0.5rem;
 603 |     border-bottom: 2px solid var(--gray-100);
 604 | }
 605 |
 606 | .stat-grid {
 607 |     display: grid;
 608 |     grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
 609 |     gap: 1rem;
 610 | }
 611 |
 612 | .stat-item {
 613 |     padding: 1rem;
 614 |     background: var(--gray-50);
 615 |     border-radius: 0.5rem;
 616 |     border-left: 4px solid var(--primary);
 617 | }
 618 |
 619 | .stat-label {
 620 |     font-size: 0.75rem;
 621 |     text-transform: uppercase;
 622 |     color: var(--gray-500);
 623 |     margin-bottom: 0.25rem;
 624 |     letter-spacing: 0.5px;
 625 | }
 626 |
 627 | .stat-value {
 628 |     font-size: 1.5rem;
 629 |     font-weight: 600;
 630 |     color: var(--gray-900);
 631 | }
 632 |
 633 | /* Charts */
 634 | .chart-container {
 635 |     position: relative;
 636 |     height: 400px;
 637 |     background: white;
 638 |     border-radius: var(--border-radius);
 639 |     padding: 1.5rem;
 640 |     margin-bottom: 1.5rem;
 641 | }
 642 |
 643 | /* Model Results */
 644 | .model-card {
 645 |     background: white;
 646 |     border-radius: var(--border-radius);
 647 |     padding: 1.5rem;
 648 |     box-shadow: var(--shadow);
 649 | }
 650 |
 651 | .model-metrics {
 652 |     margin: 1.5rem 0;
 653 | }
 654 |
 655 | .metrics-grid {
 656 |     display: grid;
 657 |     grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
 658 |     gap: 1rem;
 659 | }
 660 |
 661 | .metric-item {
 662 |     padding: 1rem;
 663 |     background: var(--gray-50);
 664 |     border-radius: 0.5rem;
 665 |     text-align: center;
 666 | }
 667 |
 668 | .metric-label {
 669 |     font-size: 0.75rem;
 670 |     color: var(--gray-500);
 671 |     text-transform: uppercase;
 672 |     margin-bottom: 0.25rem;
 673 | }
 674 |
 675 | .metric-value {
 676 |     font-size: 1.25rem;
 677 |     font-weight: 600;
 678 |     color: var(--primary);
 679 | }
 680 |
 681 | /* Feature Importance */
 682 | .feature-importance {
 683 |     margin-top: 1.5rem;
 684 | }
 685 |
 686 | .importance-chart {
 687 |     margin-top: 1rem;
 688 | }
 689 |
 690 | .importance-bar {
 691 |     display: flex;
 692 |     align-items: center;
 693 |     margin-bottom: 0.5rem;
 694 | }
 695 |
 696 | .importance-label {
 697 |     width: 150px;
 698 |     font-size: 0.875rem;
 699 |     color: var(--gray-700);
 700 | }
 701 |
 702 | .importance-value {
 703 |     flex: 1;
 704 |     height: 1.5rem;
 705 |     background: var(--gray-200);
 706 |     border-radius: 0.25rem;
 707 |     position: relative;
 708 |     overflow: hidden;
 709 | }
 710 |
 711 | .importance-fill {
 712 |     height: 100%;
 713 |     background: linear-gradient(90deg, var(--primary), var(--primary-light));
 714 |     border-radius: 0.25rem;
 715 |     transition: width 0.5s ease;
 716 | }
 717 |
 718 | .importance-value span {
 719 |     position: absolute;
 720 |     right: 0.5rem;
 721 |     top: 50%;
 722 |     transform: translateY(-50%);
 723 |     font-size: 0.75rem;
 724 |     font-weight: 500;
 725 |     color: var(--gray-700);
 726 | }
 727 |
 728 | /* Confusion Matrix */
 729 | .confusion-matrix {
 730 |     margin-top: 1.5rem;
 731 | }
 732 |
 733 | .confusion-matrix-table {
 734 |     width: 100%;
 735 |     border-collapse: collapse;
 736 |     margin-top: 1rem;
 737 | }
 738 |
 739 | .confusion-matrix-table th,
 740 | .confusion-matrix-table td {
 741 |     padding: 0.75rem;
 742 |     text-align: center;
 743 |     border: 1px solid var(--gray-300);
 744 | }
 745 |
 746 | .confusion-matrix-table th {
 747 |     background: var(--gray-100);
 748 |     font-weight: 600;
 749 |     color: var(--gray-700);
 750 | }
 751 |
 752 | .confusion-matrix-table td {
 753 |     font-weight: 500;
 754 |     background: white;
 755 | }
 756 |
 757 | /* Modals */
 758 | .modal {
 759 |     position: fixed;
 760 |     top: 0;
 761 |     left: 0;
 762 |     right: 0;
 763 |     bottom: 0;
 764 |     background: rgba(0, 0, 0, 0.5);
 765 |     display: flex;
 766 |     align-items: center;
 767 |     justify-content: center;
 768 |     z-index: 2000;
 769 |     animation: fadeIn 0.3s ease;
 770 | }
 771 |
 772 | .modal-content {
 773 |     background: white;
 774 |     border-radius: var(--border-radius);
 775 |     width: 90%;
 776 |     max-width: 600px;
 777 |     max-height: 90vh;
 778 |     overflow: hidden;
 779 |     display: flex;
 780 |     flex-direction: column;
 781 |     box-shadow: var(--shadow-xl);
 782 |     animation: slideUp 0.3s ease;
 783 | }
 784 |
 785 | @keyframes slideUp {
 786 |     from {
 787 |         transform: translateY(20px);
 788 |         opacity: 0;
 789 |     }
 790 |     to {
 791 |         transform: translateY(0);
 792 |         opacity: 1;
 793 |     }
 794 | }
 795 |
 796 | .modal-header {
 797 |     display: flex;
 798 |     justify-content: space-between;
 799 |     align-items: center;
 800 |     padding: 1.5rem;
 801 |     border-bottom: 1px solid var(--gray-200);
 802 | }
 803 |
 804 | .modal-header h3 {
 805 |     margin: 0;
 806 |     font-size: 1.25rem;
 807 |     color: var(--gray-900);
 808 | }
 809 |
 810 | .modal-close {
 811 |     width: 2rem;
 812 |     height: 2rem;
 813 |     display: flex;
 814 |     align-items: center;
 815 |     justify-content: center;
 816 |     background: transparent;
 817 |     border: none;
 818 |     font-size: 1.5rem;
 819 |     color: var(--gray-500);
 820 |     cursor: pointer;
 821 |     border-radius: 0.25rem;
 822 |     transition: var(--transition);
 823 | }
 824 |
 825 | .modal-close:hover {
 826 |     background: var(--gray-100);
 827 |     color: var(--gray-700);
 828 | }
 829 |
 830 | .modal-body {
 831 |     flex: 1;
 832 |     padding: 1.5rem;
 833 |     overflow-y: auto;
 834 | }
 835 |
 836 | .modal-footer {
 837 |     display: flex;
 838 |     justify-content: flex-end;
 839 |     gap: 0.75rem;
 840 |     padding: 1.5rem;
 841 |     border-top: 1px solid var(--gray-200);
 842 | }
 843 |
 844 | /* Empty State */
 845 | .empty-state {
 846 |     padding: 4rem;
 847 |     text-align: center;
 848 |     color: var(--gray-500);
 849 | }
 850 |
 851 | .empty-state svg {
 852 |     width: 64px;
 853 |     height: 64px;
 854 |     margin-bottom: 1.5rem;
 855 |     opacity: 0.5;
 856 | }
 857 |
 858 | .empty-state h3 {
 859 |     font-size: 1.25rem;
 860 |     margin-bottom: 0.5rem;
 861 |     color: var(--gray-700);
 862 | }
 863 |
 864 | .empty-state p {
 865 |     font-size: 0.875rem;
 866 |     color: var(--gray-500);
 867 | }
 868 |
 869 | /* Loading */
 870 | .loading-overlay {
 871 |     position: fixed;
 872 |     top: 0;
 873 |     left: 0;
 874 |     right: 0;
 875 |     bottom: 0;
 876 |     background: rgba(255, 255, 255, 0.9);
 877 |     display: flex;
 878 |     align-items: center;
 879 |     justify-content: center;
 880 |     z-index: 3000;
 881 | }
 882 |
 883 | .loading-spinner {
 884 |     width: 48px;
 885 |     height: 48px;
 886 |     border: 4px solid var(--gray-300);
 887 |     border-top-color: var(--primary);
 888 |     border-radius: 50%;
 889 |     animation: spin 1s linear infinite;
 890 | }
 891 |
 892 | @keyframes spin {
 893 |     to {
 894 |         transform: rotate(360deg);
 895 |     }
 896 | }
 897 |
 898 | .loading {
 899 |     display: inline-block;
 900 |     width: 20px;
 901 |     height: 20px;
 902 |     border: 3px solid var(--gray-300);
 903 |     border-radius: 50%;
 904 |     border-top-color: var(--primary);
 905 |     animation: spin 1s linear infinite;
 906 | }
 907 |
 908 | /* Notifications */
 909 | #notifications {
 910 |     position: fixed;
 911 |     top: calc(var(--header-height) + 1rem);
 912 |     right: 1rem;
 913 |     z-index: 2500;
 914 |     display: flex;
 915 |     flex-direction: column;
 916 |     gap: 0.75rem;
 917 |     max-width: 400px;
 918 | }
 919 |
 920 | .notification {
 921 |     padding: 1rem 1.5rem;
 922 |     border-radius: var(--border-radius);
 923 |     background: white;
 924 |     box-shadow: var(--shadow-lg);
 925 |     border-left: 4px solid;
 926 |     animation: slideInRight 0.3s ease;
 927 |     position: relative;
 928 | }
 929 |
 930 | @keyframes slideInRight {
 931 |     from {
 932 |         transform: translateX(100%);
 933 |         opacity: 0;
 934 |     }
 935 |     to {
 936 |         transform: translateX(0);
 937 |         opacity: 1;
 938 |     }
 939 | }
 940 |
 941 | .notification.fade-out {
 942 |     animation: fadeOutRight 0.3s ease;
 943 | }
 944 |
 945 | @keyframes fadeOutRight {
 946 |     to {
 947 |         transform: translateX(100%);
 948 |         opacity: 0;
 949 |     }
 950 | }
 951 |
 952 | .notification-info {
 953 |     border-left-color: var(--info);
 954 | }
 955 |
 956 | .notification-success {
 957 |     border-left-color: var(--success);
 958 | }
 959 |
 960 | .notification-warning {
 961 |     border-left-color: var(--warning);
 962 | }
 963 |
 964 | .notification-error {
 965 |     border-left-color: var(--danger);
 966 | }
 967 |
 968 | /* Statistics Table */
 969 | .statistics-table {
 970 |     width: 100%;
 971 |     border-collapse: collapse;
 972 |     margin-top: 1rem;
 973 | }
 974 |
 975 | .statistics-table thead {
 976 |     background: var(--gray-50);
 977 | }
 978 |
 979 | .statistics-table th,
 980 | .statistics-table td {
 981 |     padding: 0.75rem;
 982 |     text-align: left;
 983 |     border: 1px solid var(--gray-200);
 984 | }
 985 |
 986 | .statistics-table th {
 987 |     font-weight: 600;
 988 |     color: var(--gray-700);
 989 |     font-size: 0.875rem;
 990 | }
 991 |
 992 | .statistics-table td {
 993 |     font-size: 0.875rem;
 994 |     color: var(--gray-800);
 995 | }
 996 |
 997 | .statistics-table tbody tr:hover {
 998 |     background: var(--gray-50);
 999 | }
1000 |
1001 | /* Data Statistics */
1002 | .data-statistics {
1003 |     background: white;
1004 |     border-radius: var(--border-radius);
1005 |     padding: 1.5rem;
1006 |     margin-top: 1.5rem;
1007 |     box-shadow: var(--shadow);
1008 | }
1009 |
1010 | .data-statistics h3 {
1011 |     margin-bottom: 1rem;
1012 | }
1013 |
1014 | .stats-grid {
1015 |     display: grid;
1016 |     grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
1017 |     gap: 1rem;
1018 |     margin-bottom: 1.5rem;
1019 | }
1020 |
1021 | .stat-card {
1022 |     padding: 1rem;
1023 |     background: var(--gray-50);
1024 |     border-radius: 0.5rem;
1025 |     text-align: center;
1026 | }
1027 |
1028 | /* Missing Values Chart */
1029 | .missing-values-chart {
1030 |     margin-top: 1.5rem;
1031 | }
1032 |
1033 | .missing-values-chart h4 {
1034 |     margin-bottom: 1rem;
1035 |     color: var(--gray-700);
1036 | }
1037 |
1038 | .missing-bar {
1039 |     display: flex;
1040 |     align-items: center;
1041 |     margin-bottom: 0.75rem;
1042 | }
1043 |
1044 | .missing-bar .column-name {
1045 |     width: 120px;
1046 |     font-size: 0.875rem;
1047 |     color: var(--gray-700);
1048 | }
1049 |
1050 | .bar-container {
1051 |     flex: 1;
1052 |     height: 1.5rem;
1053 |     background: var(--gray-200);
1054 |     border-radius: 0.25rem;
1055 |     position: relative;
1056 |     overflow: hidden;
1057 | }
1058 |
1059 | .bar {
1060 |     height: 100%;
1061 |     background: linear-gradient(90deg, var(--danger), var(--warning));
1062 |     border-radius: 0.25rem;
1063 |     transition: width 0.5s ease;
1064 | }
1065 |
1066 | .percentage {
1067 |     position: absolute;
1068 |     right: 0.5rem;
1069 |     top: 50%;
1070 |     transform: translateY(-50%);
1071 |     font-size: 0.75rem;
1072 |     font-weight: 500;
1073 |     color: var(--gray-700);
1074 | }
1075 |
1076 | /* Variable Info Modal */
1077 | .variable-stats {
1078 |     display: grid;
1079 |     gap: 0.5rem;
1080 | }
1081 |
1082 | .stat-row {
1083 |     display: flex;
1084 |     justify-content: space-between;
1085 |     padding: 0.5rem;
1086 |     background: var(--gray-50);
1087 |     border-radius: 0.25rem;
1088 | }
1089 |
1090 | .stat-row:hover {
1091 |     background: var(--gray-100);
1092 | }
1093 |
1094 | /* File Info */
1095 | .file-info {
1096 |     padding: 1rem;
1097 |     background: var(--gray-100);
1098 |     border-radius: var(--border-radius);
1099 |     margin-top: 1rem;
1100 | }
1101 |
1102 | .file-name {
1103 |     font-weight: 500;
1104 |     color: var(--gray-800);
1105 |     margin-bottom: 0.5rem;
1106 | }
1107 |
1108 | .data-info {
1109 |     display: flex;
1110 |     justify-content: space-between;
1111 |     align-items: center;
1112 |     padding: 1rem;
1113 |     background: var(--gray-50);
1114 |     border-top: 1px solid var(--gray-200);
1115 | }
1116 |
1117 | /* Dark Theme Support */
1118 | [data-theme="dark"] {
1119 |     --gray-50: #1f2937;
1120 |     --gray-100: #374151;
1121 |     --gray-200: #4b5563;
1122 |     --gray-300: #6b7280;
1123 |     --gray-400: #9ca3af;
1124 |     --gray-500: #d1d5db;
1125 |     --gray-600: #e5e7eb;
1126 |     --gray-700: #f3f4f6;
1127 |     --gray-800: #f9fafb;
1128 |     --gray-900: #ffffff;
1129 | }
1130 |
1131 | [data-theme="dark"] body {
1132 |     background: linear-gradient(135deg, #1e1b4b 0%, #312e81 100%);
1133 | }
1134 |
1135 | [data-theme="dark"] .header,
1136 | [data-theme="dark"] .sidebar,
1137 | [data-theme="dark"] .content,
1138 | [data-theme="dark"] .modal-content,
1139 | [data-theme="dark"] .result-card,
1140 | [data-theme="dark"] .data-table-container {
1141 |     background: var(--gray-100);
1142 |     color: var(--gray-900);
1143 | }
1144 |
1145 | /* Responsive Design */
1146 | @media (max-width: 1280px) {
1147 |     .sidebar {
1148 |         width: 280px;
1149 |     }
1150 | }
1151 |
1152 | @media (max-width: 1024px) {
1153 |     .main-layout {
1154 |         flex-direction: column;
1155 |     }
1156 |
1157 |     .sidebar {
1158 |         width: 100%;
1159 |         border-right: none;
1160 |         border-bottom: 1px solid var(--gray-200);
1161 |         max-height: 300px;
1162 |     }
1163 |
1164 |     .stat-grid {
1165 |         grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
1166 |     }
1167 | }
1168 |
1169 | @media (max-width: 768px) {
1170 |     .header-content {
1171 |         padding: 0 1rem;
1172 |     }
1173 |
1174 |     .nav {
1175 |         display: none;
1176 |     }
1177 |
1178 |     .logo {
1179 |         font-size: 1.25rem;
1180 |     }
1181 |
1182 |     .content {
1183 |         padding: 1rem;
1184 |     }
1185 |
1186 |     .modal-content {
1187 |         width: 95%;
1188 |         margin: 1rem;
1189 |     }
1190 |
1191 |     #notifications {
1192 |         left: 1rem;
1193 |         right: 1rem;
1194 |         max-width: none;
1195 |     }
1196 | }
1197 |
1198 | @media (max-width: 480px) {
1199 |     .stats-grid {
1200 |         grid-template-columns: 1fr;
1201 |     }
1202 |
1203 |     .btn {
1204 |         width: 100%;
1205 |         justify-content: center;
1206 |     }
1207 |
1208 |     .panel-header {
1209 |         flex-direction: column;
1210 |         align-items: flex-start;
1211 |         gap: 1rem;
1212 |     }
1213 | }
1214 |
1215 | /* Print Styles */
1216 | @media print {
1217 |     .header,
1218 |     .sidebar,
1219 |     .nav,
1220 |     .btn,
1221 |     .modal {
1222 |         display: none !important;
1223 |     }
1224 |
1225 |     .main-layout {
1226 |         display: block;
1227 |     }
1228 |
1229 |     .content {
1230 |         padding: 0;
1231 |         background: white;
1232 |     }
1233 |
1234 |     .result-card {
1235 |         page-break-inside: avoid;
1236 |         box-shadow: none;
1237 |         border: 1px solid #000;
1238 |     }
1239 | }
1240 |
1241 | /* Accessibility */
1242 | .sr-only {
1243 |     position: absolute;
1244 |     width: 1px;
1245 |     height: 1px;
1246 |     padding: 0;
1247 |     margin: -1px;
1248 |     overflow: hidden;
1249 |     clip: rect(0, 0, 0, 0);
1250 |     white-space: nowrap;
1251 |     border: 0;
1252 | }
1253 |
1254 | /* Focus Styles */
1255 | *:focus-visible {
1256 |     outline: 2px solid var(--primary);
1257 |     outline-offset: 2px;
1258 | }
1259 |
1260 | /* Selection */
1261 | ::selection {
1262 |     background: var(--primary);
1263 |     color: white;
1264 | }
1265 |
1266 | ::-moz-selection {
1267 |     background: var(--primary);
1268 |     color: white;
1269 | }
1270 |
1271 | /* Scrollbar */
1272 | ::-webkit-scrollbar {
1273 |     width: 8px;
1274 |     height: 8px;
1275 | }
1276 |
1277 | ::-webkit-scrollbar-track {
1278 |     background: var(--gray-100);
1279 | }
1280 |
1281 | ::-webkit-scrollbar-thumb {
1282 |     background: var(--gray-400);
1283 |     border-radius: 4px;
1284 | }
1285 |
1286 | ::-webkit-scrollbar-thumb:hover {
1287 |     background: var(--gray-500);
1288 | }


--------------------------------------------------------------------------------
/frontend/index.html:
--------------------------------------------------------------------------------
  1 | <!DOCTYPE html>
  2 | <html lang="en">
  3 | <head>
  4 |     <meta charset="UTF-8">
  5 |     <meta name="viewport" content="width=device-width, initial-scale=1.0">
  6 |     <meta name="description"
  7 |           content="OpenStatica - Open-source statistical & ML platform scalable from lightweight analysis to advanced models">
  8 |     <meta name="keywords" content="statistics, machine learning, data analysis, open source">
  9 |     <title>OpenStatica - Statistical & ML Platform</title>
 10 |
 11 |     <!-- Favicon -->
 12 |     <link rel="icon" type="image/svg+xml" href="assets/logo.svg">
 13 |
 14 |     <!-- Fonts -->
 15 |     <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
 16 |
 17 |     <!-- Styles -->
 18 |     <link rel="stylesheet" href="css/styles.css">
 19 |
 20 |     <!-- Chart Libraries -->
 21 |     <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
 22 |     <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
 23 |
 24 |     <!-- Icons -->
 25 |     <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lucide-static@0.294.0/font/lucide.css">
 26 | </head>
 27 | <body>
 28 | <div class="app-container">
 29 |     <!-- Header -->
 30 |     <header class="header">
 31 |         <div class="header-content">
 32 |             <div class="logo">
 33 |                 <svg width="32" height="32" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
 34 |                     <path d="M3 12h4l3-9 4 18 3-9h4" stroke-linecap="round" stroke-linejoin="round"/>
 35 |                 </svg>
 36 |                 <h1>OpenStatica</h1>
 37 |             </div>
 38 |
 39 |             <nav class="nav">
 40 |                 <button class="nav-btn active" data-view="data">
 41 |                     <i class="lucide-database"></i>
 42 |                     <span>Data</span>
 43 |                 </button>
 44 |                 <button class="nav-btn" data-view="descriptive">
 45 |                     <i class="lucide-bar-chart-3"></i>
 46 |                     <span>Descriptive</span>
 47 |                 </button>
 48 |                 <button class="nav-btn" data-view="inferential">
 49 |                     <i class="lucide-activity"></i>
 50 |                     <span>Inferential</span>
 51 |                 </button>
 52 |                 <button class="nav-btn" data-view="regression">
 53 |                     <i class="lucide-trending-up"></i>
 54 |                     <span>Regression</span>
 55 |                 </button>
 56 |                 <button class="nav-btn" data-view="multivariate">
 57 |                     <i class="lucide-git-branch"></i>
 58 |                     <span>Multivariate</span>
 59 |                 </button>
 60 |                 <button class="nav-btn" data-view="ml">
 61 |                     <i class="lucide-cpu"></i>
 62 |                     <span>Machine Learning</span>
 63 |                 </button>
 64 |                 <button class="nav-btn" data-view="visualization">
 65 |                     <i class="lucide-pie-chart"></i>
 66 |                     <span>Visualization</span>
 67 |                 </button>
 68 |             </nav>
 69 |
 70 |             <div class="header-actions">
 71 |                 <button class="btn-icon" id="themeToggle" title="Toggle Theme">
 72 |                     <i class="lucide-sun"></i>
 73 |                 </button>
 74 |                 <button class="btn-icon" id="settingsBtn" title="Settings">
 75 |                     <i class="lucide-settings"></i>
 76 |                 </button>
 77 |                 <button class="btn-icon" id="helpBtn" title="Help">
 78 |                     <i class="lucide-help-circle"></i>
 79 |                 </button>
 80 |             </div>
 81 |         </div>
 82 |     </header>
 83 |
 84 |     <!-- Main Layout -->
 85 |     <div class="main-layout">
 86 |         <!-- Sidebar -->
 87 |         <aside class="sidebar">
 88 |             <!-- Data Input Section -->
 89 |             <div class="sidebar-section">
 90 |                 <h3>
 91 |                     <i class="lucide-upload"></i>
 92 |                     Data Input
 93 |                 </h3>
 94 |                 <div class="upload-area" id="uploadArea">
 95 |                     <svg width="48" height="48" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
 96 |                         <path d="M21 15v4a2 2 0 01-2 2H5a2 2 0 01-2-2v-4" stroke-linecap="round"
 97 |                               stroke-linejoin="round"/>
 98 |                         <polyline points="17 8 12 3 7 8" stroke-linecap="round" stroke-linejoin="round"/>
 99 |                         <line x1="12" y1="3" x2="12" y2="15" stroke-linecap="round" stroke-linejoin="round"/>
100 |                     </svg>
101 |                     <p>Drop CSV/Excel file here or click to browse</p>
102 |                     <small>Supports: CSV, Excel, JSON, Parquet</small>
103 |                     <input type="file" id="fileInput" accept=".csv,.xlsx,.xls,.json,.parquet" hidden>
104 |                 </div>
105 |
106 |                 <div id="fileInfo" class="file-info" style="display: none;">
107 |                     <div class="file-name"></div>
108 |                     <div class="file-actions">
109 |                         <button class="btn-text" id="clearData">
110 |                             <i class="lucide-x"></i> Clear Data
111 |                         </button>
112 |                         <button class="btn-text" id="downloadData">
113 |                             <i class="lucide-download"></i> Export
114 |                         </button>
115 |                     </div>
116 |                 </div>
117 |             </div>
118 |
119 |             <!-- Variable Selection Section -->
120 |             <div class="sidebar-section" id="variableSelector" style="display: none;">
121 |                 <h3>
122 |                     <i class="lucide-list"></i>
123 |                     Variables
124 |                 </h3>
125 |                 <div class="variable-search">
126 |                     <input type="text" class="form-control" id="variableSearch" placeholder="Search variables...">
127 |                 </div>
128 |                 <div class="variable-list" id="variableList"></div>
129 |             </div>
130 |
131 |             <!-- Analysis Options Section -->
132 |             <div class="sidebar-section" id="analysisOptions" style="display: none;">
133 |                 <h3>
134 |                     <i class="lucide-sliders"></i>
135 |                     Analysis Options
136 |                 </h3>
137 |                 <div id="optionsContent"></div>
138 |                 <button class="btn btn-primary" id="runAnalysis">
139 |                     <i class="lucide-play"></i>
140 |                     Run Analysis
141 |                 </button>
142 |             </div>
143 |
144 |             <!-- Quick Actions -->
145 |             <div class="sidebar-section">
146 |                 <h3>
147 |                     <i class="lucide-zap"></i>
148 |                     Quick Actions
149 |                 </h3>
150 |                 <div class="quick-actions">
151 |                     <button class="btn btn-secondary btn-sm" id="quickDescriptive">
152 |                         Quick Stats
153 |                     </button>
154 |                     <button class="btn btn-secondary btn-sm" id="quickCorrelation">
155 |                         Correlation Matrix
156 |                     </button>
157 |                     <button class="btn btn-secondary btn-sm" id="quickVisualize">
158 |                         Auto Visualize
159 |                     </button>
160 |                 </div>
161 |             </div>
162 |         </aside>
163 |
164 |         <!-- Content Area -->
165 |         <main class="content">
166 |             <!-- Data View -->
167 |             <div class="view-panel active" id="dataView">
168 |                 <div class="panel-header">
169 |                     <h2>Data Overview</h2>
170 |                     <div class="panel-actions">
171 |                         <span id="dataInfo"></span>
172 |                         <button class="btn btn-sm" id="transformBtn">
173 |                             <i class="lucide-wand-2"></i> Transform
174 |                         </button>
175 |                     </div>
176 |                 </div>
177 |
178 |                 <div class="data-table-container" id="dataTableContainer">
179 |                     <div class="empty-state">
180 |                         <svg width="64" height="64" viewBox="0 0 24 24" fill="none" stroke="currentColor"
181 |                              stroke-width="2">
182 |                             <rect x="3" y="3" width="18" height="18" rx="2" ry="2"/>
183 |                             <line x1="9" y1="9" x2="15" y2="9"/>
184 |                             <line x1="9" y1="15" x2="15" y2="15"/>
185 |                         </svg>
186 |                         <h3>No Data Loaded</h3>
187 |                         <p>Upload a CSV or Excel file to get started</p>
188 |                         <button class="btn btn-primary" onclick="document.getElementById('fileInput').click()">
189 |                             <i class="lucide-upload"></i> Upload Data
190 |                         </button>
191 |                     </div>
192 |                 </div>
193 |             </div>
194 |
195 |             <!-- Descriptive Statistics View -->
196 |             <div class="view-panel" id="descriptiveView">
197 |                 <div class="panel-header">
198 |                     <h2>Descriptive Statistics</h2>
199 |                     <div class="panel-actions">
200 |                         <button class="btn btn-sm" id="exportDescriptive">
201 |                             <i class="lucide-download"></i> Export
202 |                         </button>
203 |                     </div>
204 |                 </div>
205 |                 <div class="results-container" id="descriptiveResults">
206 |                     <div class="empty-state">
207 |                         <i class="lucide-bar-chart-3" style="width: 64px; height: 64px;"></i>
208 |                         <h3>No Analysis Run</h3>
209 |                         <p>Select variables and run descriptive analysis</p>
210 |                     </div>
211 |                 </div>
212 |             </div>
213 |
214 |             <!-- Inferential Statistics View -->
215 |             <div class="view-panel" id="inferentialView">
216 |                 <div class="panel-header">
217 |                     <h2>Inferential Statistics</h2>
218 |                     <div class="panel-actions">
219 |                         <button class="btn btn-sm" id="exportInferential">
220 |                             <i class="lucide-download"></i> Export
221 |                         </button>
222 |                     </div>
223 |                 </div>
224 |                 <div class="results-container" id="inferentialResults">
225 |                     <div class="empty-state">
226 |                         <i class="lucide-activity" style="width: 64px; height: 64px;"></i>
227 |                         <h3>No Analysis Run</h3>
228 |                         <p>Select a test type and run analysis</p>
229 |                     </div>
230 |                 </div>
231 |             </div>
232 |
233 |             <!-- Regression View -->
234 |             <div class="view-panel" id="regressionView">
235 |                 <div class="panel-header">
236 |                     <h2>Regression Analysis</h2>
237 |                     <div class="panel-actions">
238 |                         <button class="btn btn-sm" id="exportRegression">
239 |                             <i class="lucide-download"></i> Export
240 |                         </button>
241 |                     </div>
242 |                 </div>
243 |                 <div class="results-container" id="regressionResults">
244 |                     <div class="empty-state">
245 |                         <i class="lucide-trending-up" style="width: 64px; height: 64px;"></i>
246 |                         <h3>No Analysis Run</h3>
247 |                         <p>Configure regression model and run analysis</p>
248 |                     </div>
249 |                 </div>
250 |             </div>
251 |
252 |             <!-- Multivariate View -->
253 |             <div class="view-panel" id="multivariateView">
254 |                 <div class="panel-header">
255 |                     <h2>Multivariate Analysis</h2>
256 |                     <div class="panel-actions">
257 |                         <button class="btn btn-sm" id="exportMultivariate">
258 |                             <i class="lucide-download"></i> Export
259 |                         </button>
260 |                     </div>
261 |                 </div>
262 |                 <div class="results-container" id="multivariateResults">
263 |                     <div class="empty-state">
264 |                         <i class="lucide-git-branch" style="width: 64px; height: 64px;"></i>
265 |                         <h3>No Analysis Run</h3>
266 |                         <p>Select multivariate analysis type and run</p>
267 |                     </div>
268 |                 </div>
269 |             </div>
270 |
271 |             <!-- Machine Learning View -->
272 |             <div class="view-panel" id="mlView">
273 |                 <div class="panel-header">
274 |                     <h2>Machine Learning</h2>
275 |                     <div class="panel-actions">
276 |                         <button class="btn btn-sm" id="modelHub">
277 |                             <i class="lucide-globe"></i> Model Hub
278 |                         </button>
279 |                         <button class="btn btn-sm" id="exportModel">
280 |                             <i class="lucide-download"></i> Export Model
281 |                         </button>
282 |                     </div>
283 |                 </div>
284 |                 <div class="results-container" id="mlResults">
285 |                     <div class="empty-state">
286 |                         <i class="lucide-cpu" style="width: 64px; height: 64px;"></i>
287 |                         <h3>No Model Trained</h3>
288 |                         <p>Configure ML algorithm and train model</p>
289 |                     </div>
290 |                 </div>
291 |             </div>
292 |
293 |             <!-- Visualization View -->
294 |             <div class="view-panel" id="visualizationView">
295 |                 <div class="panel-header">
296 |                     <h2>Data Visualization</h2>
297 |                     <div class="panel-actions">
298 |                         <select class="form-control" id="chartType" style="width: 150px;">
299 |                             <option value="histogram">Histogram</option>
300 |                             <option value="scatter">Scatter Plot</option>
301 |                             <option value="box">Box Plot</option>
302 |                             <option value="heatmap">Heatmap</option>
303 |                             <option value="line">Line Chart</option>
304 |                             <option value="bar">Bar Chart</option>
305 |                             <option value="pie">Pie Chart</option>
306 |                             <option value="3d">3D Scatter</option>
307 |                         </select>
308 |                         <button class="btn btn-primary btn-sm" id="createChart">
309 |                             <i class="lucide-plus"></i> Create
310 |                         </button>
311 |                     </div>
312 |                 </div>
313 |                 <div class="viz-container" id="vizContainer">
314 |                     <div class="empty-state">
315 |                         <i class="lucide-pie-chart" style="width: 64px; height: 64px;"></i>
316 |                         <h3>No Visualizations</h3>
317 |                         <p>Select chart type and create visualization</p>
318 |                     </div>
319 |                 </div>
320 |             </div>
321 |         </main>
322 |     </div>
323 | </div>
324 |
325 | <!-- Notifications Container -->
326 | <div id="notifications"></div>
327 |
328 | <!-- Global Loader -->
329 | <div id="globalLoader" class="loading-overlay" style="display: none;">
330 |     <div class="loading-spinner"></div>
331 | </div>
332 |
333 | <!-- Scripts -->
334 | <script src="js/utils/helpers.js"></script>
335 | <script src="js/core/api.js"></script>
336 | <script src="js/core/state.js"></script>
337 | <script src="js/modules/data.js"></script>
338 | <script src="js/modules/statistics.js"></script>
339 | <script src="js/modules/ml.js"></script>
340 | <script src="js/modules/visualization.js"></script>
341 | <script src="js/core/app.js"></script>
342 | </body>
343 | </html>


--------------------------------------------------------------------------------
/frontend/js/core/api.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * OpenStatica API Client
  3 |  * Handles all communication with backend
  4 |  */
  5 |
  6 | class OpenStaticaAPI {
  7 |     constructor() {
  8 |         this.baseURL = process.env.API_URL || 'http://localhost:8000';
  9 |         this.apiVersion = 'v1';
 10 |         this.timeout = 30000;
 11 |         this.headers = {
 12 |             'Content-Type': 'application/json',
 13 |         };
 14 |     }
 15 |
 16 |     /**
 17 |      * Generic request handler with error handling
 18 |      */
 19 |     async request(endpoint, options = {}) {
 20 |         const url = `${this.baseURL}/api/${this.apiVersion}${endpoint}`;
 21 |
 22 |         const controller = new AbortController();
 23 |         const timeoutId = setTimeout(() => controller.abort(), this.timeout);
 24 |
 25 |         try {
 26 |             const response = await fetch(url, {
 27 |                 ...options,
 28 |                 headers: {
 29 |                     ...this.headers,
 30 |                     ...options.headers,
 31 |                 },
 32 |                 signal: controller.signal,
 33 |             });
 34 |
 35 |             clearTimeout(timeoutId);
 36 |
 37 |             if (!response.ok) {
 38 |                 const error = await response.json();
 39 |                 throw new Error(error.detail || error.message || `HTTP ${response.status}`);
 40 |             }
 41 |
 42 |             return await response.json();
 43 |
 44 |         } catch (error) {
 45 |             clearTimeout(timeoutId);
 46 |
 47 |             if (error.name === 'AbortError') {
 48 |                 throw new Error('Request timeout');
 49 |             }
 50 |
 51 |             throw error;
 52 |         }
 53 |     }
 54 |
 55 |     // System endpoints
 56 |     async getStatus() {
 57 |         return await this.request('/', {method: 'GET'});
 58 |     }
 59 |
 60 |     async getHealth() {
 61 |         return await this.request('/health', {method: 'GET'});
 62 |     }
 63 |
 64 |     // Data endpoints
 65 |     async uploadFile(file) {
 66 |         const formData = new FormData();
 67 |         formData.append('file', file);
 68 |
 69 |         return await fetch(`${this.baseURL}/api/${this.apiVersion}/data/upload`, {
 70 |             method: 'POST',
 71 |             body: formData,
 72 |         }).then(res => res.json());
 73 |     }
 74 |
 75 |     async getData(sessionId) {
 76 |         return await this.request(`/data/${sessionId}`);
 77 |     }
 78 |
 79 |     async deleteSession(sessionId) {
 80 |         return await this.request(`/data/${sessionId}`, {method: 'DELETE'});
 81 |     }
 82 |
 83 |     async getDataInfo(sessionId) {
 84 |         return await this.request(`/data/${sessionId}/info`);
 85 |     }
 86 |
 87 |     // Statistical endpoints
 88 |     async descriptiveStats(sessionId, params) {
 89 |         return await this.request('/statistics/descriptive', {
 90 |             method: 'POST',
 91 |             body: JSON.stringify({
 92 |                 session_id: sessionId,
 93 |                 ...params
 94 |             })
 95 |         });
 96 |     }
 97 |
 98 |     async frequencyDistribution(sessionId, params) {
 99 |         return await this.request('/statistics/frequency', {
100 |             method: 'POST',
101 |             body: JSON.stringify({
102 |                 session_id: sessionId,
103 |                 ...params
104 |             })
105 |         });
106 |     }
107 |
108 |     async tTest(sessionId, params) {
109 |         return await this.request('/statistics/ttest', {
110 |             method: 'POST',
111 |             body: JSON.stringify({
112 |                 session_id: sessionId,
113 |                 ...params
114 |             })
115 |         });
116 |     }
117 |
118 |     async anova(sessionId, params) {
119 |         return await this.request('/statistics/anova', {
120 |             method: 'POST',
121 |             body: JSON.stringify({
122 |                 session_id: sessionId,
123 |                 ...params
124 |             })
125 |         });
126 |     }
127 |
128 |     async correlation(sessionId, params) {
129 |         return await this.request('/statistics/correlation', {
130 |             method: 'POST',
131 |             body: JSON.stringify({
132 |                 session_id: sessionId,
133 |                 ...params
134 |             })
135 |         });
136 |     }
137 |
138 |     async regression(sessionId, params) {
139 |         return await this.request('/statistics/regression', {
140 |             method: 'POST',
141 |             body: JSON.stringify({
142 |                 session_id: sessionId,
143 |                 ...params
144 |             })
145 |         });
146 |     }
147 |
148 |     // ML endpoints
149 |     async trainModel(sessionId, params) {
150 |         return await this.request('/ml/train', {
151 |             method: 'POST',
152 |             body: JSON.stringify({
153 |                 session_id: sessionId,
154 |                 ...params
155 |             })
156 |         });
157 |     }
158 |
159 |     async predict(sessionId, modelId, data) {
160 |         return await this.request('/ml/predict', {
161 |             method: 'POST',
162 |             body: JSON.stringify({
163 |                 session_id: sessionId,
164 |                 model_id: modelId,
165 |                 data: data
166 |             })
167 |         });
168 |     }
169 |
170 |     async evaluateModel(sessionId, modelId, testData) {
171 |         return await this.request('/ml/evaluate', {
172 |             method: 'POST',
173 |             body: JSON.stringify({
174 |                 session_id: sessionId,
175 |                 model_id: modelId,
176 |                 test_data: testData
177 |             })
178 |         });
179 |     }
180 |
181 |     async getAvailableModels() {
182 |         return await this.request('/ml/models');
183 |     }
184 |
185 |     async getModelInfo(modelId) {
186 |         return await this.request(`/ml/models/${modelId}`);
187 |     }
188 |
189 |     // Model Hub endpoints
190 |     async searchModels(query, source = 'huggingface') {
191 |         return await this.request(`/models/search?q=${query}&source=${source}`);
192 |     }
193 |
194 |     async loadModel(modelId, source = 'huggingface') {
195 |         return await this.request('/models/load', {
196 |             method: 'POST',
197 |             body: JSON.stringify({
198 |                 model_id: modelId,
199 |                 source: source
200 |             })
201 |         });
202 |     }
203 |
204 |     // Visualization endpoints
205 |     async generatePlot(sessionId, params) {
206 |         return await this.request('/visualization/plot', {
207 |             method: 'POST',
208 |             body: JSON.stringify({
209 |                 session_id: sessionId,
210 |                 ...params
211 |             })
212 |         });
213 |     }
214 |
215 |     // Export endpoints
216 |     async exportResults(sessionId, format = 'csv') {
217 |         const response = await fetch(
218 |             `${this.baseURL}/api/${this.apiVersion}/export/${sessionId}?format=${format}`
219 |         );
220 |         return await response.blob();
221 |     }
222 |
223 |     // Plugin endpoints
224 |     async getAvailablePlugins() {
225 |         return await this.request('/plugins');
226 |     }
227 |
228 |     async enablePlugin(pluginName) {
229 |         return await this.request(`/plugins/${pluginName}/enable`, {
230 |             method: 'POST'
231 |         });
232 |     }
233 |
234 |     async disablePlugin(pluginName) {
235 |         return await this.request(`/plugins/${pluginName}/disable`, {
236 |             method: 'POST'
237 |         });
238 |     }
239 | }


--------------------------------------------------------------------------------
/frontend/js/core/app.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * OpenStatica - Main Application Class
  3 |  * Open-source statistical & ML platform
  4 |  */
  5 |
  6 | class OpenStatica {
  7 |     constructor() {
  8 |         this.version = "1.0.0";
  9 |         this.sessionId = null;
 10 |         this.currentView = 'data';
 11 |         this.data = null;
 12 |
 13 |         // Initialize core modules
 14 |         this.api = new OpenStaticaAPI();
 15 |         this.state = new StateManager();
 16 |         this.statistics = new StatisticsModule(this);
 17 |         this.ml = new MLModule(this);
 18 |         this.visualization = new VisualizationModule(this);
 19 |         this.dataModule = new DataModule(this);
 20 |
 21 |         // Plugin system
 22 |         this.plugins = new Map();
 23 |
 24 |         this.initialize();
 25 |     }
 26 |
 27 |     async initialize() {
 28 |         console.log(`🚀 OpenStatica v${this.version} initializing...`);
 29 |
 30 |         // Check backend connection
 31 |         await this.checkBackendStatus();
 32 |
 33 |         // Initialize UI
 34 |         this.initializeUI();
 35 |         this.bindEventListeners();
 36 |
 37 |         // Load plugins if available
 38 |         await this.loadPlugins();
 39 |
 40 |         console.log('✅ OpenStatica ready');
 41 |     }
 42 |
 43 |     async checkBackendStatus() {
 44 |         try {
 45 |             const status = await this.api.getStatus();
 46 |             console.log('Backend status:', status);
 47 |
 48 |             // Update UI based on available features
 49 |             if (status.features) {
 50 |                 this.updateFeatureAvailability(status.features);
 51 |             }
 52 |         } catch (error) {
 53 |             console.error('Failed to connect to backend:', error);
 54 |             this.showNotification('Failed to connect to backend', 'error');
 55 |         }
 56 |     }
 57 |
 58 |     initializeUI() {
 59 |         // Set up initial UI state
 60 |         this.updateViewVisibility();
 61 |         this.setupTheme();
 62 |         this.setupTooltips();
 63 |     }
 64 |
 65 |     bindEventListeners() {
 66 |         // Navigation
 67 |         document.querySelectorAll('.nav-btn').forEach(btn => {
 68 |             btn.addEventListener('click', (e) => {
 69 |                 this.switchView(e.target.dataset.view);
 70 |             });
 71 |         });
 72 |
 73 |         // File upload
 74 |         this.setupFileUpload();
 75 |
 76 |         // Settings
 77 |         document.getElementById('settingsBtn')?.addEventListener('click', () => {
 78 |             this.openSettings();
 79 |         });
 80 |
 81 |         // Global keyboard shortcuts
 82 |         document.addEventListener('keydown', (e) => {
 83 |             this.handleKeyboardShortcut(e);
 84 |         });
 85 |     }
 86 |
 87 |     setupFileUpload() {
 88 |         const uploadArea = document.getElementById('uploadArea');
 89 |         const fileInput = document.getElementById('fileInput');
 90 |
 91 |         if (!uploadArea || !fileInput) return;
 92 |
 93 |         // Click to upload
 94 |         uploadArea.addEventListener('click', () => fileInput.click());
 95 |
 96 |         // File selection
 97 |         fileInput.addEventListener('change', async (e) => {
 98 |             const file = e.target.files[0];
 99 |             if (file) await this.handleFileUpload(file);
100 |         });
101 |
102 |         // Drag and drop
103 |         uploadArea.addEventListener('dragover', (e) => {
104 |             e.preventDefault();
105 |             uploadArea.classList.add('dragover');
106 |         });
107 |
108 |         uploadArea.addEventListener('dragleave', () => {
109 |             uploadArea.classList.remove('dragover');
110 |         });
111 |
112 |         uploadArea.addEventListener('drop', async (e) => {
113 |             e.preventDefault();
114 |             uploadArea.classList.remove('dragover');
115 |
116 |             const file = e.dataTransfer.files[0];
117 |             if (file) await this.handleFileUpload(file);
118 |         });
119 |     }
120 |
121 |     async handleFileUpload(file) {
122 |         try {
123 |             this.showLoading(true);
124 |
125 |             // Validate file
126 |             if (!this.validateFile(file)) {
127 |                 throw new Error('Invalid file type or size');
128 |             }
129 |
130 |             // Upload file
131 |             const response = await this.api.uploadFile(file);
132 |
133 |             // Store session info
134 |             this.sessionId = response.session_id;
135 |             this.data = response;
136 |             this.state.set('currentSession', response);
137 |
138 |             // Update UI
139 |             this.dataModule.displayData(response);
140 |             this.updateUIAfterDataLoad();
141 |
142 |             this.showNotification(`Successfully loaded ${response.rows} rows × ${response.columns} columns`, 'success');
143 |
144 |         } catch (error) {
145 |             console.error('File upload error:', error);
146 |             this.showNotification(error.message, 'error');
147 |         } finally {
148 |             this.showLoading(false);
149 |         }
150 |     }
151 |
152 |     validateFile(file) {
153 |         const maxSize = 100 * 1024 * 1024; // 100MB
154 |         const allowedTypes = ['.csv', '.xlsx', '.xls', '.json', '.parquet'];
155 |
156 |         const extension = '.' + file.name.split('.').pop().toLowerCase();
157 |
158 |         if (!allowedTypes.includes(extension)) {
159 |             this.showNotification(`File type ${extension} not supported`, 'error');
160 |             return false;
161 |         }
162 |
163 |         if (file.size > maxSize) {
164 |             this.showNotification('File size exceeds 100MB limit', 'error');
165 |             return false;
166 |         }
167 |
168 |         return true;
169 |     }
170 |
171 |     updateUIAfterDataLoad() {
172 |         // Show data info
173 |         document.getElementById('dataInfo').textContent =
174 |             `${this.data.rows} rows × ${this.data.columns} columns`;
175 |
176 |         // Show file info
177 |         document.getElementById('fileInfo').style.display = 'block';
178 |
179 |         // Enable analysis options
180 |         document.getElementById('variableSelector').style.display = 'block';
181 |
182 |         // Populate variables
183 |         this.populateVariables();
184 |
185 |         // Enable relevant navigation
186 |         this.enableNavigation();
187 |     }
188 |
189 |     populateVariables() {
190 |         const container = document.getElementById('variableList');
191 |         if (!container) return;
192 |
193 |         container.innerHTML = '';
194 |
195 |         // Add numeric variables
196 |         this.data.numeric_columns.forEach(col => {
197 |             container.appendChild(this.createVariableElement(col, 'numeric'));
198 |         });
199 |
200 |         // Add categorical variables
201 |         this.data.categorical_columns.forEach(col => {
202 |             container.appendChild(this.createVariableElement(col, 'categorical'));
203 |         });
204 |
205 |         // Add datetime variables if any
206 |         if (this.data.datetime_columns) {
207 |             this.data.datetime_columns.forEach(col => {
208 |                 container.appendChild(this.createVariableElement(col, 'datetime'));
209 |             });
210 |         }
211 |     }
212 |
213 |     createVariableElement(name, type) {
214 |         const div = document.createElement('div');
215 |         div.className = 'variable-item';
216 |         div.innerHTML = `
217 |             <input type="checkbox" id="var_${name}" value="${name}">
218 |             <label for="var_${name}">${name}</label>
219 |             <span class="variable-type">${type}</span>
220 |         `;
221 |         return div;
222 |     }
223 |
224 |     switchView(view) {
225 |         if (!view) return;
226 |
227 |         this.currentView = view;
228 |
229 |         // Update navigation
230 |         document.querySelectorAll('.nav-btn').forEach(btn => {
231 |             btn.classList.toggle('active', btn.dataset.view === view);
232 |         });
233 |
234 |         // Update view panels
235 |         document.querySelectorAll('.view-panel').forEach(panel => {
236 |             panel.classList.toggle('active', panel.id === `${view}View`);
237 |         });
238 |
239 |         // Load view-specific options
240 |         this.loadViewOptions(view);
241 |
242 |         // Track view change
243 |         this.state.set('currentView', view);
244 |     }
245 |
246 |     loadViewOptions(view) {
247 |         const optionsContainer = document.getElementById('analysisOptions');
248 |         if (!optionsContainer) return;
249 |
250 |         // Show/hide options based on view
251 |         optionsContainer.style.display =
252 |             ['descriptive', 'inferential', 'regression', 'multivariate', 'ml'].includes(view)
253 |                 ? 'block' : 'none';
254 |
255 |         // Load specific options for each view
256 |         switch (view) {
257 |             case 'descriptive':
258 |                 this.statistics.loadDescriptiveOptions();
259 |                 break;
260 |             case 'inferential':
261 |                 this.statistics.loadInferentialOptions();
262 |                 break;
263 |             case 'regression':
264 |                 this.statistics.loadRegressionOptions();
265 |                 break;
266 |             case 'ml':
267 |                 this.ml.loadMLOptions();
268 |                 break;
269 |         }
270 |     }
271 |
272 |     updateViewVisibility() {
273 |         // Show only active view
274 |         const activeView = this.state.get('currentView') || 'data';
275 |         this.switchView(activeView);
276 |     }
277 |
278 |     setupTheme() {
279 |         const theme = localStorage.getItem('theme') || 'light';
280 |         document.body.dataset.theme = theme;
281 |     }
282 |
283 |     setupTooltips() {
284 |         // Initialize tooltips for help icons
285 |         document.querySelectorAll('[data-tooltip]').forEach(el => {
286 |             el.title = el.dataset.tooltip;
287 |         });
288 |     }
289 |
290 |     enableNavigation() {
291 |         // Enable navigation buttons based on data availability
292 |         document.querySelectorAll('.nav-btn').forEach(btn => {
293 |             if (btn.dataset.view !== 'data') {
294 |                 btn.disabled = false;
295 |             }
296 |         });
297 |     }
298 |
299 |     async loadPlugins() {
300 |         try {
301 |             // Check for available plugins
302 |             const plugins = await this.api.getAvailablePlugins();
303 |
304 |             if (plugins && plugins.length > 0) {
305 |                 console.log(`Loading ${plugins.length} plugins...`);
306 |
307 |                 for (const plugin of plugins) {
308 |                     await this.loadPlugin(plugin);
309 |                 }
310 |             }
311 |         } catch (error) {
312 |             console.log('No plugins available or plugin system disabled');
313 |         }
314 |     }
315 |
316 |     async loadPlugin(pluginInfo) {
317 |         try {
318 |             // Dynamic import of plugin module
319 |             const module = await import(`/plugins/${pluginInfo.name}/index.js`);
320 |             const Plugin = module.default;
321 |
322 |             const plugin = new Plugin(this);
323 |             await plugin.initialize();
324 |
325 |             this.plugins.set(pluginInfo.name, plugin);
326 |             console.log(`✓ Loaded plugin: ${pluginInfo.name}`);
327 |
328 |         } catch (error) {
329 |             console.error(`Failed to load plugin ${pluginInfo.name}:`, error);
330 |         }
331 |     }
332 |
333 |     handleKeyboardShortcut(e) {
334 |         // Ctrl/Cmd + O: Open file
335 |         if ((e.ctrlKey || e.metaKey) && e.key === 'o') {
336 |             e.preventDefault();
337 |             document.getElementById('fileInput')?.click();
338 |         }
339 |
340 |         // Ctrl/Cmd + S: Save results
341 |         if ((e.ctrlKey || e.metaKey) && e.key === 's') {
342 |             e.preventDefault();
343 |             this.saveResults();
344 |         }
345 |
346 |         // Escape: Close modals
347 |         if (e.key === 'Escape') {
348 |             this.closeAllModals();
349 |         }
350 |     }
351 |
352 |     openSettings() {
353 |         // TODO: Implement settings modal
354 |         console.log('Settings not yet implemented');
355 |     }
356 |
357 |     async saveResults() {
358 |         // TODO: Implement results export
359 |         console.log('Save results not yet implemented');
360 |     }
361 |
362 |     closeAllModals() {
363 |         document.querySelectorAll('.modal').forEach(modal => {
364 |             modal.style.display = 'none';
365 |         });
366 |     }
367 |
368 |     updateFeatureAvailability(features) {
369 |         // Update UI based on available backend features
370 |         if (!features.ml) {
371 |             document.querySelector('[data-view="ml"]')?.setAttribute('disabled', 'true');
372 |         }
373 |
374 |         this.state.set('features', features);
375 |     }
376 |
377 |     showNotification(message, type = 'info') {
378 |         // Create notification element
379 |         const notification = document.createElement('div');
380 |         notification.className = `notification notification-${type}`;
381 |         notification.textContent = message;
382 |
383 |         // Add to notification container or create one
384 |         let container = document.getElementById('notifications');
385 |         if (!container) {
386 |             container = document.createElement('div');
387 |             container.id = 'notifications';
388 |             document.body.appendChild(container);
389 |         }
390 |
391 |         container.appendChild(notification);
392 |
393 |         // Auto-remove after 5 seconds
394 |         setTimeout(() => {
395 |             notification.classList.add('fade-out');
396 |             setTimeout(() => notification.remove(), 300);
397 |         }, 5000);
398 |     }
399 |
400 |     showLoading(show = true) {
401 |         let loader = document.getElementById('globalLoader');
402 |
403 |         if (show) {
404 |             if (!loader) {
405 |                 loader = document.createElement('div');
406 |                 loader.id = 'globalLoader';
407 |                 loader.className = 'loading-overlay';
408 |                 loader.innerHTML = '<div class="loading-spinner"></div>';
409 |                 document.body.appendChild(loader);
410 |             }
411 |             loader.style.display = 'flex';
412 |         } else {
413 |             if (loader) {
414 |                 loader.style.display = 'none';
415 |             }
416 |         }
417 |     }
418 |
419 |     getSelectedVariables() {
420 |         const checkboxes = document.querySelectorAll('#variableList input:checked');
421 |         return Array.from(checkboxes).map(cb => cb.value);
422 |     }
423 |
424 |     async runAnalysis() {
425 |         const variables = this.getSelectedVariables();
426 |
427 |         if (variables.length === 0) {
428 |             this.showNotification('Please select at least one variable', 'warning');
429 |             return;
430 |         }
431 |
432 |         try {
433 |             this.showLoading(true);
434 |
435 |             let result;
436 |
437 |             switch (this.currentView) {
438 |                 case 'descriptive':
439 |                     result = await this.statistics.runDescriptiveAnalysis(variables);
440 |                     break;
441 |                 case 'inferential':
442 |                     result = await this.statistics.runInferentialAnalysis(variables);
443 |                     break;
444 |                 case 'regression':
445 |                     result = await this.statistics.runRegressionAnalysis(variables);
446 |                     break;
447 |                 case 'ml':
448 |                     result = await this.ml.runMLAnalysis(variables);
449 |                     break;
450 |                 default:
451 |                     throw new Error('Invalid analysis type');
452 |             }
453 |
454 |             // Display results
455 |             this.displayResults(result);
456 |
457 |             // Generate visualizations
458 |             await this.visualization.createVisualizations(result);
459 |
460 |             this.showNotification('Analysis completed successfully', 'success');
461 |
462 |         } catch (error) {
463 |             console.error('Analysis error:', error);
464 |             this.showNotification(error.message, 'error');
465 |         } finally {
466 |             this.showLoading(false);
467 |         }
468 |     }
469 |
470 |     displayResults(results) {
471 |         const container = document.getElementById(`${this.currentView}Results`);
472 |         if (!container) return;
473 |
474 |         // Clear previous results
475 |         container.innerHTML = '';
476 |
477 |         // Create result cards
478 |         const card = document.createElement('div');
479 |         card.className = 'result-card';
480 |
481 |         // Format and display results based on type
482 |         card.innerHTML = this.formatResults(results);
483 |
484 |         container.appendChild(card);
485 |
486 |         // Store results in state
487 |         this.state.set('lastResults', results);
488 |     }
489 |
490 |     formatResults(results) {
491 |         // TODO: Implement comprehensive result formatting
492 |         return `<pre>${JSON.stringify(results, null, 2)}</pre>`;
493 |     }
494 | }
495 |
496 | // State Manager
497 | class StateManager {
498 |     constructor() {
499 |         this.state = new Map();
500 |     }
501 |
502 |     get(key) {
503 |         return this.state.get(key);
504 |     }
505 |
506 |     set(key, value) {
507 |         this.state.set(key, value);
508 |         this.emit('stateChange', {key, value});
509 |     }
510 |
511 |     emit(event, data) {
512 |         window.dispatchEvent(new CustomEvent(event, {detail: data}));
513 |     }
514 | }
515 |
516 | // Initialize application when DOM is ready
517 | document.addEventListener('DOMContentLoaded', () => {
518 |     window.openStatica = new OpenStatica();
519 | });


--------------------------------------------------------------------------------
/frontend/js/core/state.js:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/frontend/js/core/state.js


--------------------------------------------------------------------------------
/frontend/js/modules/data.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * Data Module for OpenStatica
  3 |  */
  4 |
  5 | class DataModule {
  6 |     constructor(app) {
  7 |         this.app = app;
  8 |         this.currentData = null;
  9 |         this.transformations = [];
 10 |     }
 11 |
 12 |     displayData(dataInfo) {
 13 |         this.currentData = dataInfo;
 14 |
 15 |         // Update file info
 16 |         this.updateFileInfo(dataInfo);
 17 |
 18 |         // Display data preview
 19 |         this.displayDataTable(dataInfo.preview);
 20 |
 21 |         // Show data statistics
 22 |         this.displayDataStatistics(dataInfo);
 23 |
 24 |         // Update variable list
 25 |         this.updateVariableList(dataInfo);
 26 |     }
 27 |
 28 |     updateFileInfo(dataInfo) {
 29 |         const fileInfoElement = document.querySelector('.file-name');
 30 |         if (fileInfoElement) {
 31 |             fileInfoElement.textContent = `${dataInfo.rows} rows × ${dataInfo.columns} columns`;
 32 |         }
 33 |     }
 34 |
 35 |     displayDataTable(preview) {
 36 |         const container = document.getElementById('dataTableContainer');
 37 |         if (!container || !preview || preview.length === 0) return;
 38 |
 39 |         const columns = Object.keys(preview[0]);
 40 |
 41 |         let html = `
 42 |             <div class="data-table-wrapper">
 43 |                 <table class="data-table">
 44 |                     <thead>
 45 |                         <tr>
 46 |                             <th class="row-number">#</th>
 47 |                             ${columns.map(col => `
 48 |                                 <th>
 49 |                                     <div class="column-header">
 50 |                                         <span class="column-name">${col}</span>
 51 |                                         <span class="column-type">${this.getColumnType(col)}</span>
 52 |                                     </div>
 53 |                                 </th>
 54 |                             `).join('')}
 55 |                         </tr>
 56 |                     </thead>
 57 |                     <tbody>
 58 |         `;
 59 |
 60 |         preview.forEach((row, index) => {
 61 |             html += `
 62 |                 <tr>
 63 |                     <td class="row-number">${index + 1}</td>
 64 |                     ${columns.map(col => `
 65 |                         <td class="${this.getCellClass(row[col])}">
 66 |                             ${this.formatCellValue(row[col])}
 67 |                         </td>
 68 |                     `).join('')}
 69 |                 </tr>
 70 |             `;
 71 |         });
 72 |
 73 |         html += `
 74 |                     </tbody>
 75 |                 </table>
 76 |             </div>
 77 |             <div class="data-info">
 78 |                 <span>Showing ${preview.length} of ${this.currentData.rows} rows</span>
 79 |                 <button class="btn btn-sm" onclick="openStatica.dataModule.loadMoreData()">
 80 |                     Load More
 81 |                 </button>
 82 |             </div>
 83 |         `;
 84 |
 85 |         container.innerHTML = html;
 86 |
 87 |         // Add column sorting
 88 |         this.setupColumnSorting();
 89 |     }
 90 |
 91 |     getColumnType(columnName) {
 92 |         if (!this.currentData) return 'unknown';
 93 |
 94 |         if (this.currentData.numeric_columns.includes(columnName)) {
 95 |             return 'numeric';
 96 |         } else if (this.currentData.categorical_columns.includes(columnName)) {
 97 |             return 'categorical';
 98 |         } else if (this.currentData.datetime_columns?.includes(columnName)) {
 99 |             return 'datetime';
100 |         }
101 |         return 'text';
102 |     }
103 |
104 |     getCellClass(value) {
105 |         if (value === null || value === undefined) return 'null-value';
106 |         if (typeof value === 'number') return 'numeric-value';
107 |         if (typeof value === 'boolean') return 'boolean-value';
108 |         return 'text-value';
109 |     }
110 |
111 |     formatCellValue(value) {
112 |         if (value === null || value === undefined) return '<span class="null">null</span>';
113 |         if (typeof value === 'number') {
114 |             if (Number.isInteger(value)) return value.toString();
115 |             return value.toFixed(4);
116 |         }
117 |         if (typeof value === 'boolean') return value ? '✓' : '✗';
118 |         if (value.length > 50) return value.substring(0, 50) + '...';
119 |         return value;
120 |     }
121 |
122 |     displayDataStatistics(dataInfo) {
123 |         const statsContainer = document.createElement('div');
124 |         statsContainer.className = 'data-statistics';
125 |
126 |         statsContainer.innerHTML = `
127 |             <h3>Data Overview</h3>
128 |             <div class="stats-grid">
129 |                 <div class="stat-card">
130 |                     <div class="stat-label">Total Rows</div>
131 |                     <div class="stat-value">${dataInfo.rows.toLocaleString()}</div>
132 |                 </div>
133 |                 <div class="stat-card">
134 |                     <div class="stat-label">Total Columns</div>
135 |                     <div class="stat-value">${dataInfo.columns}</div>
136 |                 </div>
137 |                 <div class="stat-card">
138 |                     <div class="stat-label">Numeric Columns</div>
139 |                     <div class="stat-value">${dataInfo.numeric_columns.length}</div>
140 |                 </div>
141 |                 <div class="stat-card">
142 |                     <div class="stat-label">Categorical Columns</div>
143 |                     <div class="stat-value">${dataInfo.categorical_columns.length}</div>
144 |                 </div>
145 |             </div>
146 |
147 |             ${this.createMissingValuesChart(dataInfo)}
148 |             ${this.createDataTypesChart(dataInfo)}
149 |         `;
150 |
151 |         const existingStats = document.querySelector('.data-statistics');
152 |         if (existingStats) {
153 |             existingStats.replaceWith(statsContainer);
154 |         } else {
155 |             document.getElementById('dataView').appendChild(statsContainer);
156 |         }
157 |     }
158 |
159 |     createMissingValuesChart(dataInfo) {
160 |         // Create a simple bar chart for missing values
161 |         if (!dataInfo.missing_values) return '';
162 |
163 |         let html = '<div class="missing-values-chart"><h4>Missing Values</h4>';
164 |
165 |         for (const [column, count] of Object.entries(dataInfo.missing_values)) {
166 |             if (count > 0) {
167 |                 const percentage = (count / dataInfo.rows * 100).toFixed(1);
168 |                 html += `
169 |                     <div class="missing-bar">
170 |                         <span class="column-name">${column}</span>
171 |                         <div class="bar-container">
172 |                             <div class="bar" style="width: ${percentage}%"></div>
173 |                             <span class="percentage">${percentage}%</span>
174 |                         </div>
175 |                     </div>
176 |                 `;
177 |             }
178 |         }
179 |
180 |         html += '</div>';
181 |         return html;
182 |     }
183 |
184 |     createDataTypesChart(dataInfo) {
185 |         // Create a pie chart for data types distribution
186 |         const types = {
187 |             'Numeric': dataInfo.numeric_columns.length,
188 |             'Categorical': dataInfo.categorical_columns.length,
189 |             'DateTime': dataInfo.datetime_columns?.length || 0
190 |         };
191 |
192 |         return `
193 |             <div class="data-types-chart">
194 |                 <h4>Column Types Distribution</h4>
195 |                 <canvas id="dataTypesChart"></canvas>
196 |             </div>
197 |         `;
198 |     }
199 |
200 |     updateVariableList(dataInfo) {
201 |         const container = document.getElementById('variableList');
202 |         if (!container) return;
203 |
204 |         container.innerHTML = '';
205 |
206 |         // Group variables by type
207 |         const groups = [
208 |             {title: 'Numeric Variables', items: dataInfo.numeric_columns, type: 'numeric'},
209 |             {title: 'Categorical Variables', items: dataInfo.categorical_columns, type: 'categorical'},
210 |             {title: 'DateTime Variables', items: dataInfo.datetime_columns || [], type: 'datetime'}
211 |         ];
212 |
213 |         groups.forEach(group => {
214 |             if (group.items.length === 0) return;
215 |
216 |             const groupDiv = document.createElement('div');
217 |             groupDiv.className = 'variable-group';
218 |
219 |             groupDiv.innerHTML = `
220 |                 <div class="variable-group-header">
221 |                     <span>${group.title} (${group.items.length})</span>
222 |                     <button class="btn-text" onclick="openStatica.dataModule.selectAllInGroup('${group.type}')">
223 |                         Select All
224 |                     </button>
225 |                 </div>
226 |             `;
227 |
228 |             group.items.forEach(variable => {
229 |                 const varItem = document.createElement('div');
230 |                 varItem.className = 'variable-item';
231 |                 varItem.innerHTML = `
232 |                     <input type="checkbox" id="var_${variable}" value="${variable}">
233 |                     <label for="var_${variable}">${variable}</label>
234 |                     <span class="variable-type">${group.type}</span>
235 |                     <button class="btn-icon-sm" onclick="openStatica.dataModule.showVariableInfo('${variable}')" title="Variable Info">
236 |                         ℹ️
237 |                     </button>
238 |                 `;
239 |                 groupDiv.appendChild(varItem);
240 |             });
241 |
242 |             container.appendChild(groupDiv);
243 |         });
244 |     }
245 |
246 |     setupColumnSorting() {
247 |         const headers = document.querySelectorAll('.data-table th');
248 |         headers.forEach((header, index) => {
249 |             if (index === 0) return; // Skip row number column
250 |
251 |             header.style.cursor = 'pointer';
252 |             header.addEventListener('click', () => {
253 |                 this.sortByColumn(index - 1);
254 |             });
255 |         });
256 |     }
257 |
258 |     sortByColumn(columnIndex) {
259 |         // TODO: Implement column sorting
260 |         console.log(`Sorting by column ${columnIndex}`);
261 |     }
262 |
263 |     async loadMoreData() {
264 |         // TODO: Implement pagination
265 |         console.log('Loading more data...');
266 |     }
267 |
268 |     selectAllInGroup(type) {
269 |         let columns = [];
270 |         if (type === 'numeric') {
271 |             columns = this.currentData.numeric_columns;
272 |         } else if (type === 'categorical') {
273 |             columns = this.currentData.categorical_columns;
274 |         } else if (type === 'datetime') {
275 |             columns = this.currentData.datetime_columns || [];
276 |         }
277 |
278 |         columns.forEach(col => {
279 |             const checkbox = document.getElementById(`var_${col}`);
280 |             if (checkbox) checkbox.checked = true;
281 |         });
282 |     }
283 |
284 |     async showVariableInfo(variable) {
285 |         // Get detailed statistics for the variable
286 |         try {
287 |             const result = await this.app.api.descriptiveStats(
288 |                 this.app.sessionId,
289 |                 {columns: [variable], options: {include_advanced: true}}
290 |             );
291 |
292 |             // Display in a modal or popup
293 |             this.displayVariableModal(variable, result);
294 |         } catch (error) {
295 |             console.error('Failed to get variable info:', error);
296 |         }
297 |     }
298 |
299 |     displayVariableModal(variable, stats) {
300 |         // Create modal
301 |         const modal = document.createElement('div');
302 |         modal.className = 'modal';
303 |         modal.innerHTML = `
304 |             <div class="modal-content">
305 |                 <div class="modal-header">
306 |                     <h3>Variable Information: ${variable}</h3>
307 |                     <button class="modal-close" onclick="this.parentElement.parentElement.parentElement.remove()">×</button>
308 |                 </div>
309 |                 <div class="modal-body">
310 |                     ${this.formatVariableStats(stats)}
311 |                 </div>
312 |             </div>
313 |         `;
314 |
315 |         document.body.appendChild(modal);
316 |     }
317 |
318 |     formatVariableStats(stats) {
319 |         // Format statistics for display
320 |         let html = '<div class="variable-stats">';
321 |
322 |         if (stats.results && stats.results[Object.keys(stats.results)[0]]) {
323 |             const varStats = stats.results[Object.keys(stats.results)[0]];
324 |
325 |             for (const [key, value] of Object.entries(varStats)) {
326 |                 if (typeof value === 'object') continue;
327 |
328 |                 html += `
329 |                     <div class="stat-row">
330 |                         <span class="stat-label">${this.formatStatName(key)}</span>
331 |                         <span class="stat-value">${this.formatStatValue(value)}</span>
332 |                     </div>
333 |                 `;
334 |             }
335 |         }
336 |
337 |         html += '</div>';
338 |         return html;
339 |     }
340 |
341 |     formatStatName(name) {
342 |         return name.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
343 |     }
344 |
345 |     formatStatValue(value) {
346 |         if (typeof value === 'number') {
347 |             return value.toFixed(4);
348 |         }
349 |         return value;
350 |     }
351 |
352 |     async transformData(transformationType) {
353 |         const modal = document.createElement('div');
354 |         modal.className = 'modal';
355 |         modal.innerHTML = `
356 |             <div class="modal-content">
357 |                 <div class="modal-header">
358 |                     <h3>Data Transformation</h3>
359 |                     <button class="modal-close" onclick="this.parentElement.parentElement.parentElement.remove()">×</button>
360 |                 </div>
361 |                 <div class="modal-body">
362 |                     ${this.getTransformationForm(transformationType)}
363 |                 </div>
364 |                 <div class="modal-footer">
365 |                     <button class="btn btn-primary" onclick="openStatica.dataModule.applyTransformation()">
366 |                         Apply
367 |                     </button>
368 |                     <button class="btn btn-secondary" onclick="this.parentElement.parentElement.parentElement.remove()">
369 |                         Cancel
370 |                     </button>
371 |                 </div>
372 |             </div>
373 |         `;
374 |
375 |         document.body.appendChild(modal);
376 |     }
377 |
378 |     getTransformationForm(type) {
379 |         const forms = {
380 |             'normalize': `
381 |                 <div class="form-group">
382 |                     <label>Normalization Method</label>
383 |                     <select id="normMethod">
384 |                         <option value="zscore">Z-Score</option>
385 |                         <option value="minmax">Min-Max</option>
386 |                         <option value="robust">Robust Scaler</option>
387 |                     </select>
388 |                 </div>
389 |             `,
390 |             'encode': `
391 |                 <div class="form-group">
392 |                     <label>Encoding Method</label>
393 |                     <select id="encodeMethod">
394 |                         <option value="onehot">One-Hot Encoding</option>
395 |                         <option value="label">Label Encoding</option>
396 |                         <option value="ordinal">Ordinal Encoding</option>
397 |                     </select>
398 |                 </div>
399 |             `,
400 |             'impute': `
401 |                 <div class="form-group">
402 |                     <label>Imputation Method</label>
403 |                     <select id="imputeMethod">
404 |                         <option value="mean">Mean</option>
405 |                         <option value="median">Median</option>
406 |                         <option value="mode">Mode</option>
407 |                         <option value="forward">Forward Fill</option>
408 |                         <option value="backward">Backward Fill</option>
409 |                         <option value="interpolate">Interpolate</option>
410 |                     </select>
411 |                 </div>
412 |             `
413 |         };
414 |
415 |         return forms[type] || '<p>Transformation not implemented</p>';
416 |     }
417 |
418 |     async applyTransformation() {
419 |         // TODO: Implement transformation application
420 |         console.log('Applying transformation...');
421 |     }
422 | }


--------------------------------------------------------------------------------
/frontend/js/modules/ml.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * Machine Learning Module for OpenStatica
  3 |  */
  4 |
  5 | class MLModule {
  6 |     constructor(app) {
  7 |         this.app = app;
  8 |         this.currentModel = null;
  9 |         this.trainedModels = new Map();
 10 |     }
 11 |
 12 |     loadMLOptions() {
 13 |         const container = document.getElementById('optionsContent');
 14 |         if (!container) return;
 15 |
 16 |         container.innerHTML = `
 17 |             <div class="form-group">
 18 |                 <label class="form-label">ML Task</label>
 19 |                 <select class="form-control" id="mlTask">
 20 |                     <option value="classification">Classification</option>
 21 |                     <option value="regression">Regression</option>
 22 |                     <option value="clustering">Clustering</option>
 23 |                     <option value="dimensionality">Dimensionality Reduction</option>
 24 |                     <option value="anomaly">Anomaly Detection</option>
 25 |                 </select>
 26 |             </div>
 27 |
 28 |             <div id="algorithmSelection"></div>
 29 |
 30 |             <div class="form-group">
 31 |                 <label class="form-label">Target Variable</label>
 32 |                 <select class="form-control" id="targetVariable">
 33 |                     <option value="">Select for supervised learning...</option>
 34 |                     ${this.getVariableOptions()}
 35 |                 </select>
 36 |             </div>
 37 |
 38 |             <div class="form-group">
 39 |                 <label class="form-label">Train/Test Split</label>
 40 |                 <input type="range" class="form-range" id="trainTestSplit"
 41 |                        min="50" max="90" value="80" step="5">
 42 |                 <span id="splitValue">80%</span>
 43 |             </div>
 44 |
 45 |             <div class="form-group">
 46 |                 <label class="form-checkbox">
 47 |                     <input type="checkbox" id="autoML" checked>
 48 |                     <span>Use AutoML for hyperparameter tuning</span>
 49 |                 </label>
 50 |             </div>
 51 |
 52 |             <div class="form-group">
 53 |                 <label class="form-checkbox">
 54 |                     <input type="checkbox" id="crossValidate" checked>
 55 |                     <span>Perform cross-validation</span>
 56 |                 </label>
 57 |             </div>
 58 |
 59 |             <button class="btn btn-primary" onclick="window.openStatica.ml.trainModel()">
 60 |                 Train Model
 61 |             </button>
 62 |
 63 |             <div id="modelResults" class="model-results"></div>
 64 |         `;
 65 |
 66 |         this.bindMLEvents();
 67 |         this.updateAlgorithmSelection('classification');
 68 |     }
 69 |
 70 |     bindMLEvents() {
 71 |         document.getElementById('mlTask')?.addEventListener('change', (e) => {
 72 |             this.updateAlgorithmSelection(e.target.value);
 73 |         });
 74 |
 75 |         document.getElementById('trainTestSplit')?.addEventListener('input', (e) => {
 76 |             document.getElementById('splitValue').textContent = `${e.target.value}%`;
 77 |         });
 78 |     }
 79 |
 80 |     updateAlgorithmSelection(task) {
 81 |         const container = document.getElementById('algorithmSelection');
 82 |         if (!container) return;
 83 |
 84 |         let algorithms = [];
 85 |
 86 |         switch (task) {
 87 |             case 'classification':
 88 |                 algorithms = [
 89 |                     {value: 'logistic', name: 'Logistic Regression'},
 90 |                     {value: 'svm', name: 'Support Vector Machine'},
 91 |                     {value: 'rf', name: 'Random Forest'},
 92 |                     {value: 'xgboost', name: 'XGBoost'},
 93 |                     {value: 'nn', name: 'Neural Network'},
 94 |                     {value: 'knn', name: 'K-Nearest Neighbors'},
 95 |                     {value: 'nb', name: 'Naive Bayes'},
 96 |                     {value: 'dt', name: 'Decision Tree'}
 97 |                 ];
 98 |                 break;
 99 |
100 |             case 'regression':
101 |                 algorithms = [
102 |                     {value: 'linear', name: 'Linear Regression'},
103 |                     {value: 'ridge', name: 'Ridge Regression'},
104 |                     {value: 'lasso', name: 'Lasso Regression'},
105 |                     {value: 'elastic', name: 'Elastic Net'},
106 |                     {value: 'svr', name: 'Support Vector Regression'},
107 |                     {value: 'rf_reg', name: 'Random Forest Regressor'},
108 |                     {value: 'xgb_reg', name: 'XGBoost Regressor'},
109 |                     {value: 'nn_reg', name: 'Neural Network Regressor'}
110 |                 ];
111 |                 break;
112 |
113 |             case 'clustering':
114 |                 algorithms = [
115 |                     {value: 'kmeans', name: 'K-Means'},
116 |                     {value: 'dbscan', name: 'DBSCAN'},
117 |                     {value: 'hierarchical', name: 'Hierarchical Clustering'},
118 |                     {value: 'gaussian', name: 'Gaussian Mixture'},
119 |                     {value: 'meanshift', name: 'Mean Shift'},
120 |                     {value: 'spectral', name: 'Spectral Clustering'}
121 |                 ];
122 |                 break;
123 |
124 |             case 'dimensionality':
125 |                 algorithms = [
126 |                     {value: 'pca', name: 'PCA'},
127 |                     {value: 'tsne', name: 't-SNE'},
128 |                     {value: 'umap', name: 'UMAP'},
129 |                     {value: 'lda', name: 'LDA'},
130 |                     {value: 'ica', name: 'ICA'},
131 |                     {value: 'autoencoder', name: 'Autoencoder'}
132 |                 ];
133 |                 break;
134 |
135 |             case 'anomaly':
136 |                 algorithms = [
137 |                     {value: 'isolation', name: 'Isolation Forest'},
138 |                     {value: 'lof', name: 'Local Outlier Factor'},
139 |                     {value: 'ocsvm', name: 'One-Class SVM'},
140 |                     {value: 'elliptic', name: 'Elliptic Envelope'},
141 |                     {value: 'autoencoder_ad', name: 'Autoencoder'}
142 |                 ];
143 |                 break;
144 |         }
145 |
146 |         container.innerHTML = `
147 |             <div class="form-group">
148 |                 <label class="form-label">Algorithm</label>
149 |                 <select class="form-control" id="mlAlgorithm">
150 |                     ${algorithms.map(alg =>
151 |             `<option value="${alg.value}">${alg.name}</option>`
152 |         ).join('')}
153 |                 </select>
154 |             </div>
155 |         `;
156 |     }
157 |
158 |     getVariableOptions() {
159 |         if (!this.app.data) return '';
160 |
161 |         return [...this.app.data.numeric_columns, ...this.app.data.categorical_columns]
162 |             .map(col => `<option value="${col}">${col}</option>`)
163 |             .join('');
164 |     }
165 |
166 |     async trainModel() {
167 |         try {
168 |             this.app.showLoading(true);
169 |
170 |             const task = document.getElementById('mlTask')?.value;
171 |             const algorithm = document.getElementById('mlAlgorithm')?.value;
172 |             const target = document.getElementById('targetVariable')?.value;
173 |             const features = this.app.getSelectedVariables();
174 |             const trainTestSplit = document.getElementById('trainTestSplit')?.value / 100;
175 |             const autoML = document.getElementById('autoML')?.checked;
176 |             const crossValidate = document.getElementById('crossValidate')?.checked;
177 |
178 |             if (features.length === 0) {
179 |                 throw new Error('Please select feature variables');
180 |             }
181 |
182 |             if (['classification', 'regression'].includes(task) && !target) {
183 |                 throw new Error('Please select a target variable');
184 |             }
185 |
186 |             const params = {
187 |                 task: task,
188 |                 algorithm: algorithm,
189 |                 features: features,
190 |                 target: target,
191 |                 train_test_split: trainTestSplit,
192 |                 auto_ml: autoML,
193 |                 cross_validate: crossValidate
194 |             };
195 |
196 |             const result = await this.app.api.trainModel(this.app.sessionId, params);
197 |
198 |             // Store trained model
199 |             this.currentModel = result.model_id;
200 |             this.trainedModels.set(result.model_id, result);
201 |
202 |             // Display results
203 |             this.displayModelResults(result);
204 |
205 |             this.app.showNotification('Model trained successfully!', 'success');
206 |
207 |         } catch (error) {
208 |             this.app.showNotification(error.message, 'error');
209 |         } finally {
210 |             this.app.showLoading(false);
211 |         }
212 |     }
213 |
214 |     displayModelResults(results) {
215 |         const container = document.getElementById('modelResults');
216 |         if (!container) return;
217 |
218 |         container.innerHTML = `
219 |             <div class="model-card">
220 |                 <h3>Model Performance</h3>
221 |
222 |                 <div class="model-metrics">
223 |                     ${this.formatModelMetrics(results.metrics)}
224 |                 </div>
225 |
226 |                 ${results.feature_importance ? `
227 |                     <div class="feature-importance">
228 |                         <h4>Feature Importance</h4>
229 |                         ${this.formatFeatureImportance(results.feature_importance)}
230 |                     </div>
231 |                 ` : ''}
232 |
233 |                 ${results.confusion_matrix ? `
234 |                     <div class="confusion-matrix">
235 |                         <h4>Confusion Matrix</h4>
236 |                         ${this.formatConfusionMatrix(results.confusion_matrix)}
237 |                     </div>
238 |                 ` : ''}
239 |
240 |                 <div class="model-actions">
241 |                     <button class="btn btn-secondary" onclick="window.openStatica.ml.downloadModel('${results.model_id}')">
242 |                         Download Model
243 |                     </button>
244 |                     <button class="btn btn-primary" onclick="window.openStatica.ml.deployModel('${results.model_id}')">
245 |                         Deploy Model
246 |                     </button>
247 |                 </div>
248 |             </div>
249 |         `;
250 |     }
251 |
252 |     formatModelMetrics(metrics) {
253 |         let html = '<div class="metrics-grid">';
254 |
255 |         for (const [key, value] of Object.entries(metrics)) {
256 |             html += `
257 |                 <div class="metric-item">
258 |                     <div class="metric-label">${this.formatMetricName(key)}</div>
259 |                     <div class="metric-value">${this.formatMetricValue(value)}</div>
260 |                 </div>
261 |             `;
262 |         }
263 |
264 |         html += '</div>';
265 |         return html;
266 |     }
267 |
268 |     formatMetricName(name) {
269 |         const nameMap = {
270 |             'accuracy': 'Accuracy',
271 |             'precision': 'Precision',
272 |             'recall': 'Recall',
273 |             'f1_score': 'F1 Score',
274 |             'auc_roc': 'AUC-ROC',
275 |             'mse': 'Mean Squared Error',
276 |             'rmse': 'Root MSE',
277 |             'mae': 'Mean Absolute Error',
278 |             'r2': 'R² Score'
279 |         };
280 |
281 |         return nameMap[name] || name.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
282 |     }
283 |
284 |     formatMetricValue(value) {
285 |         if (typeof value === 'number') {
286 |             return value.toFixed(4);
287 |         }
288 |         return value;
289 |     }
290 |
291 |     formatFeatureImportance(importance) {
292 |         const sorted = Object.entries(importance).sort((a, b) => b[1] - a[1]);
293 |
294 |         let html = '<div class="importance-chart">';
295 |
296 |         sorted.slice(0, 10).forEach(([feature, value]) => {
297 |             const percentage = (value * 100).toFixed(1);
298 |             html += `
299 |                 <div class="importance-bar">
300 |                     <div class="importance-label">${feature}</div>
301 |                     <div class="importance-value">
302 |                         <div class="importance-fill" style="width: ${percentage}%"></div>
303 |                         <span>${percentage}%</span>
304 |                     </div>
305 |                 </div>
306 |             `;
307 |         });
308 |
309 |         html += '</div>';
310 |         return html;
311 |     }
312 |
313 |     formatConfusionMatrix(matrix) {
314 |         // Simple 2x2 confusion matrix for binary classification
315 |         if (matrix.length === 2) {
316 |             return `
317 |                 <table class="confusion-matrix-table">
318 |                     <thead>
319 |                         <tr>
320 |                             <th></th>
321 |                             <th>Predicted 0</th>
322 |                             <th>Predicted 1</th>
323 |                         </tr>
324 |                     </thead>
325 |                     <tbody>
326 |                         <tr>
327 |                             <th>Actual 0</th>
328 |                             <td>${matrix[0][0]}</td>
329 |                             <td>${matrix[0][1]}</td>
330 |                         </tr>
331 |                         <tr>
332 |                             <th>Actual 1</th>
333 |                             <td>${matrix[1][0]}</td>
334 |                             <td>${matrix[1][1]}</td>
335 |                         </tr>
336 |                     </tbody>
337 |                 </table>
338 |             `;
339 |         }
340 |
341 |         // For multi-class, create a more complex table
342 |         return '<div>Multi-class confusion matrix visualization</div>';
343 |     }
344 |
345 |     async downloadModel(modelId) {
346 |         try {
347 |             const blob = await this.app.api.exportResults(this.app.sessionId, 'model');
348 |             const url = URL.createObjectURL(blob);
349 |             const a = document.createElement('a');
350 |             a.href = url;
351 |             a.download = `model_${modelId}.pkl`;
352 |             a.click();
353 |             URL.revokeObjectURL(url);
354 |         } catch (error) {
355 |             this.app.showNotification('Failed to download model', 'error');
356 |         }
357 |     }
358 |
359 |     async deployModel(modelId) {
360 |         // TODO: Implement model deployment
361 |         this.app.showNotification('Model deployment coming soon!', 'info');
362 |     }
363 |
364 |     async runMLAnalysis(variables) {
365 |         // This is called from the main app
366 |         return await this.trainModel();
367 |     }
368 | }


--------------------------------------------------------------------------------
/frontend/js/modules/statistics.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * Statistics Module for OpenStatica
  3 |  */
  4 |
  5 | class StatisticsModule {
  6 |     constructor(app) {
  7 |         this.app = app;
  8 |         this.currentTest = null;
  9 |     }
 10 |
 11 |     loadDescriptiveOptions() {
 12 |         const container = document.getElementById('optionsContent');
 13 |         if (!container) return;
 14 |
 15 |         container.innerHTML = `
 16 |             <div class="form-group">
 17 |                 <label class="form-label">Analysis Type</label>
 18 |                 <select class="form-control" id="descriptiveType">
 19 |                     <option value="basic">Basic Statistics</option>
 20 |                     <option value="frequency">Frequency Distribution</option>
 21 |                     <option value="crosstab">Cross-tabulation</option>
 22 |                     <option value="normality">Normality Tests</option>
 23 |                     <option value="outliers">Outlier Detection</option>
 24 |                 </select>
 25 |             </div>
 26 |
 27 |             <div class="form-group">
 28 |                 <label class="form-checkbox">
 29 |                     <input type="checkbox" id="includeAdvanced" checked>
 30 |                     <span>Include advanced statistics</span>
 31 |                 </label>
 32 |             </div>
 33 |
 34 |             <div class="form-group">
 35 |                 <label class="form-checkbox">
 36 |                     <input type="checkbox" id="includeVisualizations" checked>
 37 |                     <span>Generate visualizations</span>
 38 |                 </label>
 39 |             </div>
 40 |
 41 |             <div class="form-group">
 42 |                 <label class="form-label">Confidence Level</label>
 43 |                 <select class="form-control" id="confidenceLevel">
 44 |                     <option value="0.90">90%</option>
 45 |                     <option value="0.95" selected>95%</option>
 46 |                     <option value="0.99">99%</option>
 47 |                 </select>
 48 |             </div>
 49 |         `;
 50 |
 51 |         this.bindDescriptiveEvents();
 52 |     }
 53 |
 54 |     loadInferentialOptions() {
 55 |         const container = document.getElementById('optionsContent');
 56 |         if (!container) return;
 57 |
 58 |         container.innerHTML = `
 59 |             <div class="form-group">
 60 |                 <label class="form-label">Test Type</label>
 61 |                 <select class="form-control" id="inferentialTest">
 62 |                     <optgroup label="Parametric Tests">
 63 |                         <option value="ttest_one">One-Sample T-Test</option>
 64 |                         <option value="ttest_independent">Independent T-Test</option>
 65 |                         <option value="ttest_paired">Paired T-Test</option>
 66 |                         <option value="anova_one">One-Way ANOVA</option>
 67 |                         <option value="anova_two">Two-Way ANOVA</option>
 68 |                         <option value="anova_repeated">Repeated Measures ANOVA</option>
 69 |                     </optgroup>
 70 |                     <optgroup label="Non-Parametric Tests">
 71 |                         <option value="mann_whitney">Mann-Whitney U</option>
 72 |                         <option value="wilcoxon">Wilcoxon Signed-Rank</option>
 73 |                         <option value="kruskal">Kruskal-Wallis</option>
 74 |                         <option value="friedman">Friedman Test</option>
 75 |                     </optgroup>
 76 |                     <optgroup label="Correlation Tests">
 77 |                         <option value="pearson">Pearson Correlation</option>
 78 |                         <option value="spearman">Spearman Correlation</option>
 79 |                         <option value="kendall">Kendall's Tau</option>
 80 |                     </optgroup>
 81 |                     <optgroup label="Other Tests">
 82 |                         <option value="chi_square">Chi-Square Test</option>
 83 |                         <option value="fisher">Fisher's Exact Test</option>
 84 |                         <option value="mcnemar">McNemar's Test</option>
 85 |                     </optgroup>
 86 |                 </select>
 87 |             </div>
 88 |
 89 |             <div id="testSpecificOptions"></div>
 90 |
 91 |             <div class="form-group">
 92 |                 <label class="form-label">Significance Level (α)</label>
 93 |                 <select class="form-control" id="alphaLevel">
 94 |                     <option value="0.01">0.01</option>
 95 |                     <option value="0.05" selected>0.05</option>
 96 |                     <option value="0.10">0.10</option>
 97 |                 </select>
 98 |             </div>
 99 |
100 |             <div class="form-group">
101 |                 <label class="form-checkbox">
102 |                     <input type="checkbox" id="includePostHoc" checked>
103 |                     <span>Include post-hoc tests (if applicable)</span>
104 |                 </label>
105 |             </div>
106 |
107 |             <div class="form-group">
108 |                 <label class="form-checkbox">
109 |                     <input type="checkbox" id="includeEffectSize" checked>
110 |                     <span>Calculate effect sizes</span>
111 |                 </label>
112 |             </div>
113 |         `;
114 |
115 |         this.bindInferentialEvents();
116 |     }
117 |
118 |     loadRegressionOptions() {
119 |         const container = document.getElementById('optionsContent');
120 |         if (!container) return;
121 |
122 |         container.innerHTML = `
123 |             <div class="form-group">
124 |                 <label class="form-label">Regression Type</label>
125 |                 <select class="form-control" id="regressionType">
126 |                     <optgroup label="Linear Models">
127 |                         <option value="linear">Linear Regression</option>
128 |                         <option value="multiple">Multiple Linear Regression</option>
129 |                         <option value="polynomial">Polynomial Regression</option>
130 |                         <option value="stepwise">Stepwise Regression</option>
131 |                     </optgroup>
132 |                     <optgroup label="Regularized Models">
133 |                         <option value="ridge">Ridge Regression</option>
134 |                         <option value="lasso">Lasso Regression</option>
135 |                         <option value="elastic">Elastic Net</option>
136 |                     </optgroup>
137 |                     <optgroup label="Generalized Linear Models">
138 |                         <option value="logistic">Logistic Regression</option>
139 |                         <option value="poisson">Poisson Regression</option>
140 |                         <option value="gamma">Gamma Regression</option>
141 |                     </optgroup>
142 |                     <optgroup label="Advanced Models">
143 |                         <option value="quantile">Quantile Regression</option>
144 |                         <option value="robust">Robust Regression</option>
145 |                         <option value="pls">Partial Least Squares</option>
146 |                     </optgroup>
147 |                 </select>
148 |             </div>
149 |
150 |             <div class="form-group">
151 |                 <label class="form-label">Dependent Variable</label>
152 |                 <select class="form-control" id="dependentVariable">
153 |                     <option value="">Select variable...</option>
154 |                     ${this.getNumericVariableOptions()}
155 |                 </select>
156 |             </div>
157 |
158 |             <div class="form-group">
159 |                 <label class="form-label">Feature Selection</label>
160 |                 <select class="form-control" id="featureSelection">
161 |                     <option value="none">None</option>
162 |                     <option value="forward">Forward Selection</option>
163 |                     <option value="backward">Backward Elimination</option>
164 |                     <option value="both">Bidirectional</option>
165 |                     <option value="rfe">Recursive Feature Elimination</option>
166 |                 </select>
167 |             </div>
168 |
169 |             <div class="form-group">
170 |                 <label class="form-checkbox">
171 |                     <input type="checkbox" id="includeDiagnostics" checked>
172 |                     <span>Include diagnostic plots</span>
173 |                 </label>
174 |             </div>
175 |
176 |             <div class="form-group">
177 |                 <label class="form-checkbox">
178 |                     <input type="checkbox" id="crossValidation" checked>
179 |                     <span>Perform cross-validation</span>
180 |                 </label>
181 |             </div>
182 |         `;
183 |
184 |         this.bindRegressionEvents();
185 |     }
186 |
187 |     bindDescriptiveEvents() {
188 |         document.getElementById('descriptiveType')?.addEventListener('change', (e) => {
189 |             this.updateDescriptiveSubOptions(e.target.value);
190 |         });
191 |     }
192 |
193 |     bindInferentialEvents() {
194 |         document.getElementById('inferentialTest')?.addEventListener('change', (e) => {
195 |             this.updateTestSpecificOptions(e.target.value);
196 |         });
197 |     }
198 |
199 |     bindRegressionEvents() {
200 |         document.getElementById('regressionType')?.addEventListener('change', (e) => {
201 |             this.updateRegressionSubOptions(e.target.value);
202 |         });
203 |     }
204 |
205 |     updateTestSpecificOptions(testType) {
206 |         const container = document.getElementById('testSpecificOptions');
207 |         if (!container) return;
208 |
209 |         let html = '';
210 |
211 |         if (testType.startsWith('ttest_one')) {
212 |             html = `
213 |                 <div class="form-group">
214 |                     <label class="form-label">Test Value</label>
215 |                     <input type="number" class="form-control" id="testValue" value="0">
216 |                 </div>
217 |             `;
218 |         } else if (testType.includes('anova')) {
219 |             html = `
220 |                 <div class="form-group">
221 |                     <label class="form-label">Post-hoc Test</label>
222 |                     <select class="form-control" id="postHocTest">
223 |                         <option value="tukey">Tukey HSD</option>
224 |                         <option value="bonferroni">Bonferroni</option>
225 |                         <option value="scheffe">Scheffe</option>
226 |                         <option value="sidak">Sidak</option>
227 |                     </select>
228 |                 </div>
229 |             `;
230 |         }
231 |
232 |         container.innerHTML = html;
233 |     }
234 |
235 |     getNumericVariableOptions() {
236 |         if (!this.app.data) return '';
237 |
238 |         return this.app.data.numeric_columns.map(col =>
239 |             `<option value="${col}">${col}</option>`
240 |         ).join('');
241 |     }
242 |
243 |     async runDescriptiveAnalysis(variables) {
244 |         const type = document.getElementById('descriptiveType')?.value || 'basic';
245 |         const includeAdvanced = document.getElementById('includeAdvanced')?.checked;
246 |         const includeViz = document.getElementById('includeVisualizations')?.checked;
247 |
248 |         const params = {
249 |             columns: variables,
250 |             options: {
251 |                 type: type,
252 |                 include_advanced: includeAdvanced,
253 |                 include_visualizations: includeViz
254 |             }
255 |         };
256 |
257 |         return await this.app.api.descriptiveStats(this.app.sessionId, params);
258 |     }
259 |
260 |     async runInferentialAnalysis(variables) {
261 |         const testType = document.getElementById('inferentialTest')?.value;
262 |         const alpha = parseFloat(document.getElementById('alphaLevel')?.value || 0.05);
263 |
264 |         const params = {
265 |             columns: variables,
266 |             options: {
267 |                 test_type: testType,
268 |                 alpha: alpha,
269 |                 include_post_hoc: document.getElementById('includePostHoc')?.checked,
270 |                 include_effect_size: document.getElementById('includeEffectSize')?.checked
271 |             }
272 |         };
273 |
274 |         // Add test-specific parameters
275 |         if (testType === 'ttest_one') {
276 |             params.options.test_value = parseFloat(document.getElementById('testValue')?.value || 0);
277 |         }
278 |
279 |         return await this.app.api.tTest(this.app.sessionId, params);
280 |     }
281 |
282 |     async runRegressionAnalysis(variables) {
283 |         const regressionType = document.getElementById('regressionType')?.value;
284 |         const dependent = document.getElementById('dependentVariable')?.value;
285 |
286 |         if (!dependent) {
287 |             throw new Error('Please select a dependent variable');
288 |         }
289 |
290 |         const params = {
291 |             columns: variables,
292 |             options: {
293 |                 method: regressionType,
294 |                 dependent: dependent,
295 |                 independent: variables.filter(v => v !== dependent),
296 |                 feature_selection: document.getElementById('featureSelection')?.value,
297 |                 include_diagnostics: document.getElementById('includeDiagnostics')?.checked,
298 |                 cross_validation: document.getElementById('crossValidation')?.checked
299 |             }
300 |         };
301 |
302 |         return await this.app.api.regression(this.app.sessionId, params);
303 |     }
304 |
305 |     formatStatisticalResults(results) {
306 |         // Create formatted HTML for results display
307 |         let html = '<div class="statistical-results">';
308 |
309 |         // Add summary
310 |         if (results.summary) {
311 |             html += `
312 |                 <div class="result-summary">
313 |                     <h3>Summary</h3>
314 |                     <p>${results.summary}</p>
315 |                 </div>
316 |             `;
317 |         }
318 |
319 |         // Add main statistics
320 |         if (results.statistics) {
321 |             html += '<div class="result-statistics">';
322 |             html += this.formatStatisticsTable(results.statistics);
323 |             html += '</div>';
324 |         }
325 |
326 |         // Add interpretation
327 |         if (results.interpretation) {
328 |             html += `
329 |                 <div class="result-interpretation">
330 |                     <h3>Interpretation</h3>
331 |                     <p>${results.interpretation}</p>
332 |                 </div>
333 |             `;
334 |         }
335 |
336 |         html += '</div>';
337 |         return html;
338 |     }
339 |
340 |     formatStatisticsTable(stats) {
341 |         let html = '<table class="statistics-table">';
342 |         html += '<thead><tr><th>Statistic</th><th>Value</th></tr></thead>';
343 |         html += '<tbody>';
344 |
345 |         for (const [key, value] of Object.entries(stats)) {
346 |             const formattedKey = this.formatStatisticName(key);
347 |             const formattedValue = this.formatStatisticValue(value);
348 |             html += `<tr><td>${formattedKey}</td><td>${formattedValue}</td></tr>`;
349 |         }
350 |
351 |         html += '</tbody></table>';
352 |         return html;
353 |     }
354 |
355 |     formatStatisticName(name) {
356 |         return name
357 |             .replace(/_/g, ' ')
358 |             .replace(/\b\w/g, l => l.toUpperCase());
359 |     }
360 |
361 |     formatStatisticValue(value) {
362 |         if (typeof value === 'number') {
363 |             return value.toFixed(4);
364 |         } else if (Array.isArray(value)) {
365 |             return `[${value.map(v => v.toFixed(4)).join(', ')}]`;
366 |         } else if (typeof value === 'object') {
367 |             return JSON.stringify(value, null, 2);
368 |         }
369 |         return value;
370 |     }
371 | }


--------------------------------------------------------------------------------
/frontend/js/modules/visualization.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * Visualization Module for OpenStatica
  3 |  */
  4 |
  5 | class VisualizationModule {
  6 |     constructor(app) {
  7 |         this.app = app;
  8 |         this.charts = new Map();
  9 |         this.currentChart = null;
 10 |     }
 11 |
 12 |     async createVisualizations(results) {
 13 |         const container = document.getElementById('vizContainer');
 14 |         if (!container) return;
 15 |
 16 |         // Clear previous visualizations
 17 |         container.innerHTML = '';
 18 |
 19 |         // Determine visualization type based on results
 20 |         if (results.visualization_type) {
 21 |             await this.createSpecificVisualization(results, container);
 22 |         } else {
 23 |             // Auto-detect appropriate visualizations
 24 |             await this.autoCreateVisualizations(results, container);
 25 |         }
 26 |     }
 27 |
 28 |     async autoCreateVisualizations(results, container) {
 29 |         // Create appropriate visualizations based on data type
 30 |         if (results.histogram_data) {
 31 |             await this.createHistogram(results.histogram_data, container);
 32 |         }
 33 |
 34 |         if (results.scatter_data) {
 35 |             await this.createScatterPlot(results.scatter_data, container);
 36 |         }
 37 |
 38 |         if (results.box_plot_data) {
 39 |             await this.createBoxPlot(results.box_plot_data, container);
 40 |         }
 41 |
 42 |         if (results.correlation_matrix) {
 43 |             await this.createHeatmap(results.correlation_matrix, container);
 44 |         }
 45 |
 46 |         if (results.time_series_data) {
 47 |             await this.createTimeSeriesPlot(results.time_series_data, container);
 48 |         }
 49 |     }
 50 |
 51 |     async createHistogram(data, container) {
 52 |         const div = document.createElement('div');
 53 |         div.className = 'chart-container';
 54 |         div.innerHTML = '<canvas id="histogramChart"></canvas>';
 55 |         container.appendChild(div);
 56 |
 57 |         const ctx = document.getElementById('histogramChart').getContext('2d');
 58 |
 59 |         const chart = new Chart(ctx, {
 60 |             type: 'bar',
 61 |             data: {
 62 |                 labels: data.bins,
 63 |                 datasets: [{
 64 |                     label: data.label || 'Frequency',
 65 |                     data: data.frequencies,
 66 |                     backgroundColor: 'rgba(99, 102, 241, 0.5)',
 67 |                     borderColor: 'rgba(99, 102, 241, 1)',
 68 |                     borderWidth: 1
 69 |                 }]
 70 |             },
 71 |             options: {
 72 |                 responsive: true,
 73 |                 maintainAspectRatio: false,
 74 |                 plugins: {
 75 |                     title: {
 76 |                         display: true,
 77 |                         text: data.title || 'Histogram'
 78 |                     },
 79 |                     legend: {
 80 |                         display: false
 81 |                     }
 82 |                 },
 83 |                 scales: {
 84 |                     x: {
 85 |                         title: {
 86 |                             display: true,
 87 |                             text: data.xlabel || 'Value'
 88 |                         }
 89 |                     },
 90 |                     y: {
 91 |                         title: {
 92 |                             display: true,
 93 |                             text: 'Frequency'
 94 |                         }
 95 |                     }
 96 |                 }
 97 |             }
 98 |         });
 99 |
100 |         this.charts.set('histogram', chart);
101 |     }
102 |
103 |     async createScatterPlot(data, container) {
104 |         const div = document.createElement('div');
105 |         div.className = 'chart-container';
106 |         div.id = 'scatterPlot';
107 |         container.appendChild(div);
108 |
109 |         const trace = {
110 |             x: data.x,
111 |             y: data.y,
112 |             mode: 'markers',
113 |             type: 'scatter',
114 |             name: data.label || 'Data',
115 |             marker: {
116 |                 color: data.color || 'rgba(99, 102, 241, 0.7)',
117 |                 size: 8
118 |             }
119 |         };
120 |
121 |         const layout = {
122 |             title: data.title || 'Scatter Plot',
123 |             xaxis: {title: data.xlabel || 'X'},
124 |             yaxis: {title: data.ylabel || 'Y'},
125 |             height: 400
126 |         };
127 |
128 |         // Add regression line if available
129 |         if (data.regression_line) {
130 |             const lineTrace = {
131 |                 x: data.regression_line.x,
132 |                 y: data.regression_line.y,
133 |                 mode: 'lines',
134 |                 type: 'scatter',
135 |                 name: 'Regression Line',
136 |                 line: {
137 |                     color: 'rgba(239, 68, 68, 1)',
138 |                     width: 2
139 |                 }
140 |             };
141 |             Plotly.newPlot('scatterPlot', [trace, lineTrace], layout);
142 |         } else {
143 |             Plotly.newPlot('scatterPlot', [trace], layout);
144 |         }
145 |     }
146 |
147 |     async createBoxPlot(data, container) {
148 |         const div = document.createElement('div');
149 |         div.className = 'chart-container';
150 |         div.id = 'boxPlot';
151 |         container.appendChild(div);
152 |
153 |         const traces = data.groups.map((group, i) => ({
154 |             y: group.values,
155 |             type: 'box',
156 |             name: group.name,
157 |             marker: {
158 |                 color: this.getColor(i)
159 |             }
160 |         }));
161 |
162 |         const layout = {
163 |             title: data.title || 'Box Plot',
164 |             yaxis: {title: data.ylabel || 'Value'},
165 |             height: 400
166 |         };
167 |
168 |         Plotly.newPlot('boxPlot', traces, layout);
169 |     }
170 |
171 |     async createHeatmap(data, container) {
172 |         const div = document.createElement('div');
173 |         div.className = 'chart-container';
174 |         div.id = 'heatmap';
175 |         container.appendChild(div);
176 |
177 |         const trace = {
178 |             z: data.values,
179 |             x: data.columns,
180 |             y: data.rows,
181 |             type: 'heatmap',
182 |             colorscale: 'RdBu',
183 |             reversescale: true,
184 |             showscale: true
185 |         };
186 |
187 |         const layout = {
188 |             title: data.title || 'Correlation Heatmap',
189 |             height: 500,
190 |             width: 600,
191 |             annotations: []
192 |         };
193 |
194 |         // Add text annotations
195 |         for (let i = 0; i < data.rows.length; i++) {
196 |             for (let j = 0; j < data.columns.length; j++) {
197 |                 layout.annotations.push({
198 |                     x: data.columns[j],
199 |                     y: data.rows[i],
200 |                     text: data.values[i][j].toFixed(2),
201 |                     showarrow: false,
202 |                     font: {color: Math.abs(data.values[i][j]) > 0.5 ? 'white' : 'black'}
203 |                 });
204 |             }
205 |         }
206 |
207 |         Plotly.newPlot('heatmap', [trace], layout);
208 |     }
209 |
210 |     async createTimeSeriesPlot(data, container) {
211 |         const div = document.createElement('div');
212 |         div.className = 'chart-container';
213 |         div.id = 'timeSeriesPlot';
214 |         container.appendChild(div);
215 |
216 |         const traces = data.series.map((series, i) => ({
217 |             x: series.dates,
218 |             y: series.values,
219 |             type: 'scatter',
220 |             mode: 'lines',
221 |             name: series.name,
222 |             line: {
223 |                 color: this.getColor(i),
224 |                 width: 2
225 |             }
226 |         }));
227 |
228 |         const layout = {
229 |             title: data.title || 'Time Series',
230 |             xaxis: {
231 |                 title: 'Date',
232 |                 type: 'date'
233 |             },
234 |             yaxis: {
235 |                 title: data.ylabel || 'Value'
236 |             },
237 |             height: 400
238 |         };
239 |
240 |         Plotly.newPlot('timeSeriesPlot', traces, layout);
241 |     }
242 |
243 |     async create3DPlot(data, container) {
244 |         const div = document.createElement('div');
245 |         div.className = 'chart-container';
246 |         div.id = 'plot3D';
247 |         container.appendChild(div);
248 |
249 |         const trace = {
250 |             x: data.x,
251 |             y: data.y,
252 |             z: data.z,
253 |             mode: 'markers',
254 |             type: 'scatter3d',
255 |             marker: {
256 |                 color: data.color || data.z,
257 |                 colorscale: 'Viridis',
258 |                 showscale: true,
259 |                 size: 5
260 |             }
261 |         };
262 |
263 |         const layout = {
264 |             title: data.title || '3D Scatter Plot',
265 |             scene: {
266 |                 xaxis: {title: data.xlabel || 'X'},
267 |                 yaxis: {title: data.ylabel || 'Y'},
268 |                 zaxis: {title: data.zlabel || 'Z'}
269 |             },
270 |             height: 500
271 |         };
272 |
273 |         Plotly.newPlot('plot3D', [trace], layout);
274 |     }
275 |
276 |     async createNetworkGraph(data, container) {
277 |         // Network visualization for clustering or graph analysis
278 |         const div = document.createElement('div');
279 |         div.className = 'chart-container';
280 |         div.id = 'networkGraph';
281 |         container.appendChild(div);
282 |
283 |         // TODO: Implement network graph using D3.js or vis.js
284 |         div.innerHTML = '<p>Network graph visualization coming soon</p>';
285 |     }
286 |
287 |     getColor(index) {
288 |         const colors = [
289 |             '#6366f1', '#8b5cf6', '#ec4899', '#f43f5e',
290 |             '#f97316', '#eab308', '#84cc16', '#22c55e',
291 |             '#10b981', '#14b8a6', '#06b6d4', '#0ea5e9'
292 |         ];
293 |         return colors[index % colors.length];
294 |     }
295 |
296 |     async exportChart(format = 'png') {
297 |         if (!this.currentChart) {
298 |             this.app.showNotification('No chart to export', 'warning');
299 |             return;
300 |         }
301 |
302 |         try {
303 |             if (format === 'png') {
304 |                 const canvas = document.querySelector('.chart-container canvas');
305 |                 if (canvas) {
306 |                     const url = canvas.toDataURL('image/png');
307 |                     const a = document.createElement('a');
308 |                     a.href = url;
309 |                     a.download = 'chart.png';
310 |                     a.click();
311 |                 }
312 |             } else if (format === 'svg') {
313 |                 // For Plotly charts
314 |                 Plotly.downloadImage(this.currentChart, {
315 |                     format: 'svg',
316 |                     filename: 'chart'
317 |                 });
318 |             }
319 |         } catch (error) {
320 |             this.app.showNotification('Failed to export chart', 'error');
321 |         }
322 |     }
323 |
324 |     clearCharts() {
325 |         this.charts.forEach(chart => {
326 |             if (chart.destroy) {
327 |                 chart.destroy();
328 |             }
329 |         });
330 |         this.charts.clear();
331 |
332 |         const container = document.getElementById('vizContainer');
333 |         if (container) {
334 |             container.innerHTML = '';
335 |         }
336 |     }
337 | }


--------------------------------------------------------------------------------
/frontend/js/utils/helpers.js:
--------------------------------------------------------------------------------
  1 | /**
  2 |  * State Management for OpenStatica
  3 |  */
  4 |
  5 | class StateManager {
  6 |     constructor() {
  7 |         this.state = new Map();
  8 |         this.listeners = new Map();
  9 |         this.history = [];
 10 |         this.maxHistory = 50;
 11 |
 12 |         // Initialize default state
 13 |         this.initializeState();
 14 |     }
 15 |
 16 |     initializeState() {
 17 |         this.set('app', {
 18 |             version: '1.0.0',
 19 |             theme: 'light',
 20 |             currentView: 'data'
 21 |         });
 22 |
 23 |         this.set('data', {
 24 |             loaded: false,
 25 |             sessionId: null,
 26 |             info: null,
 27 |             preview: null
 28 |         });
 29 |
 30 |         this.set('analysis', {
 31 |             lastRun: null,
 32 |             results: {},
 33 |             selectedVariables: []
 34 |         });
 35 |
 36 |         this.set('models', {
 37 |             trained: [],
 38 |             current: null
 39 |         });
 40 |
 41 |         this.set('ui', {
 42 |             loading: false,
 43 |             sidebarCollapsed: false,
 44 |             modalOpen: false
 45 |         });
 46 |     }
 47 |
 48 |     get(key, path = null) {
 49 |         const value = this.state.get(key);
 50 |
 51 |         if (path && value) {
 52 |             return this.getNestedValue(value, path);
 53 |         }
 54 |
 55 |         return value;
 56 |     }
 57 |
 58 |     set(key, value, silent = false) {
 59 |         const oldValue = this.state.get(key);
 60 |         this.state.set(key, value);
 61 |
 62 |         // Add to history
 63 |         this.addToHistory({key, oldValue, newValue: value});
 64 |
 65 |         // Notify listeners
 66 |         if (!silent) {
 67 |             this.notify(key, value, oldValue);
 68 |         }
 69 |
 70 |         // Persist to localStorage if needed
 71 |         this.persist(key, value);
 72 |     }
 73 |
 74 |     update(key, updates, silent = false) {
 75 |         const current = this.get(key) || {};
 76 |         const updated = {...current, ...updates};
 77 |         this.set(key, updated, silent);
 78 |     }
 79 |
 80 |     getNestedValue(obj, path) {
 81 |         const keys = path.split('.');
 82 |         let value = obj;
 83 |
 84 |         for (const key of keys) {
 85 |             value = value?.[key];
 86 |             if (value === undefined) break;
 87 |         }
 88 |
 89 |         return value;
 90 |     }
 91 |
 92 |     setNestedValue(key, path, value) {
 93 |         const current = this.get(key) || {};
 94 |         const keys = path.split('.');
 95 |         const lastKey = keys.pop();
 96 |
 97 |         let target = current;
 98 |         for (const k of keys) {
 99 |             if (!target[k]) target[k] = {};
100 |             target = target[k];
101 |         }
102 |
103 |         target[lastKey] = value;
104 |         this.set(key, current);
105 |     }
106 |
107 |     subscribe(key, callback) {
108 |         if (!this.listeners.has(key)) {
109 |             this.listeners.set(key, []);
110 |         }
111 |
112 |         this.listeners.get(key).push(callback);
113 |
114 |         // Return unsubscribe function
115 |         return () => {
116 |             const callbacks = this.listeners.get(key);
117 |             const index = callbacks.indexOf(callback);
118 |             if (index > -1) {
119 |                 callbacks.splice(index, 1);
120 |             }
121 |         };
122 |     }
123 |
124 |     notify(key, newValue, oldValue) {
125 |         const callbacks = this.listeners.get(key) || [];
126 |         callbacks.forEach(callback => {
127 |             callback(newValue, oldValue, key);
128 |         });
129 |
130 |         // Notify global listeners
131 |         const globalCallbacks = this.listeners.get('*') || [];
132 |         globalCallbacks.forEach(callback => {
133 |             callback({key, newValue, oldValue});
134 |         });
135 |     }
136 |
137 |     addToHistory(change) {
138 |         this.history.push({
139 |             ...change,
140 |             timestamp: Date.now()
141 |         });
142 |
143 |         // Limit history size
144 |         if (this.history.length > this.maxHistory) {
145 |             this.history.shift();
146 |         }
147 |     }
148 |
149 |     undo() {
150 |         if (this.history.length === 0) return false;
151 |
152 |         const lastChange = this.history.pop();
153 |         this.set(lastChange.key, lastChange.oldValue, true);
154 |         return true;
155 |     }
156 |
157 |     persist(key, value) {
158 |         // Only persist certain keys
159 |         const persistKeys = ['app', 'ui'];
160 |
161 |         if (persistKeys.includes(key)) {
162 |             utils.storage.set(`openstatica_${key}`, value);
163 |         }
164 |     }
165 |
166 |     restore() {
167 |         // Restore persisted state
168 |         const appState = utils.storage.get('openstatica_app');
169 |         if (appState) {
170 |             this.set('app', appState, true);
171 |         }
172 |
173 |         const uiState = utils.storage.get('openstatica_ui');
174 |         if (uiState) {
175 |             this.set('ui', uiState, true);
176 |         }
177 |     }
178 |
179 |     reset() {
180 |         this.state.clear();
181 |         this.history = [];
182 |         this.initializeState();
183 |     }
184 |
185 |     export() {
186 |         const exportData = {};
187 |         this.state.forEach((value, key) => {
188 |             exportData[key] = value;
189 |         });
190 |         return exportData;
191 |     }
192 |
193 |     import(data) {
194 |         Object.entries(data).forEach(([key, value]) => {
195 |             this.set(key, value, true);
196 |         });
197 |     }
198 | }
199 |
200 | // Create global state manager instance
201 | window.stateManager = new StateManager();


--------------------------------------------------------------------------------
/plugins/README.md:
--------------------------------------------------------------------------------
https://raw.githubusercontent.com/shamspias/open-statica/f8e8c6dcc6a67b74cb15b9605452db77a648b4ee/plugins/README.md


--------------------------------------------------------------------------------